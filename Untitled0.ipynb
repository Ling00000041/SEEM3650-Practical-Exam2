{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1Edr0TH3t2miHHH5ZOCWbtyOjVjv5vP4K",
      "authorship_tag": "ABX9TyOx4PavnvLc09Xkimb5s0kM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ling00000041/SEEM3650-Practical-Exam2/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUgxaFmmtoos",
        "outputId": "683ac494-367a-42fe-af00-747c74931993"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.27.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m123.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m108.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.1 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 tiktoken-0.9.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "# 安装 nanoGPT 所需依赖\n",
        "!pip install torch numpy transformers datasets tiktoken wandb tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 克隆 nanoGPT 仓库\n",
        "!git clone https://github.com/karpathy/nanoGPT.git\n",
        "%cd nanoGPT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GL4pm-CvPjb",
        "outputId": "20158cdb-f160-422a-bae9-d276c7b3eaf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 686, done.\u001b[K\n",
            "remote: Total 686 (delta 0), reused 0 (delta 0), pack-reused 686 (from 1)\u001b[K\n",
            "Receiving objects: 100% (686/686), 954.03 KiB | 7.34 MiB/s, done.\n",
            "Resolving deltas: 100% (387/387), done.\n",
            "/content/nanoGPT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 运行数据准备脚本，生成 train.bin 和 val.bin\n",
        "!python data/shakespeare_char/prepare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3lKVrW0vSF5",
        "outputId": "ef8bb59a-8f96-4e1e-8962-1649caafd3f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1,115,394\n",
            "all the unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n",
            "train has 1,003,854 tokens\n",
            "val has 111,540 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/train_shakespeare_char.py --device=cuda --compile=False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMcRY4ayvz40",
        "outputId": "3881dea8-34de-40ca-b25e-6b717f972b6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: device = cuda\n",
            "Overriding: compile = False\n",
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 10.65M\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "num decayed parameter tensors: 26, with 10,740,096 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.2874, val loss 4.2823\n",
            "iter 0: loss 4.2697, time 4221.25ms, mfu -100.00%\n",
            "iter 10: loss 3.1447, time 14.77ms, mfu 25.23%\n",
            "iter 20: loss 2.7362, time 14.78ms, mfu 25.23%\n",
            "iter 30: loss 2.6184, time 14.67ms, mfu 25.24%\n",
            "iter 40: loss 2.5715, time 14.72ms, mfu 25.25%\n",
            "iter 50: loss 2.5242, time 14.66ms, mfu 25.27%\n",
            "iter 60: loss 2.5094, time 15.04ms, mfu 25.22%\n",
            "iter 70: loss 2.4960, time 14.80ms, mfu 25.21%\n",
            "iter 80: loss 2.5001, time 14.57ms, mfu 25.25%\n",
            "iter 90: loss 2.4657, time 14.69ms, mfu 25.26%\n",
            "iter 100: loss 2.4576, time 14.94ms, mfu 25.23%\n",
            "iter 110: loss 2.4471, time 15.73ms, mfu 25.07%\n",
            "iter 120: loss 2.4264, time 14.95ms, mfu 25.06%\n",
            "iter 130: loss 2.4105, time 14.82ms, mfu 25.07%\n",
            "iter 140: loss 2.3995, time 15.51ms, mfu 24.96%\n",
            "iter 150: loss 2.4031, time 15.22ms, mfu 24.91%\n",
            "iter 160: loss 2.3593, time 14.79ms, mfu 24.94%\n",
            "iter 170: loss 2.3606, time 14.78ms, mfu 24.97%\n",
            "iter 180: loss 2.3148, time 15.15ms, mfu 24.93%\n",
            "iter 190: loss 2.2482, time 14.99ms, mfu 24.92%\n",
            "iter 200: loss 2.2121, time 14.89ms, mfu 24.93%\n",
            "iter 210: loss 2.1540, time 16.29ms, mfu 24.73%\n",
            "iter 220: loss 2.1400, time 15.56ms, mfu 24.65%\n",
            "iter 230: loss 2.0710, time 14.85ms, mfu 24.69%\n",
            "iter 240: loss 2.0767, time 14.63ms, mfu 24.77%\n",
            "step 250: train loss 1.9623, val loss 2.0595\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 250: loss 2.0363, time 3422.53ms, mfu 22.31%\n",
            "iter 260: loss 1.9761, time 14.68ms, mfu 22.61%\n",
            "iter 270: loss 1.9741, time 14.70ms, mfu 22.89%\n",
            "iter 280: loss 1.9881, time 14.61ms, mfu 23.15%\n",
            "iter 290: loss 1.9161, time 14.74ms, mfu 23.36%\n",
            "iter 300: loss 1.8987, time 14.68ms, mfu 23.56%\n",
            "iter 310: loss 1.8710, time 14.62ms, mfu 23.76%\n",
            "iter 320: loss 1.8622, time 14.63ms, mfu 23.93%\n",
            "iter 330: loss 1.8174, time 14.60ms, mfu 24.09%\n",
            "iter 340: loss 1.7779, time 14.76ms, mfu 24.20%\n",
            "iter 350: loss 1.8301, time 14.94ms, mfu 24.28%\n",
            "iter 360: loss 1.7679, time 14.99ms, mfu 24.34%\n",
            "iter 370: loss 1.7418, time 15.21ms, mfu 24.35%\n",
            "iter 380: loss 1.7232, time 15.46ms, mfu 24.33%\n",
            "iter 390: loss 1.7330, time 15.09ms, mfu 24.36%\n",
            "iter 400: loss 1.7584, time 14.82ms, mfu 24.44%\n",
            "iter 410: loss 1.6958, time 14.80ms, mfu 24.52%\n",
            "iter 420: loss 1.7062, time 14.58ms, mfu 24.62%\n",
            "iter 430: loss 1.6857, time 14.62ms, mfu 24.71%\n",
            "iter 440: loss 1.6525, time 14.67ms, mfu 24.78%\n",
            "iter 450: loss 1.6488, time 14.57ms, mfu 24.86%\n",
            "iter 460: loss 1.5947, time 14.60ms, mfu 24.92%\n",
            "iter 470: loss 1.6549, time 14.55ms, mfu 24.99%\n",
            "iter 480: loss 1.6163, time 14.89ms, mfu 24.99%\n",
            "iter 490: loss 1.6028, time 14.56ms, mfu 25.05%\n",
            "step 500: train loss 1.5261, val loss 1.7274\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 500: loss 1.5997, time 3485.06ms, mfu 22.56%\n",
            "iter 510: loss 1.6078, time 15.12ms, mfu 22.77%\n",
            "iter 520: loss 1.5933, time 15.22ms, mfu 22.94%\n",
            "iter 530: loss 1.5625, time 16.86ms, mfu 22.86%\n",
            "iter 540: loss 1.6108, time 14.94ms, mfu 23.06%\n",
            "iter 550: loss 1.5605, time 14.90ms, mfu 23.26%\n",
            "iter 560: loss 1.5638, time 15.00ms, mfu 23.42%\n",
            "iter 570: loss 1.5632, time 14.90ms, mfu 23.58%\n",
            "iter 580: loss 1.5269, time 14.72ms, mfu 23.75%\n",
            "iter 590: loss 1.4947, time 14.81ms, mfu 23.89%\n",
            "iter 600: loss 1.5152, time 14.80ms, mfu 24.02%\n",
            "iter 610: loss 1.5420, time 14.91ms, mfu 24.12%\n",
            "iter 620: loss 1.5287, time 14.82ms, mfu 24.22%\n",
            "iter 630: loss 1.5065, time 14.75ms, mfu 24.32%\n",
            "iter 640: loss 1.4648, time 14.76ms, mfu 24.42%\n",
            "iter 650: loss 1.4991, time 14.68ms, mfu 24.51%\n",
            "iter 660: loss 1.5039, time 15.03ms, mfu 24.54%\n",
            "iter 670: loss 1.4422, time 14.72ms, mfu 24.62%\n",
            "iter 680: loss 1.5062, time 14.72ms, mfu 24.69%\n",
            "iter 690: loss 1.4563, time 15.12ms, mfu 24.68%\n",
            "iter 700: loss 1.4783, time 14.86ms, mfu 24.72%\n",
            "iter 710: loss 1.4524, time 14.84ms, mfu 24.76%\n",
            "iter 720: loss 1.4410, time 14.77ms, mfu 24.81%\n",
            "iter 730: loss 1.4177, time 14.83ms, mfu 24.84%\n",
            "iter 740: loss 1.4246, time 14.81ms, mfu 24.87%\n",
            "step 750: train loss 1.3602, val loss 1.5905\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 750: loss 1.4170, time 3476.12ms, mfu 22.40%\n",
            "iter 760: loss 1.4356, time 14.72ms, mfu 22.69%\n",
            "iter 770: loss 1.4287, time 14.73ms, mfu 22.95%\n",
            "iter 780: loss 1.4202, time 14.65ms, mfu 23.20%\n",
            "iter 790: loss 1.4117, time 14.71ms, mfu 23.41%\n",
            "iter 800: loss 1.4227, time 14.74ms, mfu 23.60%\n",
            "iter 810: loss 1.4041, time 14.96ms, mfu 23.73%\n",
            "iter 820: loss 1.4070, time 14.69ms, mfu 23.89%\n",
            "iter 830: loss 1.3797, time 14.74ms, mfu 24.03%\n",
            "iter 840: loss 1.3907, time 14.68ms, mfu 24.17%\n",
            "iter 850: loss 1.3894, time 14.90ms, mfu 24.25%\n",
            "iter 860: loss 1.3915, time 14.74ms, mfu 24.35%\n",
            "iter 870: loss 1.3925, time 14.83ms, mfu 24.43%\n",
            "iter 880: loss 1.3697, time 14.88ms, mfu 24.49%\n",
            "iter 890: loss 1.3818, time 14.75ms, mfu 24.57%\n",
            "iter 900: loss 1.3660, time 14.70ms, mfu 24.65%\n",
            "iter 910: loss 1.3177, time 15.09ms, mfu 24.65%\n",
            "iter 920: loss 1.3607, time 14.79ms, mfu 24.71%\n",
            "iter 930: loss 1.3489, time 16.23ms, mfu 24.53%\n",
            "iter 940: loss 1.3441, time 15.66ms, mfu 24.46%\n",
            "iter 950: loss 1.3536, time 14.97ms, mfu 24.50%\n",
            "iter 960: loss 1.3633, time 16.94ms, mfu 24.25%\n",
            "iter 970: loss 1.3520, time 16.81ms, mfu 24.04%\n",
            "iter 980: loss 1.3526, time 15.51ms, mfu 24.04%\n",
            "iter 990: loss 1.3388, time 16.98ms, mfu 23.83%\n",
            "step 1000: train loss 1.2709, val loss 1.5226\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1000: loss 1.3341, time 3468.95ms, mfu 21.46%\n",
            "iter 1010: loss 1.3378, time 15.05ms, mfu 21.79%\n",
            "iter 1020: loss 1.3086, time 14.88ms, mfu 22.11%\n",
            "iter 1030: loss 1.3268, time 14.78ms, mfu 22.42%\n",
            "iter 1040: loss 1.3581, time 14.80ms, mfu 22.70%\n",
            "iter 1050: loss 1.2928, time 14.81ms, mfu 22.94%\n",
            "iter 1060: loss 1.3326, time 14.90ms, mfu 23.15%\n",
            "iter 1070: loss 1.3279, time 14.55ms, mfu 23.40%\n",
            "iter 1080: loss 1.3359, time 14.94ms, mfu 23.55%\n",
            "iter 1090: loss 1.3422, time 15.40ms, mfu 23.62%\n",
            "iter 1100: loss 1.3177, time 14.82ms, mfu 23.77%\n",
            "iter 1110: loss 1.2914, time 14.75ms, mfu 23.92%\n",
            "iter 1120: loss 1.2912, time 14.84ms, mfu 24.04%\n",
            "iter 1130: loss 1.2828, time 14.71ms, mfu 24.17%\n",
            "iter 1140: loss 1.2970, time 15.34ms, mfu 24.18%\n",
            "iter 1150: loss 1.3048, time 14.71ms, mfu 24.29%\n",
            "iter 1160: loss 1.3236, time 15.08ms, mfu 24.34%\n",
            "iter 1170: loss 1.2910, time 15.09ms, mfu 24.37%\n",
            "iter 1180: loss 1.3076, time 14.75ms, mfu 24.46%\n",
            "iter 1190: loss 1.2681, time 14.88ms, mfu 24.52%\n",
            "iter 1200: loss 1.2878, time 14.69ms, mfu 24.60%\n",
            "iter 1210: loss 1.2557, time 14.94ms, mfu 24.64%\n",
            "iter 1220: loss 1.3061, time 14.79ms, mfu 24.69%\n",
            "iter 1230: loss 1.2907, time 15.58ms, mfu 24.62%\n",
            "iter 1240: loss 1.2982, time 15.52ms, mfu 24.56%\n",
            "step 1250: train loss 1.2002, val loss 1.4952\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1250: loss 1.2685, time 3488.99ms, mfu 22.11%\n",
            "iter 1260: loss 1.2799, time 14.60ms, mfu 22.45%\n",
            "iter 1270: loss 1.2593, time 15.23ms, mfu 22.65%\n",
            "iter 1280: loss 1.2534, time 14.92ms, mfu 22.89%\n",
            "iter 1290: loss 1.2832, time 15.20ms, mfu 23.05%\n",
            "iter 1300: loss 1.2979, time 15.20ms, mfu 23.19%\n",
            "iter 1310: loss 1.2374, time 15.06ms, mfu 23.35%\n",
            "iter 1320: loss 1.3018, time 14.86ms, mfu 23.52%\n",
            "iter 1330: loss 1.2599, time 14.77ms, mfu 23.69%\n",
            "iter 1340: loss 1.3009, time 14.80ms, mfu 23.84%\n",
            "iter 1350: loss 1.2574, time 15.13ms, mfu 23.92%\n",
            "iter 1360: loss 1.2758, time 14.99ms, mfu 24.01%\n",
            "iter 1370: loss 1.2473, time 15.08ms, mfu 24.08%\n",
            "iter 1380: loss 1.2563, time 14.98ms, mfu 24.16%\n",
            "iter 1390: loss 1.2436, time 15.01ms, mfu 24.23%\n",
            "iter 1400: loss 1.2575, time 15.03ms, mfu 24.29%\n",
            "iter 1410: loss 1.2453, time 14.84ms, mfu 24.37%\n",
            "iter 1420: loss 1.2693, time 14.67ms, mfu 24.47%\n",
            "iter 1430: loss 1.2404, time 14.87ms, mfu 24.53%\n",
            "iter 1440: loss 1.2428, time 14.73ms, mfu 24.61%\n",
            "iter 1450: loss 1.2244, time 14.67ms, mfu 24.68%\n",
            "iter 1460: loss 1.2402, time 14.67ms, mfu 24.76%\n",
            "iter 1470: loss 1.2163, time 14.63ms, mfu 24.83%\n",
            "iter 1480: loss 1.2089, time 15.19ms, mfu 24.80%\n",
            "iter 1490: loss 1.2345, time 15.01ms, mfu 24.80%\n",
            "step 1500: train loss 1.1506, val loss 1.4772\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1500: loss 1.1838, time 3468.66ms, mfu 22.33%\n",
            "iter 1510: loss 1.2323, time 15.41ms, mfu 22.52%\n",
            "iter 1520: loss 1.2260, time 14.74ms, mfu 22.79%\n",
            "iter 1530: loss 1.2509, time 16.22ms, mfu 22.81%\n",
            "iter 1540: loss 1.1910, time 14.78ms, mfu 23.05%\n",
            "iter 1550: loss 1.2350, time 14.76ms, mfu 23.27%\n",
            "iter 1560: loss 1.2047, time 14.74ms, mfu 23.47%\n",
            "iter 1570: loss 1.2305, time 14.86ms, mfu 23.63%\n",
            "iter 1580: loss 1.2006, time 14.81ms, mfu 23.78%\n",
            "iter 1590: loss 1.1915, time 14.83ms, mfu 23.92%\n",
            "iter 1600: loss 1.1974, time 15.62ms, mfu 23.91%\n",
            "iter 1610: loss 1.2317, time 14.77ms, mfu 24.04%\n",
            "iter 1620: loss 1.1827, time 14.63ms, mfu 24.19%\n",
            "iter 1630: loss 1.1973, time 14.91ms, mfu 24.27%\n",
            "iter 1640: loss 1.2022, time 15.04ms, mfu 24.32%\n",
            "iter 1650: loss 1.1833, time 14.89ms, mfu 24.39%\n",
            "iter 1660: loss 1.2185, time 15.31ms, mfu 24.38%\n",
            "iter 1670: loss 1.1983, time 14.76ms, mfu 24.47%\n",
            "iter 1680: loss 1.1946, time 14.79ms, mfu 24.54%\n",
            "iter 1690: loss 1.2013, time 15.39ms, mfu 24.51%\n",
            "iter 1700: loss 1.1874, time 14.70ms, mfu 24.59%\n",
            "iter 1710: loss 1.1768, time 14.93ms, mfu 24.63%\n",
            "iter 1720: loss 1.1843, time 14.69ms, mfu 24.70%\n",
            "iter 1730: loss 1.1975, time 14.70ms, mfu 24.77%\n",
            "iter 1740: loss 1.1666, time 14.80ms, mfu 24.81%\n",
            "step 1750: train loss 1.1039, val loss 1.4653\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1750: loss 1.1858, time 3503.72ms, mfu 22.34%\n",
            "iter 1760: loss 1.1868, time 14.70ms, mfu 22.64%\n",
            "iter 1770: loss 1.1950, time 15.64ms, mfu 22.76%\n",
            "iter 1780: loss 1.1942, time 15.64ms, mfu 22.87%\n",
            "iter 1790: loss 1.1883, time 15.50ms, mfu 22.99%\n",
            "iter 1800: loss 1.1794, time 14.95ms, mfu 23.18%\n",
            "iter 1810: loss 1.1536, time 14.87ms, mfu 23.37%\n",
            "iter 1820: loss 1.1694, time 14.85ms, mfu 23.54%\n",
            "iter 1830: loss 1.1679, time 15.51ms, mfu 23.59%\n",
            "iter 1840: loss 1.1636, time 14.82ms, mfu 23.74%\n",
            "iter 1850: loss 1.1621, time 15.25ms, mfu 23.81%\n",
            "iter 1860: loss 1.1795, time 14.89ms, mfu 23.93%\n",
            "iter 1870: loss 1.1444, time 14.87ms, mfu 24.05%\n",
            "iter 1880: loss 1.1795, time 14.84ms, mfu 24.15%\n",
            "iter 1890: loss 1.1787, time 14.83ms, mfu 24.25%\n",
            "iter 1900: loss 1.1306, time 14.87ms, mfu 24.33%\n",
            "iter 1910: loss 1.1648, time 17.40ms, mfu 24.04%\n",
            "iter 1920: loss 1.1730, time 15.00ms, mfu 24.12%\n",
            "iter 1930: loss 1.1394, time 14.88ms, mfu 24.21%\n",
            "iter 1940: loss 1.1201, time 14.87ms, mfu 24.30%\n",
            "iter 1950: loss 1.1391, time 14.84ms, mfu 24.38%\n",
            "iter 1960: loss 1.1543, time 14.84ms, mfu 24.45%\n",
            "iter 1970: loss 1.1531, time 14.88ms, mfu 24.51%\n",
            "iter 1980: loss 1.1503, time 14.78ms, mfu 24.58%\n",
            "iter 1990: loss 1.1579, time 14.86ms, mfu 24.63%\n",
            "step 2000: train loss 1.0571, val loss 1.4774\n",
            "iter 2000: loss 1.1268, time 3239.03ms, mfu 22.18%\n",
            "iter 2010: loss 1.1298, time 14.87ms, mfu 22.47%\n",
            "iter 2020: loss 1.1219, time 15.00ms, mfu 22.70%\n",
            "iter 2030: loss 1.1584, time 14.84ms, mfu 22.94%\n",
            "iter 2040: loss 1.1479, time 14.91ms, mfu 23.15%\n",
            "iter 2050: loss 1.1084, time 15.09ms, mfu 23.30%\n",
            "iter 2060: loss 1.1004, time 14.98ms, mfu 23.46%\n",
            "iter 2070: loss 1.1237, time 14.94ms, mfu 23.61%\n",
            "iter 2080: loss 1.1232, time 14.86ms, mfu 23.75%\n",
            "iter 2090: loss 1.1287, time 15.31ms, mfu 23.81%\n",
            "iter 2100: loss 1.1297, time 14.53ms, mfu 24.00%\n",
            "iter 2110: loss 1.1295, time 14.43ms, mfu 24.18%\n",
            "iter 2120: loss 1.1312, time 14.42ms, mfu 24.34%\n",
            "iter 2130: loss 1.1385, time 15.77ms, mfu 24.27%\n",
            "iter 2140: loss 1.1337, time 14.95ms, mfu 24.34%\n",
            "iter 2150: loss 1.1170, time 14.62ms, mfu 24.45%\n",
            "iter 2160: loss 1.1473, time 14.70ms, mfu 24.54%\n",
            "iter 2170: loss 1.1315, time 14.47ms, mfu 24.66%\n",
            "iter 2180: loss 1.1099, time 15.04ms, mfu 24.67%\n",
            "iter 2190: loss 1.1052, time 14.81ms, mfu 24.72%\n",
            "iter 2200: loss 1.1250, time 15.78ms, mfu 24.61%\n",
            "iter 2210: loss 1.1120, time 16.13ms, mfu 24.46%\n",
            "iter 2220: loss 1.1191, time 14.79ms, mfu 24.53%\n",
            "iter 2230: loss 1.1165, time 15.03ms, mfu 24.56%\n",
            "iter 2240: loss 1.1332, time 14.71ms, mfu 24.64%\n",
            "step 2250: train loss 1.0100, val loss 1.4823\n",
            "iter 2250: loss 1.1089, time 3244.16ms, mfu 22.18%\n",
            "iter 2260: loss 1.1072, time 15.15ms, mfu 22.43%\n",
            "iter 2270: loss 1.1278, time 14.59ms, mfu 22.74%\n",
            "iter 2280: loss 1.0936, time 14.53ms, mfu 23.03%\n",
            "iter 2290: loss 1.1449, time 14.67ms, mfu 23.27%\n",
            "iter 2300: loss 1.1223, time 15.21ms, mfu 23.39%\n",
            "iter 2310: loss 1.0943, time 14.63ms, mfu 23.60%\n",
            "iter 2320: loss 1.0949, time 14.91ms, mfu 23.74%\n",
            "iter 2330: loss 1.1064, time 14.64ms, mfu 23.91%\n",
            "iter 2340: loss 1.1172, time 14.79ms, mfu 24.04%\n",
            "iter 2350: loss 1.1089, time 14.59ms, mfu 24.19%\n",
            "iter 2360: loss 1.1051, time 15.24ms, mfu 24.21%\n",
            "iter 2370: loss 1.0929, time 14.81ms, mfu 24.31%\n",
            "iter 2380: loss 1.0801, time 14.84ms, mfu 24.39%\n",
            "iter 2390: loss 1.0858, time 14.72ms, mfu 24.48%\n",
            "iter 2400: loss 1.0747, time 14.53ms, mfu 24.60%\n",
            "iter 2410: loss 1.0696, time 14.64ms, mfu 24.68%\n",
            "iter 2420: loss 1.0837, time 14.89ms, mfu 24.72%\n",
            "iter 2430: loss 1.0569, time 14.60ms, mfu 24.80%\n",
            "iter 2440: loss 1.0546, time 14.75ms, mfu 24.84%\n",
            "iter 2450: loss 1.0808, time 14.55ms, mfu 24.92%\n",
            "iter 2460: loss 1.0853, time 14.46ms, mfu 25.00%\n",
            "iter 2470: loss 1.0846, time 14.57ms, mfu 25.06%\n",
            "iter 2480: loss 1.0760, time 14.80ms, mfu 25.07%\n",
            "iter 2490: loss 1.0542, time 14.54ms, mfu 25.13%\n",
            "step 2500: train loss 0.9622, val loss 1.4874\n",
            "iter 2500: loss 1.0857, time 3235.59ms, mfu 22.63%\n",
            "iter 2510: loss 1.0747, time 14.70ms, mfu 22.90%\n",
            "iter 2520: loss 1.0461, time 14.54ms, mfu 23.17%\n",
            "iter 2530: loss 1.0479, time 15.04ms, mfu 23.33%\n",
            "iter 2540: loss 1.0566, time 15.01ms, mfu 23.48%\n",
            "iter 2550: loss 1.0715, time 14.97ms, mfu 23.62%\n",
            "iter 2560: loss 1.0610, time 14.67ms, mfu 23.80%\n",
            "iter 2570: loss 1.0807, time 14.74ms, mfu 23.95%\n",
            "iter 2580: loss 1.0819, time 14.62ms, mfu 24.10%\n",
            "iter 2590: loss 1.0701, time 15.14ms, mfu 24.15%\n",
            "iter 2600: loss 1.0604, time 14.63ms, mfu 24.29%\n",
            "iter 2610: loss 1.0492, time 14.88ms, mfu 24.36%\n",
            "iter 2620: loss 1.0408, time 14.98ms, mfu 24.41%\n",
            "iter 2630: loss 1.0299, time 14.48ms, mfu 24.54%\n",
            "iter 2640: loss 1.0396, time 14.59ms, mfu 24.64%\n",
            "iter 2650: loss 1.0693, time 14.60ms, mfu 24.73%\n",
            "iter 2660: loss 1.0369, time 14.98ms, mfu 24.75%\n",
            "iter 2670: loss 1.0197, time 14.60ms, mfu 24.82%\n",
            "iter 2680: loss 1.0490, time 14.56ms, mfu 24.90%\n",
            "iter 2690: loss 1.0629, time 14.67ms, mfu 24.95%\n",
            "iter 2700: loss 1.0209, time 14.63ms, mfu 25.00%\n",
            "iter 2710: loss 1.0478, time 15.54ms, mfu 24.90%\n",
            "iter 2720: loss 1.0483, time 14.59ms, mfu 24.96%\n",
            "iter 2730: loss 1.0652, time 15.12ms, mfu 24.93%\n",
            "iter 2740: loss 1.0295, time 15.35ms, mfu 24.87%\n",
            "step 2750: train loss 0.9153, val loss 1.5051\n",
            "iter 2750: loss 1.0408, time 3242.54ms, mfu 22.39%\n",
            "iter 2760: loss 1.0381, time 14.54ms, mfu 22.72%\n",
            "iter 2770: loss 1.0284, time 14.62ms, mfu 22.99%\n",
            "iter 2780: loss 1.0312, time 14.48ms, mfu 23.27%\n",
            "iter 2790: loss 1.0356, time 19.15ms, mfu 22.89%\n",
            "iter 2800: loss 1.0148, time 14.88ms, mfu 23.10%\n",
            "iter 2810: loss 1.0396, time 14.82ms, mfu 23.31%\n",
            "iter 2820: loss 1.0238, time 15.24ms, mfu 23.42%\n",
            "iter 2830: loss 1.0335, time 14.71ms, mfu 23.61%\n",
            "iter 2840: loss 0.9966, time 14.93ms, mfu 23.75%\n",
            "iter 2850: loss 1.0255, time 14.65ms, mfu 23.91%\n",
            "iter 2860: loss 1.0221, time 14.72ms, mfu 24.05%\n",
            "iter 2870: loss 1.0071, time 14.56ms, mfu 24.21%\n",
            "iter 2880: loss 1.0243, time 14.76ms, mfu 24.31%\n",
            "iter 2890: loss 1.0173, time 15.07ms, mfu 24.35%\n",
            "iter 2900: loss 0.9976, time 14.93ms, mfu 24.41%\n",
            "iter 2910: loss 1.0394, time 14.57ms, mfu 24.53%\n",
            "iter 2920: loss 1.0109, time 14.72ms, mfu 24.61%\n",
            "iter 2930: loss 0.9982, time 14.57ms, mfu 24.71%\n",
            "iter 2940: loss 0.9958, time 15.09ms, mfu 24.70%\n",
            "iter 2950: loss 1.0247, time 14.67ms, mfu 24.77%\n",
            "iter 2960: loss 1.0041, time 15.30ms, mfu 24.73%\n",
            "iter 2970: loss 0.9863, time 15.39ms, mfu 24.68%\n",
            "iter 2980: loss 0.9979, time 14.75ms, mfu 24.74%\n",
            "iter 2990: loss 0.9863, time 14.76ms, mfu 24.79%\n",
            "step 3000: train loss 0.8676, val loss 1.5157\n",
            "iter 3000: loss 0.9874, time 3250.29ms, mfu 22.32%\n",
            "iter 3010: loss 0.9921, time 14.52ms, mfu 22.66%\n",
            "iter 3020: loss 1.0012, time 14.62ms, mfu 22.94%\n",
            "iter 3030: loss 1.0051, time 14.59ms, mfu 23.20%\n",
            "iter 3040: loss 1.0241, time 14.99ms, mfu 23.37%\n",
            "iter 3050: loss 0.9811, time 14.58ms, mfu 23.58%\n",
            "iter 3060: loss 0.9955, time 14.97ms, mfu 23.72%\n",
            "iter 3070: loss 1.0200, time 14.68ms, mfu 23.88%\n",
            "iter 3080: loss 0.9891, time 14.53ms, mfu 24.06%\n",
            "iter 3090: loss 0.9809, time 14.64ms, mfu 24.20%\n",
            "iter 3100: loss 0.9896, time 14.75ms, mfu 24.30%\n",
            "iter 3110: loss 0.9713, time 15.03ms, mfu 24.35%\n",
            "iter 3120: loss 0.9981, time 14.65ms, mfu 24.46%\n",
            "iter 3130: loss 0.9751, time 14.75ms, mfu 24.54%\n",
            "iter 3140: loss 0.9833, time 14.88ms, mfu 24.59%\n",
            "iter 3150: loss 0.9997, time 14.64ms, mfu 24.68%\n",
            "iter 3160: loss 1.0022, time 14.73ms, mfu 24.74%\n",
            "iter 3170: loss 0.9598, time 14.85ms, mfu 24.77%\n",
            "iter 3180: loss 0.9814, time 14.65ms, mfu 24.84%\n",
            "iter 3190: loss 0.9983, time 14.79ms, mfu 24.88%\n",
            "iter 3200: loss 0.9620, time 14.92ms, mfu 24.89%\n",
            "iter 3210: loss 0.9668, time 15.22ms, mfu 24.85%\n",
            "iter 3220: loss 0.9658, time 14.63ms, mfu 24.91%\n",
            "iter 3230: loss 0.9588, time 14.81ms, mfu 24.93%\n",
            "iter 3240: loss 0.9680, time 14.79ms, mfu 24.96%\n",
            "step 3250: train loss 0.8253, val loss 1.5610\n",
            "iter 3250: loss 0.9683, time 3244.43ms, mfu 22.47%\n",
            "iter 3260: loss 0.9610, time 14.55ms, mfu 22.79%\n",
            "iter 3270: loss 0.9718, time 14.82ms, mfu 23.02%\n",
            "iter 3280: loss 0.9469, time 14.63ms, mfu 23.27%\n",
            "iter 3290: loss 0.9456, time 14.85ms, mfu 23.45%\n",
            "iter 3300: loss 0.9492, time 15.93ms, mfu 23.44%\n",
            "iter 3310: loss 0.9578, time 14.92ms, mfu 23.60%\n",
            "iter 3320: loss 0.9667, time 15.06ms, mfu 23.71%\n",
            "iter 3330: loss 0.9607, time 14.87ms, mfu 23.85%\n",
            "iter 3340: loss 0.9559, time 14.75ms, mfu 23.99%\n",
            "iter 3350: loss 0.9547, time 14.76ms, mfu 24.12%\n",
            "iter 3360: loss 0.9358, time 14.68ms, mfu 24.24%\n",
            "iter 3370: loss 0.9594, time 14.50ms, mfu 24.39%\n",
            "iter 3380: loss 0.9460, time 14.66ms, mfu 24.49%\n",
            "iter 3390: loss 0.9513, time 15.11ms, mfu 24.51%\n",
            "iter 3400: loss 0.9506, time 14.76ms, mfu 24.58%\n",
            "iter 3410: loss 0.9539, time 15.50ms, mfu 24.53%\n",
            "iter 3420: loss 0.9482, time 16.28ms, mfu 24.36%\n",
            "iter 3430: loss 0.9484, time 14.72ms, mfu 24.46%\n",
            "iter 3440: loss 0.9771, time 14.94ms, mfu 24.51%\n",
            "iter 3450: loss 0.9574, time 14.64ms, mfu 24.60%\n",
            "iter 3460: loss 0.9468, time 14.55ms, mfu 24.70%\n",
            "iter 3470: loss 0.9416, time 14.76ms, mfu 24.75%\n",
            "iter 3480: loss 0.9521, time 14.82ms, mfu 24.79%\n",
            "iter 3490: loss 0.9218, time 14.60ms, mfu 24.87%\n",
            "step 3500: train loss 0.7856, val loss 1.5704\n",
            "iter 3500: loss 0.9071, time 3244.34ms, mfu 22.39%\n",
            "iter 3510: loss 0.9220, time 14.85ms, mfu 22.66%\n",
            "iter 3520: loss 0.9291, time 15.38ms, mfu 22.82%\n",
            "iter 3530: loss 0.9542, time 14.69ms, mfu 23.07%\n",
            "iter 3540: loss 0.9339, time 14.53ms, mfu 23.33%\n",
            "iter 3550: loss 0.9298, time 14.67ms, mfu 23.54%\n",
            "iter 3560: loss 0.9542, time 14.91ms, mfu 23.68%\n",
            "iter 3570: loss 0.9311, time 14.66ms, mfu 23.85%\n",
            "iter 3580: loss 0.9351, time 15.41ms, mfu 23.89%\n",
            "iter 3590: loss 0.9274, time 14.91ms, mfu 24.00%\n",
            "iter 3600: loss 0.9210, time 14.60ms, mfu 24.15%\n",
            "iter 3610: loss 0.9225, time 14.47ms, mfu 24.31%\n",
            "iter 3620: loss 0.9088, time 15.20ms, mfu 24.33%\n",
            "iter 3630: loss 0.9362, time 14.97ms, mfu 24.39%\n",
            "iter 3640: loss 0.9226, time 14.58ms, mfu 24.51%\n",
            "iter 3650: loss 0.9138, time 14.77ms, mfu 24.58%\n",
            "iter 3660: loss 0.9401, time 14.63ms, mfu 24.67%\n",
            "iter 3670: loss 0.9444, time 14.55ms, mfu 24.76%\n",
            "iter 3680: loss 0.9100, time 14.82ms, mfu 24.80%\n",
            "iter 3690: loss 0.9337, time 14.66ms, mfu 24.86%\n",
            "iter 3700: loss 0.8752, time 15.78ms, mfu 24.74%\n",
            "iter 3710: loss 0.8846, time 15.30ms, mfu 24.70%\n",
            "iter 3720: loss 0.9072, time 14.83ms, mfu 24.74%\n",
            "iter 3730: loss 0.9040, time 15.07ms, mfu 24.74%\n",
            "iter 3740: loss 0.9073, time 16.26ms, mfu 24.56%\n",
            "step 3750: train loss 0.7457, val loss 1.6019\n",
            "iter 3750: loss 0.9092, time 3260.73ms, mfu 22.11%\n",
            "iter 3760: loss 0.9384, time 15.24ms, mfu 22.35%\n",
            "iter 3770: loss 0.9384, time 14.57ms, mfu 22.67%\n",
            "iter 3780: loss 0.9212, time 14.70ms, mfu 22.94%\n",
            "iter 3790: loss 0.9069, time 14.57ms, mfu 23.20%\n",
            "iter 3800: loss 0.9186, time 14.55ms, mfu 23.44%\n",
            "iter 3810: loss 0.9263, time 14.63ms, mfu 23.64%\n",
            "iter 3820: loss 0.8866, time 14.43ms, mfu 23.86%\n",
            "iter 3830: loss 0.9047, time 14.53ms, mfu 24.04%\n",
            "iter 3840: loss 0.8864, time 14.93ms, mfu 24.13%\n",
            "iter 3850: loss 0.8919, time 14.73ms, mfu 24.25%\n",
            "iter 3860: loss 0.8760, time 14.93ms, mfu 24.32%\n",
            "iter 3870: loss 0.8968, time 14.75ms, mfu 24.41%\n",
            "iter 3880: loss 0.8931, time 14.51ms, mfu 24.54%\n",
            "iter 3890: loss 0.8912, time 15.47ms, mfu 24.50%\n",
            "iter 3900: loss 0.8899, time 14.57ms, mfu 24.60%\n",
            "iter 3910: loss 0.8879, time 14.55ms, mfu 24.70%\n",
            "iter 3920: loss 0.8763, time 15.07ms, mfu 24.71%\n",
            "iter 3930: loss 0.8993, time 14.45ms, mfu 24.81%\n",
            "iter 3940: loss 0.8846, time 14.39ms, mfu 24.92%\n",
            "iter 3950: loss 0.8832, time 15.48ms, mfu 24.84%\n",
            "iter 3960: loss 0.9146, time 14.56ms, mfu 24.91%\n",
            "iter 3970: loss 0.8987, time 14.54ms, mfu 24.98%\n",
            "iter 3980: loss 0.9004, time 14.61ms, mfu 25.03%\n",
            "iter 3990: loss 0.8823, time 14.67ms, mfu 25.07%\n",
            "step 4000: train loss 0.7152, val loss 1.6218\n",
            "iter 4000: loss 0.8531, time 3239.77ms, mfu 22.58%\n",
            "iter 4010: loss 0.8894, time 14.72ms, mfu 22.85%\n",
            "iter 4020: loss 0.8884, time 15.29ms, mfu 23.00%\n",
            "iter 4030: loss 0.8902, time 14.44ms, mfu 23.28%\n",
            "iter 4040: loss 0.8829, time 14.44ms, mfu 23.53%\n",
            "iter 4050: loss 0.8762, time 14.40ms, mfu 23.77%\n",
            "iter 4060: loss 0.8652, time 14.49ms, mfu 23.96%\n",
            "iter 4070: loss 0.8668, time 14.94ms, mfu 24.06%\n",
            "iter 4080: loss 0.8874, time 14.45ms, mfu 24.24%\n",
            "iter 4090: loss 0.8554, time 14.48ms, mfu 24.39%\n",
            "iter 4100: loss 0.9030, time 14.36ms, mfu 24.54%\n",
            "iter 4110: loss 0.8763, time 14.74ms, mfu 24.61%\n",
            "iter 4120: loss 0.8803, time 14.61ms, mfu 24.70%\n",
            "iter 4130: loss 0.8655, time 14.53ms, mfu 24.80%\n",
            "iter 4140: loss 0.8806, time 14.90ms, mfu 24.82%\n",
            "iter 4150: loss 0.8695, time 14.52ms, mfu 24.90%\n",
            "iter 4160: loss 0.8515, time 14.60ms, mfu 24.97%\n",
            "iter 4170: loss 0.8727, time 16.16ms, mfu 24.77%\n",
            "iter 4180: loss 0.8674, time 15.27ms, mfu 24.74%\n",
            "iter 4190: loss 0.8814, time 14.54ms, mfu 24.83%\n",
            "iter 4200: loss 0.8675, time 14.48ms, mfu 24.92%\n",
            "iter 4210: loss 0.8753, time 16.31ms, mfu 24.71%\n",
            "iter 4220: loss 0.8680, time 14.61ms, mfu 24.79%\n",
            "iter 4230: loss 0.8852, time 14.65ms, mfu 24.86%\n",
            "iter 4240: loss 0.8753, time 14.47ms, mfu 24.95%\n",
            "step 4250: train loss 0.6840, val loss 1.6456\n",
            "iter 4250: loss 0.8766, time 3245.31ms, mfu 22.46%\n",
            "iter 4260: loss 0.8635, time 14.92ms, mfu 22.71%\n",
            "iter 4270: loss 0.8622, time 14.88ms, mfu 22.95%\n",
            "iter 4280: loss 0.8649, time 14.84ms, mfu 23.16%\n",
            "iter 4290: loss 0.8351, time 15.07ms, mfu 23.32%\n",
            "iter 4300: loss 0.8393, time 14.95ms, mfu 23.48%\n",
            "iter 4310: loss 0.8556, time 15.01ms, mfu 23.61%\n",
            "iter 4320: loss 0.8342, time 14.92ms, mfu 23.75%\n",
            "iter 4330: loss 0.8636, time 14.95ms, mfu 23.87%\n",
            "iter 4340: loss 0.8420, time 15.26ms, mfu 23.92%\n",
            "iter 4350: loss 0.8428, time 15.33ms, mfu 23.96%\n",
            "iter 4360: loss 0.8633, time 14.90ms, mfu 24.07%\n",
            "iter 4370: loss 0.8565, time 14.87ms, mfu 24.16%\n",
            "iter 4380: loss 0.8373, time 14.96ms, mfu 24.24%\n",
            "iter 4390: loss 0.8744, time 14.99ms, mfu 24.30%\n",
            "iter 4400: loss 0.8464, time 15.04ms, mfu 24.35%\n",
            "iter 4410: loss 0.8626, time 14.98ms, mfu 24.40%\n",
            "iter 4420: loss 0.8705, time 14.93ms, mfu 24.46%\n",
            "iter 4430: loss 0.8452, time 15.00ms, mfu 24.50%\n",
            "iter 4440: loss 0.8484, time 14.92ms, mfu 24.54%\n",
            "iter 4450: loss 0.8560, time 15.13ms, mfu 24.55%\n",
            "iter 4460: loss 0.8472, time 15.07ms, mfu 24.57%\n",
            "iter 4470: loss 0.8531, time 15.08ms, mfu 24.58%\n",
            "iter 4480: loss 0.8259, time 14.89ms, mfu 24.63%\n",
            "iter 4490: loss 0.8456, time 15.03ms, mfu 24.64%\n",
            "step 4500: train loss 0.6604, val loss 1.6586\n",
            "iter 4500: loss 0.8625, time 3241.95ms, mfu 22.19%\n",
            "iter 4510: loss 0.8397, time 15.43ms, mfu 22.39%\n",
            "iter 4520: loss 0.8402, time 14.85ms, mfu 22.66%\n",
            "iter 4530: loss 0.8523, time 15.04ms, mfu 22.87%\n",
            "iter 4540: loss 0.8414, time 14.75ms, mfu 23.11%\n",
            "iter 4550: loss 0.8799, time 15.67ms, mfu 23.18%\n",
            "iter 4560: loss 0.8436, time 14.98ms, mfu 23.35%\n",
            "iter 4570: loss 0.8392, time 14.65ms, mfu 23.55%\n",
            "iter 4580: loss 0.8567, time 14.62ms, mfu 23.75%\n",
            "iter 4590: loss 0.8629, time 15.06ms, mfu 23.85%\n",
            "iter 4600: loss 0.8383, time 14.73ms, mfu 23.99%\n",
            "iter 4610: loss 0.8719, time 15.07ms, mfu 24.07%\n",
            "iter 4620: loss 0.8313, time 15.94ms, mfu 24.00%\n",
            "iter 4630: loss 0.8323, time 15.36ms, mfu 24.02%\n",
            "iter 4640: loss 0.8483, time 14.76ms, mfu 24.15%\n",
            "iter 4650: loss 0.8626, time 15.29ms, mfu 24.17%\n",
            "iter 4660: loss 0.8551, time 14.73ms, mfu 24.28%\n",
            "iter 4670: loss 0.8495, time 15.81ms, mfu 24.21%\n",
            "iter 4680: loss 0.8581, time 14.71ms, mfu 24.32%\n",
            "iter 4690: loss 0.8472, time 14.95ms, mfu 24.38%\n",
            "iter 4700: loss 0.8318, time 16.00ms, mfu 24.27%\n",
            "iter 4710: loss 0.7918, time 15.15ms, mfu 24.31%\n",
            "iter 4720: loss 0.8356, time 14.74ms, mfu 24.40%\n",
            "iter 4730: loss 0.8240, time 16.01ms, mfu 24.29%\n",
            "iter 4740: loss 0.8422, time 14.76ms, mfu 24.39%\n",
            "step 4750: train loss 0.6427, val loss 1.6719\n",
            "iter 4750: loss 0.8084, time 3237.40ms, mfu 21.96%\n",
            "iter 4760: loss 0.8234, time 14.63ms, mfu 22.31%\n",
            "iter 4770: loss 0.8112, time 14.80ms, mfu 22.60%\n",
            "iter 4780: loss 0.8140, time 14.94ms, mfu 22.83%\n",
            "iter 4790: loss 0.8389, time 14.80ms, mfu 23.07%\n",
            "iter 4800: loss 0.8280, time 14.66ms, mfu 23.30%\n",
            "iter 4810: loss 0.8493, time 14.72ms, mfu 23.50%\n",
            "iter 4820: loss 0.8254, time 14.70ms, mfu 23.69%\n",
            "iter 4830: loss 0.8278, time 15.79ms, mfu 23.68%\n",
            "iter 4840: loss 0.8407, time 14.84ms, mfu 23.82%\n",
            "iter 4850: loss 0.8205, time 14.67ms, mfu 23.98%\n",
            "iter 4860: loss 0.8295, time 14.59ms, mfu 24.13%\n",
            "iter 4870: loss 0.8198, time 14.76ms, mfu 24.25%\n",
            "iter 4880: loss 0.8431, time 15.07ms, mfu 24.29%\n",
            "iter 4890: loss 0.8226, time 14.67ms, mfu 24.40%\n",
            "iter 4900: loss 0.8127, time 15.04ms, mfu 24.44%\n",
            "iter 4910: loss 0.8295, time 14.78ms, mfu 24.52%\n",
            "iter 4920: loss 0.8171, time 14.66ms, mfu 24.61%\n",
            "iter 4930: loss 0.8107, time 14.69ms, mfu 24.68%\n",
            "iter 4940: loss 0.8048, time 14.82ms, mfu 24.73%\n",
            "iter 4950: loss 0.8250, time 15.06ms, mfu 24.73%\n",
            "iter 4960: loss 0.8310, time 14.81ms, mfu 24.78%\n",
            "iter 4970: loss 0.7918, time 14.76ms, mfu 24.82%\n",
            "iter 4980: loss 0.8028, time 14.69ms, mfu 24.88%\n",
            "iter 4990: loss 0.8296, time 14.75ms, mfu 24.92%\n",
            "step 5000: train loss 0.6298, val loss 1.6942\n",
            "iter 5000: loss 0.8272, time 3251.90ms, mfu 22.44%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 从训练好的模型中采样生成文本\n",
        "!python sample.py --out_dir=out-shakespeare-char"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRtlDFVdv0kG",
        "outputId": "07738ac6-07bf-4cc5-ec32-0696f7edf7a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: out_dir = out-shakespeare-char\n",
            "number of parameters: 10.65M\n",
            "Loading meta from data/shakespeare_char/meta.pkl...\n",
            "\n",
            "\n",
            "ANGELO:\n",
            "And I will be so, but by the boast of allies.\n",
            "\n",
            "ANGELO:\n",
            "\n",
            "ISABELLA:\n",
            "No more better to lay atend it, and none arrizonous\n",
            "to the toftick of the land; and their eyes accessation\n",
            "in the sea-bed bloody of this swelving bosom.\n",
            "\n",
            "ISABELLA:\n",
            "I would the same tyrant to see and a bloody\n",
            "to the world that most noble duke in the story, and have\n",
            "him do proclaim for us: thou thrust for the ground: there is not\n",
            "the people's door nor full of stock in hards. Who hath stooding straight\n",
            "by Romeo force and Nat\n",
            "---------------\n",
            "\n",
            "Men part, I must tell you thee, I am gone.\n",
            "\n",
            "BUCKINGHAM:\n",
            "Not that I may be glad with my grace,\n",
            "That confesses should sound my castle lord.\n",
            "\n",
            "GLOUCESTER:\n",
            "Be not so: it is so to desire over some sound.\n",
            "\n",
            "KING EDWARD IV:\n",
            "What, looks he not? will I he need to the way?\n",
            "\n",
            "Abbot:\n",
            "Here's no word of less but some death will not so deeds.\n",
            "\n",
            "LADY GREY:\n",
            "Nay, by the nointed Captain Blunt, of the king.\n",
            "\n",
            "GLOUCESTER:\n",
            "This it is it better it, but well meet be so.\n",
            "\n",
            "LADY GREY:\n",
            "My lord, I know, would have my youngest he\n",
            "---------------\n",
            "\n",
            "Men that I bore. This is the new-gentle lack\n",
            "To the prize take and notable of this money,\n",
            "And mine own leads till thine own to my tongue\n",
            "And child from the block of his cause in the day,\n",
            "And all in this blame at my breatheir of wives.\n",
            "\n",
            "BUCKINGHAM:\n",
            "I did that stir your Holione mockets myself:\n",
            "My old conscience the Tower, my lord, and I make him for you.\n",
            "But lord Catesby; then will not be rest.\n",
            "\n",
            "CATESBY:\n",
            "The ground that I leave her hence to unto my wife;\n",
            "And, if I have no lie of miserable lies,\n",
            "Sh\n",
            "---------------\n",
            "\n",
            "\n",
            "First Citizen:\n",
            "Ay, but our holy soul.\n",
            "\n",
            "COMINIUS:\n",
            "What can you to content wherefore?\n",
            "\n",
            "MENENIUS:\n",
            "Never course be hath been patient proclaim'd\n",
            "Here many should not sway I with half\n",
            "In this privator, to prove at the harm\n",
            "Of bringing him with his swift against the voice,\n",
            "And that's off, the court of his bones,\n",
            "That he hath but to his power life of his life,\n",
            "And to him would that have said thy poor hand\n",
            "From the son of hot way and hold the heavens,\n",
            "And with his bed good deeds to him when his\n",
            "In his s\n",
            "---------------\n",
            "\n",
            "Be every bell to me:\n",
            "I know your lordship be false, but a friar of man,\n",
            "And that we have reconciled this soon a world.\n",
            "\n",
            "BUCKINGHAM:\n",
            "Tut, we were press'd forth you in this deceit.\n",
            "\n",
            "BUCKINGHAM:\n",
            "No, now you your lord was not upon your highness:\n",
            "If your lord comfort with the man unto your grace\n",
            "Be your advised, and will not put upon you.\n",
            "\n",
            "KING RICHARD III:\n",
            "Good son, my lord, go see here we begun.\n",
            "\n",
            "KING RICHARD III:\n",
            "And then move be a thing as I am,\n",
            "To you do rehear your son, would not see that seem\n",
            "\n",
            "---------------\n",
            "\n",
            "\n",
            "MENENIUS:\n",
            "He's a little for thing to her.\n",
            "\n",
            "CORIOLANUS:\n",
            "Ay, I am a book, believe, and well hang Rome\n",
            "That's a world, that worthy than a from me have ever\n",
            "In an execution's debt.\n",
            "\n",
            "First Senator:\n",
            "Because you for Rome, so I am, for you.\n",
            "\n",
            "Servant:\n",
            "A sorry, a man, that you cannot say, here's\n",
            "A man a rose to the case of your highness,\n",
            "But he shall in the air of our eyes and have no.\n",
            "\n",
            "Second Servingman:\n",
            "Here's Corioli in Phoebus, who can send his\n",
            "to his behalf inhappy there, but one so his\n",
            "arms an is t\n",
            "---------------\n",
            "\n",
            "She was served his blood against the set,\n",
            "And so hardly a sexact of this all,\n",
            "The wholesome shall the are free four and stored\n",
            "That have been drinking in the soldiers,\n",
            "And in the royal lion of my best\n",
            "May be satisfied to my wrongs. Here comes go:\n",
            "Be gone, hark how and for me, sir, where he would\n",
            "My father was like a more vex in this lawful air\n",
            "With a petty person of a whole beggar-hointed bagged,\n",
            "And to make one of ane obedience and powers\n",
            "In arms so of pasting fortunes to the world of that\n",
            "Will\n",
            "---------------\n",
            "\n",
            "lady, who doth kill thee here?\n",
            "\n",
            "KING RICHARD II:\n",
            "You are all plain, you follow strange to see thee.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Yes, my noble lord, beseech your grace and gentle duke;\n",
            "And yet I have deserved with you; yet your thoughts,\n",
            "For when I have last off your eyes in sacred tears.\n",
            "\n",
            "JOHN OF GAUNT:\n",
            "Nor how I am quitter'd with your country's mouth\n",
            "To make your thrown at the proof interchange of my life?\n",
            "So disguised his character and his house come to away,\n",
            "When he should not put up the world of the To\n",
            "---------------\n",
            "\n",
            "\n",
            "LEONTES:\n",
            "The rest I should seem and oppose a prisoner, if the\n",
            "Enducing shall seem for't and so estate of hate\n",
            "The stars of course, so breathed a son, the blood o' the house,\n",
            "Which power the tradicious Edward, and now now,\n",
            "Of whom I wrought with their own manly against borne babes\n",
            "That may be be so to danger and of sailing death;\n",
            "Mistrust grief have upon the fairest thing that was made me\n",
            "His grace to wake so down with their worses,\n",
            "And tears that supply upon your hraldied steeds.\n",
            "\n",
            "Lord Master:\n",
            "\n",
            "---------------\n",
            "\n",
            "How chan how I say, and here should not stand forth,\n",
            "She is a man best fled him.\n",
            "\n",
            "MENENIUS:\n",
            "He shall be not so?\n",
            "\n",
            "VOLUMNIA:\n",
            "Nay, you shall be a thousand consul?\n",
            "\n",
            "VOLUMNIA:\n",
            "You shall not very wanton against home; and who you\n",
            "have of rough in love with her services and fee\n",
            "And love-discontented upon her hands more than in which\n",
            "I have seen 'twash'd on the too vows.\n",
            "\n",
            "CORIOLANUS:\n",
            "What! what, ho! it is that a law is in speaken of face,\n",
            "To be obey'd in the county's power. Pro's the voice,\n",
            "And both the \n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_Cb3Z-wUCTu",
        "outputId": "76493aa0-45f0-4e21-ea00-4deb3cd6d2e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# 选择合适的 n_embd\n",
        "n_embd = 420\n",
        "n_layer = 7\n",
        "head_list = [2, 3, 5, 7]\n",
        "\n",
        "for n_head in head_list:\n",
        "    config_file = f'config/train_shakespeare_char_{n_head}head.py'\n",
        "    # 复制模板\n",
        "    shutil.copy('config/train_shakespeare_char.py', config_file)\n",
        "    # 修改配置文件内容\n",
        "    with open(config_file, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    with open(config_file, 'w') as f:\n",
        "        for line in lines:\n",
        "            if line.strip().startswith('n_layer'):\n",
        "                f.write(f'n_layer = {n_layer}\\n')\n",
        "            elif line.strip().startswith('n_head'):\n",
        "                f.write(f'n_head = {n_head}\\n')\n",
        "            elif line.strip().startswith('n_embd'):\n",
        "                f.write(f'n_embd = {n_embd}\\n')\n",
        "            elif line.strip().startswith('# device ='):\n",
        "                f.write(f'device = \"cuda\"\\n')\n",
        "            elif line.strip().startswith('# compile ='):\n",
        "                f.write(f'compile = False\\n')\n",
        "            else:\n",
        "                f.write(line)\n",
        "    # 训练\n",
        "    print(f'==== 训练 n_head={n_head} ====')\n",
        "    !python train.py {config_file} --device=cuda --compile=False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJq662-Ev3XX",
        "outputId": "5f631f81-08ef-4f3d-aa93-478c95d2a96f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== 训练 n_head=2 ====\n",
            "Overriding config with config/train_shakespeare_char_2head.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 7\n",
            "n_head = 2\n",
            "n_embd = 420\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "device = \"cuda\"\n",
            "compile = False\n",
            "\n",
            "Overriding: device = cuda\n",
            "Overriding: compile = False\n",
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 14.85M\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "num decayed parameter tensors: 30, with 14,952,420 parameters\n",
            "num non-decayed parameter tensors: 15, with 6,300 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.3031, val loss 4.2992\n",
            "iter 0: loss 4.2787, time 6004.38ms, mfu -100.00%\n",
            "iter 10: loss 3.1025, time 34.50ms, mfu 14.94%\n",
            "iter 20: loss 2.6934, time 34.57ms, mfu 14.93%\n",
            "iter 30: loss 2.5713, time 34.41ms, mfu 14.94%\n",
            "iter 40: loss 2.5617, time 34.60ms, mfu 14.93%\n",
            "iter 50: loss 2.5163, time 34.56ms, mfu 14.93%\n",
            "iter 60: loss 2.4832, time 34.76ms, mfu 14.92%\n",
            "iter 70: loss 2.4880, time 34.57ms, mfu 14.92%\n",
            "iter 80: loss 2.4913, time 34.49ms, mfu 14.92%\n",
            "iter 90: loss 2.4633, time 34.50ms, mfu 14.92%\n",
            "iter 100: loss 2.4424, time 34.53ms, mfu 14.92%\n",
            "iter 110: loss 2.4295, time 34.65ms, mfu 14.92%\n",
            "iter 120: loss 2.4089, time 34.53ms, mfu 14.92%\n",
            "iter 130: loss 2.3785, time 35.01ms, mfu 14.90%\n",
            "iter 140: loss 2.3533, time 34.56ms, mfu 14.90%\n",
            "iter 150: loss 2.3254, time 34.54ms, mfu 14.90%\n",
            "iter 160: loss 2.2694, time 34.51ms, mfu 14.91%\n",
            "iter 170: loss 2.1738, time 34.49ms, mfu 14.91%\n",
            "iter 180: loss 2.1700, time 34.57ms, mfu 14.91%\n",
            "iter 190: loss 2.1131, time 34.61ms, mfu 14.91%\n",
            "iter 200: loss 2.1010, time 34.47ms, mfu 14.91%\n",
            "iter 210: loss 2.0557, time 34.52ms, mfu 14.91%\n",
            "iter 220: loss 2.0461, time 34.53ms, mfu 14.91%\n",
            "iter 230: loss 2.0172, time 34.60ms, mfu 14.91%\n",
            "iter 240: loss 1.9868, time 34.49ms, mfu 14.92%\n",
            "step 250: train loss 1.8955, val loss 2.0189\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 250: loss 1.9758, time 5873.21ms, mfu 13.43%\n",
            "iter 260: loss 1.9563, time 34.48ms, mfu 13.58%\n",
            "iter 270: loss 1.9489, time 34.46ms, mfu 13.72%\n",
            "iter 280: loss 1.9015, time 34.45ms, mfu 13.84%\n",
            "iter 290: loss 1.8989, time 34.17ms, mfu 13.97%\n",
            "iter 300: loss 1.8588, time 34.52ms, mfu 14.06%\n",
            "iter 310: loss 1.8850, time 34.54ms, mfu 14.15%\n",
            "iter 320: loss 1.8556, time 34.54ms, mfu 14.23%\n",
            "iter 330: loss 1.8606, time 34.48ms, mfu 14.30%\n",
            "iter 340: loss 1.8286, time 34.50ms, mfu 14.36%\n",
            "iter 350: loss 1.8285, time 34.48ms, mfu 14.42%\n",
            "iter 360: loss 1.8176, time 34.58ms, mfu 14.47%\n",
            "iter 370: loss 1.7697, time 34.48ms, mfu 14.52%\n",
            "iter 380: loss 1.7739, time 34.47ms, mfu 14.56%\n",
            "iter 390: loss 1.7967, time 34.49ms, mfu 14.60%\n",
            "iter 400: loss 1.7749, time 34.41ms, mfu 14.64%\n",
            "iter 410: loss 1.7610, time 34.59ms, mfu 14.66%\n",
            "iter 420: loss 1.7481, time 34.45ms, mfu 14.69%\n",
            "iter 430: loss 1.7563, time 34.50ms, mfu 14.72%\n",
            "iter 440: loss 1.6942, time 34.54ms, mfu 14.74%\n",
            "iter 450: loss 1.7191, time 34.46ms, mfu 14.76%\n",
            "iter 460: loss 1.7201, time 34.36ms, mfu 14.78%\n",
            "iter 470: loss 1.6980, time 34.51ms, mfu 14.80%\n",
            "iter 480: loss 1.6900, time 34.42ms, mfu 14.82%\n",
            "iter 490: loss 1.7135, time 34.46ms, mfu 14.83%\n",
            "step 500: train loss 1.5599, val loss 1.7667\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 500: loss 1.6560, time 5886.65ms, mfu 13.36%\n",
            "iter 510: loss 1.6225, time 34.53ms, mfu 13.51%\n",
            "iter 520: loss 1.6612, time 34.53ms, mfu 13.65%\n",
            "iter 530: loss 1.6330, time 34.34ms, mfu 13.79%\n",
            "iter 540: loss 1.6421, time 34.49ms, mfu 13.90%\n",
            "iter 550: loss 1.6319, time 34.60ms, mfu 14.00%\n",
            "iter 560: loss 1.6150, time 34.45ms, mfu 14.10%\n",
            "iter 570: loss 1.6163, time 34.52ms, mfu 14.18%\n",
            "iter 580: loss 1.5564, time 34.37ms, mfu 14.26%\n",
            "iter 590: loss 1.5766, time 34.54ms, mfu 14.33%\n",
            "iter 600: loss 1.6010, time 34.80ms, mfu 14.38%\n",
            "iter 610: loss 1.5416, time 34.17ms, mfu 14.45%\n",
            "iter 620: loss 1.5770, time 34.34ms, mfu 14.50%\n",
            "iter 630: loss 1.5590, time 34.67ms, mfu 14.54%\n",
            "iter 640: loss 1.5665, time 34.42ms, mfu 14.58%\n",
            "iter 650: loss 1.5299, time 34.55ms, mfu 14.62%\n",
            "iter 660: loss 1.5373, time 34.54ms, mfu 14.65%\n",
            "iter 670: loss 1.5577, time 34.37ms, mfu 14.68%\n",
            "iter 680: loss 1.5530, time 34.52ms, mfu 14.71%\n",
            "iter 690: loss 1.5463, time 34.50ms, mfu 14.73%\n",
            "iter 700: loss 1.5302, time 34.74ms, mfu 14.74%\n",
            "iter 710: loss 1.5276, time 34.49ms, mfu 14.76%\n",
            "iter 720: loss 1.5142, time 34.48ms, mfu 14.78%\n",
            "iter 730: loss 1.5045, time 34.39ms, mfu 14.80%\n",
            "iter 740: loss 1.4791, time 34.49ms, mfu 14.81%\n",
            "step 750: train loss 1.3925, val loss 1.6009\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 750: loss 1.4964, time 5872.07ms, mfu 13.34%\n",
            "iter 760: loss 1.4551, time 34.39ms, mfu 13.51%\n",
            "iter 770: loss 1.4872, time 34.48ms, mfu 13.65%\n",
            "iter 780: loss 1.4608, time 34.55ms, mfu 13.78%\n",
            "iter 790: loss 1.4622, time 34.39ms, mfu 13.90%\n",
            "iter 800: loss 1.4667, time 34.46ms, mfu 14.00%\n",
            "iter 810: loss 1.4528, time 34.56ms, mfu 14.09%\n",
            "iter 820: loss 1.4520, time 34.73ms, mfu 14.17%\n",
            "iter 830: loss 1.4897, time 34.60ms, mfu 14.24%\n",
            "iter 840: loss 1.4474, time 34.53ms, mfu 14.31%\n",
            "iter 850: loss 1.4528, time 34.35ms, mfu 14.38%\n",
            "iter 860: loss 1.4310, time 34.52ms, mfu 14.43%\n",
            "iter 870: loss 1.4401, time 34.62ms, mfu 14.48%\n",
            "iter 880: loss 1.4301, time 34.57ms, mfu 14.52%\n",
            "iter 890: loss 1.4408, time 34.47ms, mfu 14.56%\n",
            "iter 900: loss 1.3952, time 34.41ms, mfu 14.61%\n",
            "iter 910: loss 1.4254, time 34.26ms, mfu 14.65%\n",
            "iter 920: loss 1.3965, time 34.49ms, mfu 14.68%\n",
            "iter 930: loss 1.4152, time 34.68ms, mfu 14.70%\n",
            "iter 940: loss 1.3775, time 34.43ms, mfu 14.72%\n",
            "iter 950: loss 1.4162, time 34.52ms, mfu 14.74%\n",
            "iter 960: loss 1.3305, time 34.49ms, mfu 14.76%\n",
            "iter 970: loss 1.3888, time 34.51ms, mfu 14.78%\n",
            "iter 980: loss 1.4081, time 34.46ms, mfu 14.80%\n",
            "iter 990: loss 1.3746, time 34.57ms, mfu 14.81%\n",
            "step 1000: train loss 1.2944, val loss 1.5364\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1000: loss 1.3749, time 5893.55ms, mfu 13.34%\n",
            "iter 1010: loss 1.3678, time 34.54ms, mfu 13.50%\n",
            "iter 1020: loss 1.3865, time 34.52ms, mfu 13.64%\n",
            "iter 1030: loss 1.3991, time 34.54ms, mfu 13.77%\n",
            "iter 1040: loss 1.3804, time 34.69ms, mfu 13.88%\n",
            "iter 1050: loss 1.3693, time 34.79ms, mfu 13.97%\n",
            "iter 1060: loss 1.3714, time 34.47ms, mfu 14.07%\n",
            "iter 1070: loss 1.3608, time 34.47ms, mfu 14.16%\n",
            "iter 1080: loss 1.3531, time 34.51ms, mfu 14.23%\n",
            "iter 1090: loss 1.3620, time 34.29ms, mfu 14.31%\n",
            "iter 1100: loss 1.3704, time 34.53ms, mfu 14.37%\n",
            "iter 1110: loss 1.3868, time 34.53ms, mfu 14.43%\n",
            "iter 1120: loss 1.3151, time 34.49ms, mfu 14.48%\n",
            "iter 1130: loss 1.3410, time 34.64ms, mfu 14.52%\n",
            "iter 1140: loss 1.3522, time 34.51ms, mfu 14.56%\n",
            "iter 1150: loss 1.3481, time 34.47ms, mfu 14.60%\n",
            "iter 1160: loss 1.3397, time 34.07ms, mfu 14.65%\n",
            "iter 1170: loss 1.3424, time 34.52ms, mfu 14.68%\n",
            "iter 1180: loss 1.3459, time 34.56ms, mfu 14.70%\n",
            "iter 1190: loss 1.3106, time 34.47ms, mfu 14.73%\n",
            "iter 1200: loss 1.3600, time 34.41ms, mfu 14.75%\n",
            "iter 1210: loss 1.3470, time 34.51ms, mfu 14.77%\n",
            "iter 1220: loss 1.3481, time 34.56ms, mfu 14.79%\n",
            "iter 1230: loss 1.3364, time 34.61ms, mfu 14.80%\n",
            "iter 1240: loss 1.3225, time 34.54ms, mfu 14.81%\n",
            "step 1250: train loss 1.2307, val loss 1.4977\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1250: loss 1.3040, time 5896.02ms, mfu 13.34%\n",
            "iter 1260: loss 1.3244, time 34.55ms, mfu 13.49%\n",
            "iter 1270: loss 1.3320, time 34.50ms, mfu 13.64%\n",
            "iter 1280: loss 1.3434, time 34.48ms, mfu 13.77%\n",
            "iter 1290: loss 1.3152, time 34.52ms, mfu 13.88%\n",
            "iter 1300: loss 1.3293, time 34.46ms, mfu 13.99%\n",
            "iter 1310: loss 1.2600, time 34.65ms, mfu 14.08%\n",
            "iter 1320: loss 1.3223, time 34.41ms, mfu 14.17%\n",
            "iter 1330: loss 1.2895, time 34.55ms, mfu 14.24%\n",
            "iter 1340: loss 1.3255, time 34.49ms, mfu 14.31%\n",
            "iter 1350: loss 1.3113, time 34.55ms, mfu 14.37%\n",
            "iter 1360: loss 1.2594, time 34.49ms, mfu 14.43%\n",
            "iter 1370: loss 1.2826, time 34.51ms, mfu 14.48%\n",
            "iter 1380: loss 1.2765, time 34.45ms, mfu 14.53%\n",
            "iter 1390: loss 1.2799, time 34.40ms, mfu 14.57%\n",
            "iter 1400: loss 1.2520, time 34.52ms, mfu 14.61%\n",
            "iter 1410: loss 1.3094, time 34.45ms, mfu 14.64%\n",
            "iter 1420: loss 1.2728, time 34.51ms, mfu 14.67%\n",
            "iter 1430: loss 1.3009, time 34.56ms, mfu 14.70%\n",
            "iter 1440: loss 1.2489, time 34.53ms, mfu 14.72%\n",
            "iter 1450: loss 1.2805, time 34.61ms, mfu 14.74%\n",
            "iter 1460: loss 1.2995, time 34.40ms, mfu 14.76%\n",
            "iter 1470: loss 1.2534, time 34.57ms, mfu 14.78%\n",
            "iter 1480: loss 1.2691, time 34.71ms, mfu 14.78%\n",
            "iter 1490: loss 1.2376, time 34.47ms, mfu 14.80%\n",
            "step 1500: train loss 1.1785, val loss 1.4921\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1500: loss 1.2582, time 5895.54ms, mfu 13.33%\n",
            "iter 1510: loss 1.2555, time 34.62ms, mfu 13.48%\n",
            "iter 1520: loss 1.2610, time 34.54ms, mfu 13.63%\n",
            "iter 1530: loss 1.2433, time 34.45ms, mfu 13.76%\n",
            "iter 1540: loss 1.2493, time 34.61ms, mfu 13.87%\n",
            "iter 1550: loss 1.2497, time 34.55ms, mfu 13.98%\n",
            "iter 1560: loss 1.2465, time 34.48ms, mfu 14.08%\n",
            "iter 1570: loss 1.2782, time 34.46ms, mfu 14.16%\n",
            "iter 1580: loss 1.2238, time 34.55ms, mfu 14.24%\n",
            "iter 1590: loss 1.2090, time 34.45ms, mfu 14.31%\n",
            "iter 1600: loss 1.2449, time 34.48ms, mfu 14.37%\n",
            "iter 1610: loss 1.2333, time 34.52ms, mfu 14.43%\n",
            "iter 1620: loss 1.2066, time 34.51ms, mfu 14.48%\n",
            "iter 1630: loss 1.2470, time 34.55ms, mfu 14.52%\n",
            "iter 1640: loss 1.2346, time 34.56ms, mfu 14.56%\n",
            "iter 1650: loss 1.2443, time 34.44ms, mfu 14.60%\n",
            "iter 1660: loss 1.2351, time 34.49ms, mfu 14.64%\n",
            "iter 1670: loss 1.2470, time 34.55ms, mfu 14.67%\n",
            "iter 1680: loss 1.2188, time 34.60ms, mfu 14.69%\n",
            "iter 1690: loss 1.2264, time 34.50ms, mfu 14.71%\n",
            "iter 1700: loss 1.2384, time 34.56ms, mfu 14.73%\n",
            "iter 1710: loss 1.2186, time 34.51ms, mfu 14.75%\n",
            "iter 1720: loss 1.2195, time 34.49ms, mfu 14.77%\n",
            "iter 1730: loss 1.2064, time 34.55ms, mfu 14.79%\n",
            "iter 1740: loss 1.2480, time 34.52ms, mfu 14.80%\n",
            "step 1750: train loss 1.1262, val loss 1.4783\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1750: loss 1.2055, time 5887.68ms, mfu 13.33%\n",
            "iter 1760: loss 1.2175, time 34.51ms, mfu 13.49%\n",
            "iter 1770: loss 1.2126, time 34.48ms, mfu 13.64%\n",
            "iter 1780: loss 1.1747, time 34.56ms, mfu 13.76%\n",
            "iter 1790: loss 1.1930, time 34.55ms, mfu 13.88%\n",
            "iter 1800: loss 1.1930, time 34.63ms, mfu 13.98%\n",
            "iter 1810: loss 1.2574, time 34.59ms, mfu 14.07%\n",
            "iter 1820: loss 1.1910, time 34.57ms, mfu 14.15%\n",
            "iter 1830: loss 1.2163, time 34.54ms, mfu 14.23%\n",
            "iter 1840: loss 1.1826, time 34.44ms, mfu 14.30%\n",
            "iter 1850: loss 1.2270, time 34.56ms, mfu 14.37%\n",
            "iter 1860: loss 1.1780, time 34.32ms, mfu 14.43%\n",
            "iter 1870: loss 1.2220, time 34.49ms, mfu 14.48%\n",
            "iter 1880: loss 1.1794, time 34.53ms, mfu 14.53%\n",
            "iter 1890: loss 1.2119, time 34.49ms, mfu 14.57%\n",
            "iter 1900: loss 1.1788, time 34.57ms, mfu 14.60%\n",
            "iter 1910: loss 1.1723, time 34.83ms, mfu 14.62%\n",
            "iter 1920: loss 1.1598, time 34.53ms, mfu 14.65%\n",
            "iter 1930: loss 1.1981, time 34.38ms, mfu 14.69%\n",
            "iter 1940: loss 1.1997, time 34.49ms, mfu 14.71%\n",
            "iter 1950: loss 1.2004, time 34.43ms, mfu 14.74%\n",
            "iter 1960: loss 1.1961, time 34.56ms, mfu 14.75%\n",
            "iter 1970: loss 1.1844, time 34.60ms, mfu 14.77%\n",
            "iter 1980: loss 1.1834, time 34.90ms, mfu 14.77%\n",
            "iter 1990: loss 1.1631, time 34.61ms, mfu 14.78%\n",
            "step 2000: train loss 1.0823, val loss 1.4827\n",
            "iter 2000: loss 1.1740, time 5566.37ms, mfu 13.31%\n",
            "iter 2010: loss 1.1816, time 34.60ms, mfu 13.47%\n",
            "iter 2020: loss 1.1500, time 34.52ms, mfu 13.62%\n",
            "iter 2030: loss 1.1623, time 34.50ms, mfu 13.75%\n",
            "iter 2040: loss 1.1633, time 34.67ms, mfu 13.86%\n",
            "iter 2050: loss 1.1421, time 34.53ms, mfu 13.97%\n",
            "iter 2060: loss 1.1680, time 34.32ms, mfu 14.07%\n",
            "iter 2070: loss 1.1291, time 34.43ms, mfu 14.16%\n",
            "iter 2080: loss 1.1721, time 34.49ms, mfu 14.24%\n",
            "iter 2090: loss 1.1627, time 34.49ms, mfu 14.31%\n",
            "iter 2100: loss 1.1485, time 34.58ms, mfu 14.37%\n",
            "iter 2110: loss 1.1591, time 34.53ms, mfu 14.42%\n",
            "iter 2120: loss 1.1597, time 34.50ms, mfu 14.48%\n",
            "iter 2130: loss 1.1527, time 34.65ms, mfu 14.52%\n",
            "iter 2140: loss 1.1468, time 34.46ms, mfu 14.56%\n",
            "iter 2150: loss 1.1333, time 34.31ms, mfu 14.61%\n",
            "iter 2160: loss 1.1644, time 34.44ms, mfu 14.64%\n",
            "iter 2170: loss 1.1306, time 34.56ms, mfu 14.67%\n",
            "iter 2180: loss 1.1404, time 34.55ms, mfu 14.69%\n",
            "iter 2190: loss 1.1314, time 34.45ms, mfu 14.72%\n",
            "iter 2200: loss 1.1458, time 34.54ms, mfu 14.74%\n",
            "iter 2210: loss 1.1444, time 34.53ms, mfu 14.76%\n",
            "iter 2220: loss 1.1509, time 34.47ms, mfu 14.78%\n",
            "iter 2230: loss 1.1317, time 34.53ms, mfu 14.79%\n",
            "iter 2240: loss 1.1681, time 34.65ms, mfu 14.80%\n",
            "step 2250: train loss 1.0347, val loss 1.4736\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2250: loss 1.1533, time 5889.55ms, mfu 13.33%\n",
            "iter 2260: loss 1.1143, time 34.67ms, mfu 13.48%\n",
            "iter 2270: loss 1.1385, time 34.49ms, mfu 13.63%\n",
            "iter 2280: loss 1.1281, time 34.50ms, mfu 13.76%\n",
            "iter 2290: loss 1.1105, time 34.55ms, mfu 13.88%\n",
            "iter 2300: loss 1.1122, time 34.57ms, mfu 13.98%\n",
            "iter 2310: loss 1.1174, time 34.53ms, mfu 14.07%\n",
            "iter 2320: loss 1.1343, time 34.48ms, mfu 14.16%\n",
            "iter 2330: loss 1.1047, time 34.53ms, mfu 14.24%\n",
            "iter 2340: loss 1.1244, time 34.29ms, mfu 14.32%\n",
            "iter 2350: loss 1.1382, time 34.47ms, mfu 14.38%\n",
            "iter 2360: loss 1.0927, time 34.55ms, mfu 14.43%\n",
            "iter 2370: loss 1.1069, time 34.48ms, mfu 14.48%\n",
            "iter 2380: loss 1.0976, time 34.54ms, mfu 14.53%\n",
            "iter 2390: loss 1.0992, time 34.48ms, mfu 14.57%\n",
            "iter 2400: loss 1.0962, time 34.56ms, mfu 14.60%\n",
            "iter 2410: loss 1.1069, time 34.44ms, mfu 14.64%\n",
            "iter 2420: loss 1.0795, time 34.51ms, mfu 14.67%\n",
            "iter 2430: loss 1.1103, time 34.47ms, mfu 14.70%\n",
            "iter 2440: loss 1.1069, time 34.52ms, mfu 14.72%\n",
            "iter 2450: loss 1.0946, time 34.53ms, mfu 14.74%\n",
            "iter 2460: loss 1.0895, time 34.48ms, mfu 14.76%\n",
            "iter 2470: loss 1.0858, time 34.55ms, mfu 14.78%\n",
            "iter 2480: loss 1.1117, time 34.68ms, mfu 14.79%\n",
            "iter 2490: loss 1.0771, time 34.52ms, mfu 14.80%\n",
            "step 2500: train loss 0.9904, val loss 1.4986\n",
            "iter 2500: loss 1.1185, time 5564.58ms, mfu 13.33%\n",
            "iter 2510: loss 1.0745, time 34.47ms, mfu 13.49%\n",
            "iter 2520: loss 1.0940, time 34.54ms, mfu 13.63%\n",
            "iter 2530: loss 1.1110, time 34.51ms, mfu 13.76%\n",
            "iter 2540: loss 1.0709, time 34.51ms, mfu 13.88%\n",
            "iter 2550: loss 1.0914, time 34.50ms, mfu 13.99%\n",
            "iter 2560: loss 1.0477, time 34.49ms, mfu 14.08%\n",
            "iter 2570: loss 1.0732, time 34.55ms, mfu 14.17%\n",
            "iter 2580: loss 1.0884, time 34.55ms, mfu 14.24%\n",
            "iter 2590: loss 1.1007, time 34.49ms, mfu 14.31%\n",
            "iter 2600: loss 1.0588, time 34.56ms, mfu 14.37%\n",
            "iter 2610: loss 1.0660, time 34.42ms, mfu 14.43%\n",
            "iter 2620: loss 1.1036, time 34.52ms, mfu 14.48%\n",
            "iter 2630: loss 1.0440, time 34.50ms, mfu 14.53%\n",
            "iter 2640: loss 1.0728, time 34.47ms, mfu 14.57%\n",
            "iter 2650: loss 1.0451, time 34.56ms, mfu 14.60%\n",
            "iter 2660: loss 1.0205, time 34.45ms, mfu 14.64%\n",
            "iter 2670: loss 1.0651, time 34.58ms, mfu 14.67%\n",
            "iter 2680: loss 1.0509, time 34.54ms, mfu 14.69%\n",
            "iter 2690: loss 1.0497, time 34.49ms, mfu 14.72%\n",
            "iter 2700: loss 1.0777, time 34.53ms, mfu 14.74%\n",
            "iter 2710: loss 1.0414, time 34.49ms, mfu 14.76%\n",
            "iter 2720: loss 1.0566, time 34.55ms, mfu 14.77%\n",
            "iter 2730: loss 1.0595, time 34.36ms, mfu 14.80%\n",
            "iter 2740: loss 1.0473, time 34.54ms, mfu 14.81%\n",
            "step 2750: train loss 0.9434, val loss 1.5079\n",
            "iter 2750: loss 1.0503, time 5576.22ms, mfu 13.34%\n",
            "iter 2760: loss 1.0282, time 34.52ms, mfu 13.50%\n",
            "iter 2770: loss 1.0551, time 34.61ms, mfu 13.64%\n",
            "iter 2780: loss 1.0327, time 34.51ms, mfu 13.77%\n",
            "iter 2790: loss 1.0482, time 34.48ms, mfu 13.88%\n",
            "iter 2800: loss 1.0265, time 34.55ms, mfu 13.99%\n",
            "iter 2810: loss 1.0359, time 34.49ms, mfu 14.08%\n",
            "iter 2820: loss 1.0424, time 34.53ms, mfu 14.17%\n",
            "iter 2830: loss 1.0475, time 34.50ms, mfu 14.24%\n",
            "iter 2840: loss 1.0597, time 34.50ms, mfu 14.31%\n",
            "iter 2850: loss 1.0372, time 34.55ms, mfu 14.37%\n",
            "iter 2860: loss 1.0246, time 34.50ms, mfu 14.43%\n",
            "iter 2870: loss 1.0118, time 34.54ms, mfu 14.48%\n",
            "iter 2880: loss 1.0460, time 34.50ms, mfu 14.52%\n",
            "iter 2890: loss 1.0241, time 34.53ms, mfu 14.56%\n",
            "iter 2900: loss 1.0719, time 34.49ms, mfu 14.60%\n",
            "iter 2910: loss 1.0380, time 34.50ms, mfu 14.64%\n",
            "iter 2920: loss 1.0346, time 34.49ms, mfu 14.67%\n",
            "iter 2930: loss 1.0301, time 34.53ms, mfu 14.69%\n",
            "iter 2940: loss 1.0332, time 34.52ms, mfu 14.72%\n",
            "iter 2950: loss 1.0266, time 34.56ms, mfu 14.74%\n",
            "iter 2960: loss 1.0040, time 34.71ms, mfu 14.75%\n",
            "iter 2970: loss 1.0235, time 34.54ms, mfu 14.76%\n",
            "iter 2980: loss 1.0039, time 34.49ms, mfu 14.78%\n",
            "iter 2990: loss 1.0216, time 34.53ms, mfu 14.80%\n",
            "step 3000: train loss 0.8923, val loss 1.5383\n",
            "iter 3000: loss 1.0496, time 5557.78ms, mfu 13.33%\n",
            "iter 3010: loss 1.0082, time 34.53ms, mfu 13.49%\n",
            "iter 3020: loss 1.0095, time 34.49ms, mfu 13.63%\n",
            "iter 3030: loss 1.0121, time 34.54ms, mfu 13.76%\n",
            "iter 3040: loss 1.0213, time 34.47ms, mfu 13.88%\n",
            "iter 3050: loss 1.0001, time 34.53ms, mfu 13.98%\n",
            "iter 3060: loss 1.0232, time 34.47ms, mfu 14.08%\n",
            "iter 3070: loss 1.0187, time 34.52ms, mfu 14.17%\n",
            "iter 3080: loss 1.0362, time 34.45ms, mfu 14.25%\n",
            "iter 3090: loss 1.0069, time 34.76ms, mfu 14.30%\n",
            "iter 3100: loss 1.0364, time 34.55ms, mfu 14.36%\n",
            "iter 3110: loss 0.9747, time 34.53ms, mfu 14.42%\n",
            "iter 3120: loss 1.0070, time 34.54ms, mfu 14.47%\n",
            "iter 3130: loss 0.9567, time 34.46ms, mfu 14.52%\n",
            "iter 3140: loss 1.0045, time 34.53ms, mfu 14.56%\n",
            "iter 3150: loss 0.9644, time 34.57ms, mfu 14.59%\n",
            "iter 3160: loss 1.0083, time 34.53ms, mfu 14.63%\n",
            "iter 3170: loss 0.9344, time 34.47ms, mfu 14.66%\n",
            "iter 3180: loss 0.9880, time 34.62ms, mfu 14.68%\n",
            "iter 3190: loss 1.0002, time 34.47ms, mfu 14.71%\n",
            "iter 3200: loss 0.9955, time 34.78ms, mfu 14.72%\n",
            "iter 3210: loss 0.9966, time 34.52ms, mfu 14.74%\n",
            "iter 3220: loss 1.0077, time 34.55ms, mfu 14.76%\n",
            "iter 3230: loss 0.9881, time 34.52ms, mfu 14.78%\n",
            "iter 3240: loss 0.9883, time 34.50ms, mfu 14.79%\n",
            "step 3250: train loss 0.8451, val loss 1.5590\n",
            "iter 3250: loss 0.9895, time 5556.11ms, mfu 13.32%\n",
            "iter 3260: loss 0.9923, time 34.57ms, mfu 13.48%\n",
            "iter 3270: loss 0.9709, time 34.49ms, mfu 13.63%\n",
            "iter 3280: loss 0.9807, time 34.55ms, mfu 13.76%\n",
            "iter 3290: loss 0.9768, time 34.54ms, mfu 13.87%\n",
            "iter 3300: loss 0.9801, time 34.35ms, mfu 13.99%\n",
            "iter 3310: loss 0.9718, time 34.54ms, mfu 14.08%\n",
            "iter 3320: loss 0.9649, time 34.50ms, mfu 14.16%\n",
            "iter 3330: loss 0.9554, time 34.45ms, mfu 14.24%\n",
            "iter 3340: loss 0.9744, time 34.50ms, mfu 14.31%\n",
            "iter 3350: loss 0.9542, time 34.47ms, mfu 14.38%\n",
            "iter 3360: loss 0.9749, time 34.52ms, mfu 14.43%\n",
            "iter 3370: loss 0.9728, time 34.49ms, mfu 14.48%\n",
            "iter 3380: loss 0.9489, time 34.55ms, mfu 14.53%\n",
            "iter 3390: loss 0.9723, time 34.58ms, mfu 14.56%\n",
            "iter 3400: loss 0.9436, time 34.37ms, mfu 14.61%\n",
            "iter 3410: loss 0.9637, time 34.58ms, mfu 14.64%\n",
            "iter 3420: loss 0.9394, time 34.48ms, mfu 14.67%\n",
            "iter 3430: loss 0.9658, time 34.54ms, mfu 14.69%\n",
            "iter 3440: loss 0.9389, time 34.43ms, mfu 14.72%\n",
            "iter 3450: loss 0.9443, time 35.33ms, mfu 14.71%\n",
            "iter 3460: loss 0.9302, time 34.44ms, mfu 14.73%\n",
            "iter 3470: loss 0.9767, time 34.47ms, mfu 14.75%\n",
            "iter 3480: loss 0.9349, time 34.48ms, mfu 14.77%\n",
            "iter 3490: loss 0.9324, time 34.54ms, mfu 14.79%\n",
            "step 3500: train loss 0.7994, val loss 1.5878\n",
            "iter 3500: loss 0.9372, time 5564.50ms, mfu 13.32%\n",
            "iter 3510: loss 0.9404, time 34.47ms, mfu 13.48%\n",
            "iter 3520: loss 0.9369, time 34.52ms, mfu 13.63%\n",
            "iter 3530: loss 0.9313, time 34.54ms, mfu 13.76%\n",
            "iter 3540: loss 0.9439, time 34.77ms, mfu 13.86%\n",
            "iter 3550: loss 0.9388, time 34.68ms, mfu 13.96%\n",
            "iter 3560: loss 0.9210, time 34.47ms, mfu 14.06%\n",
            "iter 3570: loss 0.9549, time 34.59ms, mfu 14.15%\n",
            "iter 3580: loss 0.9366, time 34.36ms, mfu 14.23%\n",
            "iter 3590: loss 0.9381, time 34.52ms, mfu 14.30%\n",
            "iter 3600: loss 0.9422, time 34.53ms, mfu 14.36%\n",
            "iter 3610: loss 0.9570, time 34.56ms, mfu 14.42%\n",
            "iter 3620: loss 0.9160, time 34.42ms, mfu 14.47%\n",
            "iter 3630: loss 0.9043, time 34.52ms, mfu 14.52%\n",
            "iter 3640: loss 0.9288, time 34.50ms, mfu 14.56%\n",
            "iter 3650: loss 0.9244, time 34.54ms, mfu 14.60%\n",
            "iter 3660: loss 0.9190, time 34.57ms, mfu 14.63%\n",
            "iter 3670: loss 0.9155, time 34.54ms, mfu 14.66%\n",
            "iter 3680: loss 0.9167, time 34.47ms, mfu 14.69%\n",
            "iter 3690: loss 0.8900, time 34.69ms, mfu 14.70%\n",
            "iter 3700: loss 0.9000, time 34.51ms, mfu 14.73%\n",
            "iter 3710: loss 0.9076, time 34.53ms, mfu 14.75%\n",
            "iter 3720: loss 0.9092, time 34.50ms, mfu 14.77%\n",
            "iter 3730: loss 0.9097, time 34.56ms, mfu 14.78%\n",
            "iter 3740: loss 0.9416, time 34.51ms, mfu 14.80%\n",
            "step 3750: train loss 0.7599, val loss 1.6211\n",
            "iter 3750: loss 0.9093, time 5559.93ms, mfu 13.33%\n",
            "iter 3760: loss 0.9015, time 34.58ms, mfu 13.48%\n",
            "iter 3770: loss 0.9084, time 34.54ms, mfu 13.63%\n",
            "iter 3780: loss 0.9109, time 34.50ms, mfu 13.76%\n",
            "iter 3790: loss 0.8534, time 34.39ms, mfu 13.88%\n",
            "iter 3800: loss 0.9325, time 34.52ms, mfu 13.99%\n",
            "iter 3810: loss 0.8962, time 34.55ms, mfu 14.08%\n",
            "iter 3820: loss 0.9185, time 34.44ms, mfu 14.17%\n",
            "iter 3830: loss 0.9213, time 34.44ms, mfu 14.25%\n",
            "iter 3840: loss 0.9004, time 34.71ms, mfu 14.31%\n",
            "iter 3850: loss 0.8981, time 34.49ms, mfu 14.37%\n",
            "iter 3860: loss 0.8947, time 34.61ms, mfu 14.42%\n",
            "iter 3870: loss 0.9091, time 34.47ms, mfu 14.47%\n",
            "iter 3880: loss 0.9055, time 34.50ms, mfu 14.52%\n",
            "iter 3890: loss 0.9072, time 34.50ms, mfu 14.56%\n",
            "iter 3900: loss 0.9018, time 34.66ms, mfu 14.59%\n",
            "iter 3910: loss 0.8780, time 34.51ms, mfu 14.63%\n",
            "iter 3920: loss 0.8970, time 34.60ms, mfu 14.65%\n",
            "iter 3930: loss 0.8975, time 34.51ms, mfu 14.68%\n",
            "iter 3940: loss 0.8861, time 34.56ms, mfu 14.71%\n",
            "iter 3950: loss 0.8994, time 34.50ms, mfu 14.73%\n",
            "iter 3960: loss 0.8911, time 34.47ms, mfu 14.75%\n",
            "iter 3970: loss 0.9117, time 34.53ms, mfu 14.77%\n",
            "iter 3980: loss 0.8894, time 34.50ms, mfu 14.78%\n",
            "iter 3990: loss 0.8709, time 34.49ms, mfu 14.80%\n",
            "step 4000: train loss 0.7218, val loss 1.6395\n",
            "iter 4000: loss 0.9037, time 5562.66ms, mfu 13.33%\n",
            "iter 4010: loss 0.8856, time 34.46ms, mfu 13.49%\n",
            "iter 4020: loss 0.8673, time 34.51ms, mfu 13.64%\n",
            "iter 4030: loss 0.9264, time 34.59ms, mfu 13.76%\n",
            "iter 4040: loss 0.8882, time 34.47ms, mfu 13.88%\n",
            "iter 4050: loss 0.8862, time 34.49ms, mfu 13.99%\n",
            "iter 4060: loss 0.8962, time 34.63ms, mfu 14.08%\n",
            "iter 4070: loss 0.8909, time 34.54ms, mfu 14.16%\n",
            "iter 4080: loss 0.8912, time 34.53ms, mfu 14.24%\n",
            "iter 4090: loss 0.8991, time 34.49ms, mfu 14.31%\n",
            "iter 4100: loss 0.8915, time 34.54ms, mfu 14.37%\n",
            "iter 4110: loss 0.8705, time 34.53ms, mfu 14.43%\n",
            "iter 4120: loss 0.8983, time 34.50ms, mfu 14.48%\n",
            "iter 4130: loss 0.8634, time 34.54ms, mfu 14.52%\n",
            "iter 4140: loss 0.8815, time 34.72ms, mfu 14.55%\n",
            "iter 4150: loss 0.8792, time 34.50ms, mfu 14.59%\n",
            "iter 4160: loss 0.8870, time 34.50ms, mfu 14.63%\n",
            "iter 4170: loss 0.8744, time 34.47ms, mfu 14.66%\n",
            "iter 4180: loss 0.8592, time 34.52ms, mfu 14.69%\n",
            "iter 4190: loss 0.8626, time 34.53ms, mfu 14.71%\n",
            "iter 4200: loss 0.8496, time 34.88ms, mfu 14.72%\n",
            "iter 4210: loss 0.8730, time 34.57ms, mfu 14.74%\n",
            "iter 4220: loss 0.8967, time 34.46ms, mfu 14.76%\n",
            "iter 4230: loss 0.8588, time 34.52ms, mfu 14.77%\n",
            "iter 4240: loss 0.8531, time 34.48ms, mfu 14.79%\n",
            "step 4250: train loss 0.6908, val loss 1.6629\n",
            "iter 4250: loss 0.8447, time 5551.53ms, mfu 13.32%\n",
            "iter 4260: loss 0.8679, time 34.55ms, mfu 13.48%\n",
            "iter 4270: loss 0.8692, time 34.32ms, mfu 13.63%\n",
            "iter 4280: loss 0.8785, time 34.50ms, mfu 13.77%\n",
            "iter 4290: loss 0.8794, time 34.51ms, mfu 13.88%\n",
            "iter 4300: loss 0.8525, time 34.54ms, mfu 13.99%\n",
            "iter 4310: loss 0.8624, time 34.71ms, mfu 14.07%\n",
            "iter 4320: loss 0.8422, time 34.56ms, mfu 14.16%\n",
            "iter 4330: loss 0.8616, time 34.46ms, mfu 14.24%\n",
            "iter 4340: loss 0.8540, time 34.55ms, mfu 14.30%\n",
            "iter 4350: loss 0.8590, time 34.49ms, mfu 14.37%\n",
            "iter 4360: loss 0.8469, time 34.56ms, mfu 14.42%\n",
            "iter 4370: loss 0.8462, time 34.46ms, mfu 14.48%\n",
            "iter 4380: loss 0.8502, time 34.54ms, mfu 14.52%\n",
            "iter 4390: loss 0.8473, time 34.50ms, mfu 14.56%\n",
            "iter 4400: loss 0.8610, time 34.50ms, mfu 14.60%\n",
            "iter 4410: loss 0.8586, time 34.43ms, mfu 14.64%\n",
            "iter 4420: loss 0.8540, time 34.49ms, mfu 14.67%\n",
            "iter 4430: loss 0.8382, time 34.21ms, mfu 14.71%\n",
            "iter 4440: loss 0.8411, time 34.49ms, mfu 14.73%\n",
            "iter 4450: loss 0.8426, time 34.50ms, mfu 14.75%\n",
            "iter 4460: loss 0.8348, time 34.45ms, mfu 14.77%\n",
            "iter 4470: loss 0.8304, time 34.55ms, mfu 14.79%\n",
            "iter 4480: loss 0.8382, time 34.49ms, mfu 14.80%\n",
            "iter 4490: loss 0.8625, time 34.78ms, mfu 14.80%\n",
            "step 4500: train loss 0.6631, val loss 1.6841\n",
            "iter 4500: loss 0.8701, time 5558.32ms, mfu 13.33%\n",
            "iter 4510: loss 0.8209, time 34.50ms, mfu 13.49%\n",
            "iter 4520: loss 0.8460, time 34.33ms, mfu 13.64%\n",
            "iter 4530: loss 0.8338, time 35.08ms, mfu 13.75%\n",
            "iter 4540: loss 0.8495, time 34.48ms, mfu 13.87%\n",
            "iter 4550: loss 0.8395, time 34.46ms, mfu 13.98%\n",
            "iter 4560: loss 0.8395, time 34.48ms, mfu 14.07%\n",
            "iter 4570: loss 0.8273, time 34.56ms, mfu 14.16%\n",
            "iter 4580: loss 0.8369, time 34.48ms, mfu 14.24%\n",
            "iter 4590: loss 0.8267, time 34.43ms, mfu 14.31%\n",
            "iter 4600: loss 0.8320, time 34.64ms, mfu 14.37%\n",
            "iter 4610: loss 0.8230, time 34.48ms, mfu 14.42%\n",
            "iter 4620: loss 0.8307, time 34.47ms, mfu 14.48%\n",
            "iter 4630: loss 0.8563, time 34.53ms, mfu 14.52%\n",
            "iter 4640: loss 0.8397, time 34.53ms, mfu 14.56%\n",
            "iter 4650: loss 0.8187, time 34.43ms, mfu 14.60%\n",
            "iter 4660: loss 0.8131, time 34.52ms, mfu 14.64%\n",
            "iter 4670: loss 0.8433, time 34.51ms, mfu 14.67%\n",
            "iter 4680: loss 0.8409, time 34.45ms, mfu 14.69%\n",
            "iter 4690: loss 0.8271, time 34.52ms, mfu 14.72%\n",
            "iter 4700: loss 0.8276, time 34.42ms, mfu 14.74%\n",
            "iter 4710: loss 0.8213, time 34.42ms, mfu 14.77%\n",
            "iter 4720: loss 0.8223, time 34.54ms, mfu 14.78%\n",
            "iter 4730: loss 0.8212, time 34.53ms, mfu 14.80%\n",
            "iter 4740: loss 0.8276, time 34.44ms, mfu 14.81%\n",
            "step 4750: train loss 0.6413, val loss 1.7190\n",
            "iter 4750: loss 0.8266, time 5553.64ms, mfu 13.34%\n",
            "iter 4760: loss 0.8244, time 34.50ms, mfu 13.50%\n",
            "iter 4770: loss 0.8094, time 34.53ms, mfu 13.64%\n",
            "iter 4780: loss 0.8214, time 34.52ms, mfu 13.77%\n",
            "iter 4790: loss 0.8196, time 34.36ms, mfu 13.89%\n",
            "iter 4800: loss 0.8107, time 34.50ms, mfu 14.00%\n",
            "iter 4810: loss 0.8527, time 34.46ms, mfu 14.09%\n",
            "iter 4820: loss 0.8351, time 34.56ms, mfu 14.18%\n",
            "iter 4830: loss 0.8307, time 34.56ms, mfu 14.25%\n",
            "iter 4840: loss 0.8433, time 34.70ms, mfu 14.31%\n",
            "iter 4850: loss 0.8384, time 34.46ms, mfu 14.37%\n",
            "iter 4860: loss 0.8177, time 34.54ms, mfu 14.43%\n",
            "iter 4870: loss 0.8581, time 34.57ms, mfu 14.48%\n",
            "iter 4880: loss 0.8171, time 34.46ms, mfu 14.52%\n",
            "iter 4890: loss 0.8240, time 34.53ms, mfu 14.56%\n",
            "iter 4900: loss 0.8086, time 34.51ms, mfu 14.60%\n",
            "iter 4910: loss 0.8423, time 34.54ms, mfu 14.63%\n",
            "iter 4920: loss 0.8489, time 34.52ms, mfu 14.66%\n",
            "iter 4930: loss 0.8203, time 34.54ms, mfu 14.69%\n",
            "iter 4940: loss 0.7975, time 34.51ms, mfu 14.71%\n",
            "iter 4950: loss 0.8015, time 34.57ms, mfu 14.73%\n",
            "iter 4960: loss 0.8324, time 34.48ms, mfu 14.75%\n",
            "iter 4970: loss 0.8478, time 34.41ms, mfu 14.78%\n",
            "iter 4980: loss 0.8180, time 34.52ms, mfu 14.79%\n",
            "iter 4990: loss 0.7924, time 34.49ms, mfu 14.81%\n",
            "step 5000: train loss 0.6248, val loss 1.7194\n",
            "iter 5000: loss 0.7879, time 5562.05ms, mfu 13.34%\n",
            "==== 训练 n_head=3 ====\n",
            "Overriding config with config/train_shakespeare_char_3head.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 7\n",
            "n_head = 3\n",
            "n_embd = 420\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "device = \"cuda\"\n",
            "compile = False\n",
            "\n",
            "Overriding: device = cuda\n",
            "Overriding: compile = False\n",
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 14.85M\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "num decayed parameter tensors: 30, with 14,952,420 parameters\n",
            "num non-decayed parameter tensors: 15, with 6,300 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.3031, val loss 4.2993\n",
            "iter 0: loss 4.2793, time 5990.10ms, mfu -100.00%\n",
            "iter 10: loss 3.0999, time 34.52ms, mfu 14.93%\n",
            "iter 20: loss 2.6946, time 34.47ms, mfu 14.93%\n",
            "iter 30: loss 2.5796, time 34.51ms, mfu 14.93%\n",
            "iter 40: loss 2.5652, time 34.49ms, mfu 14.93%\n",
            "iter 50: loss 2.5191, time 34.48ms, mfu 14.93%\n",
            "iter 60: loss 2.4912, time 34.71ms, mfu 14.93%\n",
            "iter 70: loss 2.4802, time 34.59ms, mfu 14.92%\n",
            "iter 80: loss 2.4904, time 34.49ms, mfu 14.92%\n",
            "iter 90: loss 2.4765, time 34.34ms, mfu 14.93%\n",
            "iter 100: loss 2.4510, time 34.51ms, mfu 14.93%\n",
            "iter 110: loss 2.4258, time 34.54ms, mfu 14.93%\n",
            "iter 120: loss 2.4093, time 34.46ms, mfu 14.93%\n",
            "iter 130: loss 2.3901, time 34.58ms, mfu 14.93%\n",
            "iter 140: loss 2.3737, time 34.76ms, mfu 14.92%\n",
            "iter 150: loss 2.3470, time 34.50ms, mfu 14.92%\n",
            "iter 160: loss 2.3038, time 34.56ms, mfu 14.92%\n",
            "iter 170: loss 2.1754, time 34.97ms, mfu 14.90%\n",
            "iter 180: loss 2.1767, time 34.51ms, mfu 14.91%\n",
            "iter 190: loss 2.1031, time 34.49ms, mfu 14.91%\n",
            "iter 200: loss 2.0945, time 34.47ms, mfu 14.91%\n",
            "iter 210: loss 2.0497, time 34.53ms, mfu 14.91%\n",
            "iter 220: loss 2.0241, time 34.52ms, mfu 14.92%\n",
            "iter 230: loss 2.0087, time 34.52ms, mfu 14.92%\n",
            "iter 240: loss 1.9567, time 34.52ms, mfu 14.92%\n",
            "step 250: train loss 1.8954, val loss 2.0285\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 250: loss 1.9689, time 5890.63ms, mfu 13.44%\n",
            "iter 260: loss 1.9181, time 34.51ms, mfu 13.58%\n",
            "iter 270: loss 1.9189, time 34.06ms, mfu 13.74%\n",
            "iter 280: loss 1.8823, time 34.44ms, mfu 13.86%\n",
            "iter 290: loss 1.8688, time 34.66ms, mfu 13.96%\n",
            "iter 300: loss 1.8353, time 34.55ms, mfu 14.06%\n",
            "iter 310: loss 1.8592, time 34.55ms, mfu 14.14%\n",
            "iter 320: loss 1.8300, time 34.42ms, mfu 14.23%\n",
            "iter 330: loss 1.8203, time 34.51ms, mfu 14.30%\n",
            "iter 340: loss 1.8051, time 34.50ms, mfu 14.36%\n",
            "iter 350: loss 1.7915, time 34.53ms, mfu 14.42%\n",
            "iter 360: loss 1.7925, time 34.46ms, mfu 14.47%\n",
            "iter 370: loss 1.7395, time 34.54ms, mfu 14.52%\n",
            "iter 380: loss 1.7414, time 34.50ms, mfu 14.56%\n",
            "iter 390: loss 1.7667, time 34.65ms, mfu 14.59%\n",
            "iter 400: loss 1.7420, time 34.47ms, mfu 14.63%\n",
            "iter 410: loss 1.7192, time 34.56ms, mfu 14.65%\n",
            "iter 420: loss 1.7031, time 34.48ms, mfu 14.68%\n",
            "iter 430: loss 1.7210, time 34.47ms, mfu 14.71%\n",
            "iter 440: loss 1.6642, time 34.66ms, mfu 14.73%\n",
            "iter 450: loss 1.6838, time 34.82ms, mfu 14.73%\n",
            "iter 460: loss 1.6987, time 34.54ms, mfu 14.75%\n",
            "iter 470: loss 1.6590, time 34.25ms, mfu 14.78%\n",
            "iter 480: loss 1.6447, time 34.52ms, mfu 14.80%\n",
            "iter 490: loss 1.6677, time 34.41ms, mfu 14.81%\n",
            "step 500: train loss 1.5284, val loss 1.7309\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 500: loss 1.6186, time 5879.21ms, mfu 13.34%\n",
            "iter 510: loss 1.5844, time 34.52ms, mfu 13.50%\n",
            "iter 520: loss 1.6217, time 34.46ms, mfu 13.65%\n",
            "iter 530: loss 1.5918, time 34.60ms, mfu 13.77%\n",
            "iter 540: loss 1.6067, time 34.50ms, mfu 13.89%\n",
            "iter 550: loss 1.5951, time 34.45ms, mfu 13.99%\n",
            "iter 560: loss 1.5850, time 34.48ms, mfu 14.09%\n",
            "iter 570: loss 1.5673, time 34.50ms, mfu 14.17%\n",
            "iter 580: loss 1.5157, time 34.60ms, mfu 14.25%\n",
            "iter 590: loss 1.5408, time 34.59ms, mfu 14.31%\n",
            "iter 600: loss 1.5569, time 34.61ms, mfu 14.37%\n",
            "iter 610: loss 1.5140, time 34.44ms, mfu 14.43%\n",
            "iter 620: loss 1.5400, time 34.54ms, mfu 14.48%\n",
            "iter 630: loss 1.5133, time 34.25ms, mfu 14.54%\n",
            "iter 640: loss 1.5320, time 34.56ms, mfu 14.57%\n",
            "iter 650: loss 1.4920, time 34.77ms, mfu 14.60%\n",
            "iter 660: loss 1.5014, time 34.56ms, mfu 14.63%\n",
            "iter 670: loss 1.5209, time 34.50ms, mfu 14.66%\n",
            "iter 680: loss 1.5054, time 34.47ms, mfu 14.69%\n",
            "iter 690: loss 1.5119, time 34.56ms, mfu 14.71%\n",
            "iter 700: loss 1.4891, time 34.45ms, mfu 14.74%\n",
            "iter 710: loss 1.4911, time 34.55ms, mfu 14.75%\n",
            "iter 720: loss 1.4715, time 34.59ms, mfu 14.77%\n",
            "iter 730: loss 1.4726, time 34.67ms, mfu 14.78%\n",
            "iter 740: loss 1.4408, time 34.54ms, mfu 14.79%\n",
            "step 750: train loss 1.3623, val loss 1.5774\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 750: loss 1.4614, time 5863.74ms, mfu 13.32%\n",
            "iter 760: loss 1.4172, time 34.59ms, mfu 13.48%\n",
            "iter 770: loss 1.4487, time 34.58ms, mfu 13.62%\n",
            "iter 780: loss 1.4256, time 34.48ms, mfu 13.75%\n",
            "iter 790: loss 1.4251, time 34.51ms, mfu 13.87%\n",
            "iter 800: loss 1.4277, time 34.49ms, mfu 13.98%\n",
            "iter 810: loss 1.4265, time 34.56ms, mfu 14.07%\n",
            "iter 820: loss 1.4076, time 34.62ms, mfu 14.15%\n",
            "iter 830: loss 1.4552, time 34.51ms, mfu 14.23%\n",
            "iter 840: loss 1.4096, time 34.58ms, mfu 14.30%\n",
            "iter 850: loss 1.4185, time 34.52ms, mfu 14.36%\n",
            "iter 860: loss 1.3929, time 34.50ms, mfu 14.42%\n",
            "iter 870: loss 1.3903, time 34.50ms, mfu 14.47%\n",
            "iter 880: loss 1.3973, time 34.53ms, mfu 14.52%\n",
            "iter 890: loss 1.4012, time 34.53ms, mfu 14.56%\n",
            "iter 900: loss 1.3644, time 34.45ms, mfu 14.60%\n",
            "iter 910: loss 1.3900, time 34.45ms, mfu 14.63%\n",
            "iter 920: loss 1.3704, time 34.72ms, mfu 14.65%\n",
            "iter 930: loss 1.3919, time 34.49ms, mfu 14.68%\n",
            "iter 940: loss 1.3494, time 34.54ms, mfu 14.71%\n",
            "iter 950: loss 1.3867, time 34.50ms, mfu 14.73%\n",
            "iter 960: loss 1.3019, time 34.53ms, mfu 14.75%\n",
            "iter 970: loss 1.3512, time 34.42ms, mfu 14.77%\n",
            "iter 980: loss 1.3865, time 34.49ms, mfu 14.79%\n",
            "iter 990: loss 1.3479, time 34.57ms, mfu 14.80%\n",
            "step 1000: train loss 1.2671, val loss 1.5170\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1000: loss 1.3485, time 5861.17ms, mfu 13.33%\n",
            "iter 1010: loss 1.3344, time 34.52ms, mfu 13.49%\n",
            "iter 1020: loss 1.3623, time 34.39ms, mfu 13.64%\n",
            "iter 1030: loss 1.3559, time 34.53ms, mfu 13.77%\n",
            "iter 1040: loss 1.3442, time 34.68ms, mfu 13.88%\n",
            "iter 1050: loss 1.3284, time 34.55ms, mfu 13.98%\n",
            "iter 1060: loss 1.3358, time 34.45ms, mfu 14.08%\n",
            "iter 1070: loss 1.3226, time 34.32ms, mfu 14.17%\n",
            "iter 1080: loss 1.3307, time 34.53ms, mfu 14.25%\n",
            "iter 1090: loss 1.3189, time 34.54ms, mfu 14.32%\n",
            "iter 1100: loss 1.3338, time 34.66ms, mfu 14.37%\n",
            "iter 1110: loss 1.3527, time 34.54ms, mfu 14.43%\n",
            "iter 1120: loss 1.2797, time 34.50ms, mfu 14.48%\n",
            "iter 1130: loss 1.3148, time 34.46ms, mfu 14.52%\n",
            "iter 1140: loss 1.3236, time 34.53ms, mfu 14.57%\n",
            "iter 1150: loss 1.3174, time 34.44ms, mfu 14.60%\n",
            "iter 1160: loss 1.3136, time 34.88ms, mfu 14.62%\n",
            "iter 1170: loss 1.3146, time 34.58ms, mfu 14.65%\n",
            "iter 1180: loss 1.3034, time 34.59ms, mfu 14.67%\n",
            "iter 1190: loss 1.2853, time 34.56ms, mfu 14.70%\n",
            "iter 1200: loss 1.3194, time 34.48ms, mfu 14.72%\n",
            "iter 1210: loss 1.3274, time 34.36ms, mfu 14.75%\n",
            "iter 1220: loss 1.3228, time 34.51ms, mfu 14.77%\n",
            "iter 1230: loss 1.3094, time 34.50ms, mfu 14.79%\n",
            "iter 1240: loss 1.2990, time 34.49ms, mfu 14.80%\n",
            "step 1250: train loss 1.2038, val loss 1.4836\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1250: loss 1.2798, time 5870.52ms, mfu 13.33%\n",
            "iter 1260: loss 1.3026, time 34.48ms, mfu 13.49%\n",
            "iter 1270: loss 1.3016, time 34.74ms, mfu 13.63%\n",
            "iter 1280: loss 1.3039, time 34.43ms, mfu 13.76%\n",
            "iter 1290: loss 1.2903, time 34.51ms, mfu 13.88%\n",
            "iter 1300: loss 1.2971, time 34.35ms, mfu 13.99%\n",
            "iter 1310: loss 1.2220, time 34.41ms, mfu 14.09%\n",
            "iter 1320: loss 1.2803, time 34.75ms, mfu 14.16%\n",
            "iter 1330: loss 1.2559, time 33.72ms, mfu 14.28%\n",
            "iter 1340: loss 1.2865, time 34.55ms, mfu 14.34%\n",
            "iter 1350: loss 1.2754, time 34.53ms, mfu 14.40%\n",
            "iter 1360: loss 1.2357, time 34.06ms, mfu 14.47%\n",
            "iter 1370: loss 1.2551, time 33.95ms, mfu 14.54%\n",
            "iter 1380: loss 1.2348, time 34.55ms, mfu 14.58%\n",
            "iter 1390: loss 1.2525, time 34.48ms, mfu 14.62%\n",
            "iter 1400: loss 1.2266, time 34.54ms, mfu 14.65%\n",
            "iter 1410: loss 1.2695, time 34.52ms, mfu 14.68%\n",
            "iter 1420: loss 1.2454, time 34.51ms, mfu 14.70%\n",
            "iter 1430: loss 1.2656, time 34.61ms, mfu 14.72%\n",
            "iter 1440: loss 1.2174, time 34.32ms, mfu 14.75%\n",
            "iter 1450: loss 1.2446, time 34.57ms, mfu 14.77%\n",
            "iter 1460: loss 1.2655, time 34.51ms, mfu 14.78%\n",
            "iter 1470: loss 1.2252, time 34.50ms, mfu 14.80%\n",
            "iter 1480: loss 1.2375, time 34.40ms, mfu 14.82%\n",
            "iter 1490: loss 1.2102, time 34.49ms, mfu 14.83%\n",
            "step 1500: train loss 1.1494, val loss 1.4736\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1500: loss 1.2291, time 5863.90ms, mfu 13.35%\n",
            "iter 1510: loss 1.2289, time 34.22ms, mfu 13.53%\n",
            "iter 1520: loss 1.2305, time 34.40ms, mfu 13.67%\n",
            "iter 1530: loss 1.2146, time 34.53ms, mfu 13.80%\n",
            "iter 1540: loss 1.2076, time 34.60ms, mfu 13.91%\n",
            "iter 1550: loss 1.2231, time 34.42ms, mfu 14.01%\n",
            "iter 1560: loss 1.2169, time 34.48ms, mfu 14.11%\n",
            "iter 1570: loss 1.2448, time 34.51ms, mfu 14.19%\n",
            "iter 1580: loss 1.1937, time 34.52ms, mfu 14.26%\n",
            "iter 1590: loss 1.1822, time 34.55ms, mfu 14.33%\n",
            "iter 1600: loss 1.2088, time 34.64ms, mfu 14.38%\n",
            "iter 1610: loss 1.2035, time 34.48ms, mfu 14.44%\n",
            "iter 1620: loss 1.1754, time 34.54ms, mfu 14.49%\n",
            "iter 1630: loss 1.2165, time 34.50ms, mfu 14.53%\n",
            "iter 1640: loss 1.2031, time 34.48ms, mfu 14.57%\n",
            "iter 1650: loss 1.2136, time 34.59ms, mfu 14.61%\n",
            "iter 1660: loss 1.2077, time 34.48ms, mfu 14.64%\n",
            "iter 1670: loss 1.2196, time 34.50ms, mfu 14.67%\n",
            "iter 1680: loss 1.1842, time 34.53ms, mfu 14.70%\n",
            "iter 1690: loss 1.1945, time 34.53ms, mfu 14.72%\n",
            "iter 1700: loss 1.2039, time 34.44ms, mfu 14.74%\n",
            "iter 1710: loss 1.1907, time 34.46ms, mfu 14.76%\n",
            "iter 1720: loss 1.1812, time 34.51ms, mfu 14.78%\n",
            "iter 1730: loss 1.1824, time 34.53ms, mfu 14.80%\n",
            "iter 1740: loss 1.2139, time 34.50ms, mfu 14.81%\n",
            "step 1750: train loss 1.0940, val loss 1.4707\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1750: loss 1.1752, time 5890.55ms, mfu 13.34%\n",
            "iter 1760: loss 1.1840, time 34.53ms, mfu 13.50%\n",
            "iter 1770: loss 1.1764, time 34.50ms, mfu 13.64%\n",
            "iter 1780: loss 1.1486, time 34.51ms, mfu 13.77%\n",
            "iter 1790: loss 1.1564, time 34.59ms, mfu 13.88%\n",
            "iter 1800: loss 1.1588, time 34.66ms, mfu 13.98%\n",
            "iter 1810: loss 1.2188, time 34.57ms, mfu 14.07%\n",
            "iter 1820: loss 1.1574, time 34.50ms, mfu 14.16%\n",
            "iter 1830: loss 1.1829, time 34.49ms, mfu 14.24%\n",
            "iter 1840: loss 1.1565, time 34.66ms, mfu 14.30%\n",
            "iter 1850: loss 1.2013, time 34.49ms, mfu 14.37%\n",
            "iter 1860: loss 1.1423, time 34.49ms, mfu 14.42%\n",
            "iter 1870: loss 1.1793, time 34.39ms, mfu 14.48%\n",
            "iter 1880: loss 1.1489, time 34.49ms, mfu 14.53%\n",
            "iter 1890: loss 1.1728, time 34.56ms, mfu 14.56%\n",
            "iter 1900: loss 1.1546, time 34.52ms, mfu 14.60%\n",
            "iter 1910: loss 1.1446, time 34.62ms, mfu 14.63%\n",
            "iter 1920: loss 1.1153, time 34.53ms, mfu 14.66%\n",
            "iter 1930: loss 1.1668, time 34.49ms, mfu 14.69%\n",
            "iter 1940: loss 1.1640, time 34.59ms, mfu 14.71%\n",
            "iter 1950: loss 1.1667, time 34.50ms, mfu 14.73%\n",
            "iter 1960: loss 1.1594, time 34.55ms, mfu 14.75%\n",
            "iter 1970: loss 1.1458, time 34.49ms, mfu 14.77%\n",
            "iter 1980: loss 1.1461, time 34.56ms, mfu 14.78%\n",
            "iter 1990: loss 1.1267, time 34.50ms, mfu 14.80%\n",
            "step 2000: train loss 1.0424, val loss 1.4690\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2000: loss 1.1356, time 5890.78ms, mfu 13.33%\n",
            "iter 2010: loss 1.1501, time 34.46ms, mfu 13.49%\n",
            "iter 2020: loss 1.1048, time 34.56ms, mfu 13.63%\n",
            "iter 2030: loss 1.1314, time 34.39ms, mfu 13.77%\n",
            "iter 2040: loss 1.1250, time 34.50ms, mfu 13.89%\n",
            "iter 2050: loss 1.1078, time 34.43ms, mfu 13.99%\n",
            "iter 2060: loss 1.1222, time 34.49ms, mfu 14.09%\n",
            "iter 2070: loss 1.0930, time 34.52ms, mfu 14.17%\n",
            "iter 2080: loss 1.1338, time 34.56ms, mfu 14.25%\n",
            "iter 2090: loss 1.1263, time 34.50ms, mfu 14.32%\n",
            "iter 2100: loss 1.1190, time 34.47ms, mfu 14.38%\n",
            "iter 2110: loss 1.1170, time 34.51ms, mfu 14.43%\n",
            "iter 2120: loss 1.1194, time 34.42ms, mfu 14.49%\n",
            "iter 2130: loss 1.1188, time 34.45ms, mfu 14.54%\n",
            "iter 2140: loss 1.0961, time 34.38ms, mfu 14.58%\n",
            "iter 2150: loss 1.1009, time 34.52ms, mfu 14.62%\n",
            "iter 2160: loss 1.1202, time 34.48ms, mfu 14.65%\n",
            "iter 2170: loss 1.0874, time 34.40ms, mfu 14.68%\n",
            "iter 2180: loss 1.1087, time 34.47ms, mfu 14.71%\n",
            "iter 2190: loss 1.0824, time 34.57ms, mfu 14.73%\n",
            "iter 2200: loss 1.1058, time 34.50ms, mfu 14.75%\n",
            "iter 2210: loss 1.1005, time 34.49ms, mfu 14.77%\n",
            "iter 2220: loss 1.0976, time 34.71ms, mfu 14.78%\n",
            "iter 2230: loss 1.0920, time 34.53ms, mfu 14.79%\n",
            "iter 2240: loss 1.1252, time 34.41ms, mfu 14.81%\n",
            "step 2250: train loss 0.9890, val loss 1.4668\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2250: loss 1.1144, time 5865.67ms, mfu 13.34%\n",
            "iter 2260: loss 1.0656, time 34.49ms, mfu 13.50%\n",
            "iter 2270: loss 1.0886, time 34.49ms, mfu 13.64%\n",
            "iter 2280: loss 1.0865, time 34.53ms, mfu 13.77%\n",
            "iter 2290: loss 1.0692, time 34.48ms, mfu 13.89%\n",
            "iter 2300: loss 1.0697, time 34.55ms, mfu 13.99%\n",
            "iter 2310: loss 1.0702, time 34.50ms, mfu 14.09%\n",
            "iter 2320: loss 1.0965, time 34.52ms, mfu 14.17%\n",
            "iter 2330: loss 1.0602, time 34.80ms, mfu 14.23%\n",
            "iter 2340: loss 1.0911, time 34.63ms, mfu 14.30%\n",
            "iter 2350: loss 1.0967, time 34.58ms, mfu 14.36%\n",
            "iter 2360: loss 1.0515, time 34.49ms, mfu 14.42%\n",
            "iter 2370: loss 1.0708, time 34.32ms, mfu 14.48%\n",
            "iter 2380: loss 1.0529, time 34.51ms, mfu 14.52%\n",
            "iter 2390: loss 1.0516, time 34.56ms, mfu 14.56%\n",
            "iter 2400: loss 1.0514, time 34.48ms, mfu 14.60%\n",
            "iter 2410: loss 1.0591, time 34.43ms, mfu 14.64%\n",
            "iter 2420: loss 1.0356, time 34.55ms, mfu 14.67%\n",
            "iter 2430: loss 1.0752, time 34.52ms, mfu 14.69%\n",
            "iter 2440: loss 1.0523, time 34.13ms, mfu 14.73%\n",
            "iter 2450: loss 1.0528, time 34.36ms, mfu 14.76%\n",
            "iter 2460: loss 1.0583, time 34.48ms, mfu 14.78%\n",
            "iter 2470: loss 1.0367, time 34.53ms, mfu 14.79%\n",
            "iter 2480: loss 1.0677, time 34.46ms, mfu 14.81%\n",
            "iter 2490: loss 1.0321, time 34.52ms, mfu 14.82%\n",
            "step 2500: train loss 0.9377, val loss 1.5067\n",
            "iter 2500: loss 1.0556, time 5531.61ms, mfu 13.35%\n",
            "iter 2510: loss 1.0327, time 34.55ms, mfu 13.50%\n",
            "iter 2520: loss 1.0471, time 34.53ms, mfu 13.65%\n",
            "iter 2530: loss 1.0612, time 34.47ms, mfu 13.78%\n",
            "iter 2540: loss 1.0283, time 34.54ms, mfu 13.89%\n",
            "iter 2550: loss 1.0448, time 34.48ms, mfu 14.00%\n",
            "iter 2560: loss 1.0078, time 34.54ms, mfu 14.09%\n",
            "iter 2570: loss 1.0193, time 34.49ms, mfu 14.17%\n",
            "iter 2580: loss 1.0487, time 34.58ms, mfu 14.25%\n",
            "iter 2590: loss 1.0497, time 34.75ms, mfu 14.31%\n",
            "iter 2600: loss 1.0182, time 34.60ms, mfu 14.36%\n",
            "iter 2610: loss 1.0250, time 34.56ms, mfu 14.42%\n",
            "iter 2620: loss 1.0468, time 35.32ms, mfu 14.44%\n",
            "iter 2630: loss 0.9934, time 34.33ms, mfu 14.49%\n",
            "iter 2640: loss 1.0279, time 34.46ms, mfu 14.54%\n",
            "iter 2650: loss 0.9973, time 34.54ms, mfu 14.58%\n",
            "iter 2660: loss 0.9729, time 34.48ms, mfu 14.61%\n",
            "iter 2670: loss 1.0147, time 34.50ms, mfu 14.65%\n",
            "iter 2680: loss 1.0121, time 34.47ms, mfu 14.68%\n",
            "iter 2690: loss 1.0101, time 34.57ms, mfu 14.70%\n",
            "iter 2700: loss 1.0315, time 34.55ms, mfu 14.72%\n",
            "iter 2710: loss 0.9951, time 34.54ms, mfu 14.74%\n",
            "iter 2720: loss 1.0036, time 34.52ms, mfu 14.76%\n",
            "iter 2730: loss 1.0175, time 34.53ms, mfu 14.78%\n",
            "iter 2740: loss 1.0020, time 34.54ms, mfu 14.79%\n",
            "step 2750: train loss 0.8826, val loss 1.5236\n",
            "iter 2750: loss 1.0046, time 5542.08ms, mfu 13.32%\n",
            "iter 2760: loss 0.9794, time 34.51ms, mfu 13.48%\n",
            "iter 2770: loss 0.9993, time 34.48ms, mfu 13.63%\n",
            "iter 2780: loss 0.9749, time 34.46ms, mfu 13.76%\n",
            "iter 2790: loss 0.9945, time 34.53ms, mfu 13.88%\n",
            "iter 2800: loss 0.9780, time 34.44ms, mfu 13.99%\n",
            "iter 2810: loss 0.9987, time 34.54ms, mfu 14.08%\n",
            "iter 2820: loss 0.9835, time 34.44ms, mfu 14.17%\n",
            "iter 2830: loss 0.9950, time 34.52ms, mfu 14.24%\n",
            "iter 2840: loss 1.0084, time 34.54ms, mfu 14.31%\n",
            "iter 2850: loss 0.9856, time 34.53ms, mfu 14.37%\n",
            "iter 2860: loss 0.9717, time 34.50ms, mfu 14.43%\n",
            "iter 2870: loss 0.9663, time 34.43ms, mfu 14.48%\n",
            "iter 2880: loss 0.9909, time 34.61ms, mfu 14.52%\n",
            "iter 2890: loss 0.9736, time 34.44ms, mfu 14.57%\n",
            "iter 2900: loss 1.0222, time 34.56ms, mfu 14.60%\n",
            "iter 2910: loss 0.9875, time 34.46ms, mfu 14.64%\n",
            "iter 2920: loss 0.9920, time 34.54ms, mfu 14.67%\n",
            "iter 2930: loss 0.9762, time 34.52ms, mfu 14.69%\n",
            "iter 2940: loss 0.9851, time 34.52ms, mfu 14.72%\n",
            "iter 2950: loss 0.9748, time 34.54ms, mfu 14.74%\n",
            "iter 2960: loss 0.9546, time 34.47ms, mfu 14.76%\n",
            "iter 2970: loss 0.9777, time 34.57ms, mfu 14.77%\n",
            "iter 2980: loss 0.9559, time 34.34ms, mfu 14.80%\n",
            "iter 2990: loss 0.9658, time 34.57ms, mfu 14.81%\n",
            "step 3000: train loss 0.8277, val loss 1.5525\n",
            "iter 3000: loss 0.9965, time 5540.87ms, mfu 13.34%\n",
            "iter 3010: loss 0.9442, time 34.36ms, mfu 13.50%\n",
            "iter 3020: loss 0.9663, time 34.49ms, mfu 13.65%\n",
            "iter 3030: loss 0.9690, time 34.33ms, mfu 13.78%\n",
            "iter 3040: loss 0.9608, time 34.48ms, mfu 13.90%\n",
            "iter 3050: loss 0.9544, time 34.53ms, mfu 14.00%\n",
            "iter 3060: loss 0.9625, time 34.50ms, mfu 14.10%\n",
            "iter 3070: loss 0.9599, time 34.52ms, mfu 14.18%\n",
            "iter 3080: loss 0.9792, time 34.41ms, mfu 14.26%\n",
            "iter 3090: loss 0.9501, time 34.54ms, mfu 14.32%\n",
            "iter 3100: loss 0.9810, time 34.49ms, mfu 14.39%\n",
            "iter 3110: loss 0.9151, time 34.54ms, mfu 14.44%\n",
            "iter 3120: loss 0.9494, time 34.50ms, mfu 14.49%\n",
            "iter 3130: loss 0.9034, time 34.54ms, mfu 14.53%\n",
            "iter 3140: loss 0.9491, time 34.50ms, mfu 14.57%\n",
            "iter 3150: loss 0.9151, time 34.36ms, mfu 14.62%\n",
            "iter 3160: loss 0.9469, time 34.49ms, mfu 14.65%\n",
            "iter 3170: loss 0.8799, time 34.50ms, mfu 14.68%\n",
            "iter 3180: loss 0.9292, time 34.53ms, mfu 14.70%\n",
            "iter 3190: loss 0.9402, time 34.49ms, mfu 14.73%\n",
            "iter 3200: loss 0.9274, time 34.51ms, mfu 14.75%\n",
            "iter 3210: loss 0.9424, time 34.52ms, mfu 14.77%\n",
            "iter 3220: loss 0.9483, time 34.52ms, mfu 14.78%\n",
            "iter 3230: loss 0.9209, time 34.55ms, mfu 14.80%\n",
            "iter 3240: loss 0.9257, time 34.47ms, mfu 14.81%\n",
            "step 3250: train loss 0.7721, val loss 1.5960\n",
            "iter 3250: loss 0.9389, time 5542.92ms, mfu 13.34%\n",
            "iter 3260: loss 0.9443, time 34.53ms, mfu 13.50%\n",
            "iter 3270: loss 0.9124, time 34.32ms, mfu 13.65%\n",
            "iter 3280: loss 0.9177, time 34.47ms, mfu 13.78%\n",
            "iter 3290: loss 0.9177, time 34.50ms, mfu 13.89%\n",
            "iter 3300: loss 0.9164, time 34.46ms, mfu 14.00%\n",
            "iter 3310: loss 0.9162, time 34.53ms, mfu 14.09%\n",
            "iter 3320: loss 0.9021, time 34.56ms, mfu 14.17%\n",
            "iter 3330: loss 0.8981, time 34.49ms, mfu 14.25%\n",
            "iter 3340: loss 0.9202, time 34.61ms, mfu 14.32%\n",
            "iter 3350: loss 0.9022, time 34.53ms, mfu 14.38%\n",
            "iter 3360: loss 0.9204, time 34.42ms, mfu 14.44%\n",
            "iter 3370: loss 0.9125, time 34.59ms, mfu 14.48%\n",
            "iter 3380: loss 0.8885, time 34.49ms, mfu 14.53%\n",
            "iter 3390: loss 0.9105, time 34.51ms, mfu 14.57%\n",
            "iter 3400: loss 0.8857, time 34.44ms, mfu 14.61%\n",
            "iter 3410: loss 0.9008, time 34.54ms, mfu 14.64%\n",
            "iter 3420: loss 0.8742, time 34.51ms, mfu 14.67%\n",
            "iter 3430: loss 0.9052, time 34.54ms, mfu 14.69%\n",
            "iter 3440: loss 0.8907, time 34.47ms, mfu 14.72%\n",
            "iter 3450: loss 0.8776, time 34.51ms, mfu 14.74%\n",
            "iter 3460: loss 0.8723, time 34.48ms, mfu 14.76%\n",
            "iter 3470: loss 0.9234, time 34.58ms, mfu 14.78%\n",
            "iter 3480: loss 0.8598, time 34.50ms, mfu 14.79%\n",
            "iter 3490: loss 0.8731, time 34.55ms, mfu 14.80%\n",
            "step 3500: train loss 0.7225, val loss 1.6234\n",
            "iter 3500: loss 0.8746, time 5549.90ms, mfu 13.33%\n",
            "iter 3510: loss 0.8774, time 34.53ms, mfu 13.49%\n",
            "iter 3520: loss 0.8732, time 34.48ms, mfu 13.64%\n",
            "iter 3530: loss 0.8763, time 34.51ms, mfu 13.77%\n",
            "iter 3540: loss 0.8854, time 34.49ms, mfu 13.88%\n",
            "iter 3550: loss 0.8794, time 34.54ms, mfu 13.99%\n",
            "iter 3560: loss 0.8537, time 34.45ms, mfu 14.08%\n",
            "iter 3570: loss 0.9031, time 34.52ms, mfu 14.17%\n",
            "iter 3580: loss 0.8634, time 34.45ms, mfu 14.25%\n",
            "iter 3590: loss 0.8692, time 34.56ms, mfu 14.32%\n",
            "iter 3600: loss 0.8883, time 34.44ms, mfu 14.38%\n",
            "iter 3610: loss 0.8936, time 34.54ms, mfu 14.43%\n",
            "iter 3620: loss 0.8510, time 34.38ms, mfu 14.49%\n",
            "iter 3630: loss 0.8379, time 34.56ms, mfu 14.53%\n",
            "iter 3640: loss 0.8653, time 34.46ms, mfu 14.57%\n",
            "iter 3650: loss 0.8706, time 34.51ms, mfu 14.61%\n",
            "iter 3660: loss 0.8602, time 34.49ms, mfu 14.64%\n",
            "iter 3670: loss 0.8513, time 34.55ms, mfu 14.67%\n",
            "iter 3680: loss 0.8654, time 34.53ms, mfu 14.70%\n",
            "iter 3690: loss 0.8331, time 34.64ms, mfu 14.71%\n",
            "iter 3700: loss 0.8278, time 34.48ms, mfu 14.74%\n",
            "iter 3710: loss 0.8459, time 34.45ms, mfu 14.76%\n",
            "iter 3720: loss 0.8421, time 34.47ms, mfu 14.78%\n",
            "iter 3730: loss 0.8529, time 34.61ms, mfu 14.79%\n",
            "iter 3740: loss 0.8731, time 34.53ms, mfu 14.80%\n",
            "step 3750: train loss 0.6753, val loss 1.6629\n",
            "iter 3750: loss 0.8546, time 5552.57ms, mfu 13.33%\n",
            "iter 3760: loss 0.8328, time 34.49ms, mfu 13.49%\n",
            "iter 3770: loss 0.8495, time 34.54ms, mfu 13.64%\n",
            "iter 3780: loss 0.8447, time 34.08ms, mfu 13.78%\n",
            "iter 3790: loss 0.7978, time 34.55ms, mfu 13.90%\n",
            "iter 3800: loss 0.8608, time 34.38ms, mfu 14.01%\n",
            "iter 3810: loss 0.8312, time 34.34ms, mfu 14.11%\n",
            "iter 3820: loss 0.8543, time 34.56ms, mfu 14.19%\n",
            "iter 3830: loss 0.8700, time 34.30ms, mfu 14.27%\n",
            "iter 3840: loss 0.8409, time 34.56ms, mfu 14.34%\n",
            "iter 3850: loss 0.8266, time 34.42ms, mfu 14.40%\n",
            "iter 3860: loss 0.8324, time 34.50ms, mfu 14.45%\n",
            "iter 3870: loss 0.8428, time 34.49ms, mfu 14.50%\n",
            "iter 3880: loss 0.8441, time 34.58ms, mfu 14.54%\n",
            "iter 3890: loss 0.8440, time 34.58ms, mfu 14.58%\n",
            "iter 3900: loss 0.8289, time 34.53ms, mfu 14.61%\n",
            "iter 3910: loss 0.8109, time 34.52ms, mfu 14.64%\n",
            "iter 3920: loss 0.8257, time 34.46ms, mfu 14.68%\n",
            "iter 3930: loss 0.8332, time 34.53ms, mfu 14.70%\n",
            "iter 3940: loss 0.8133, time 34.46ms, mfu 14.73%\n",
            "iter 3950: loss 0.8420, time 34.53ms, mfu 14.75%\n",
            "iter 3960: loss 0.8301, time 34.50ms, mfu 14.77%\n",
            "iter 3970: loss 0.8472, time 34.53ms, mfu 14.78%\n",
            "iter 3980: loss 0.8189, time 34.54ms, mfu 14.79%\n",
            "iter 3990: loss 0.8153, time 34.50ms, mfu 14.81%\n",
            "step 4000: train loss 0.6367, val loss 1.6911\n",
            "iter 4000: loss 0.8395, time 5544.73ms, mfu 13.34%\n",
            "iter 4010: loss 0.8140, time 34.38ms, mfu 13.50%\n",
            "iter 4020: loss 0.8065, time 34.72ms, mfu 13.64%\n",
            "iter 4030: loss 0.8577, time 34.49ms, mfu 13.77%\n",
            "iter 4040: loss 0.8147, time 34.48ms, mfu 13.89%\n",
            "iter 4050: loss 0.8272, time 34.55ms, mfu 13.99%\n",
            "iter 4060: loss 0.8344, time 34.47ms, mfu 14.08%\n",
            "iter 4070: loss 0.8252, time 34.54ms, mfu 14.17%\n",
            "iter 4080: loss 0.8296, time 34.53ms, mfu 14.24%\n",
            "iter 4090: loss 0.8329, time 34.47ms, mfu 14.31%\n",
            "iter 4100: loss 0.8215, time 34.44ms, mfu 14.38%\n",
            "iter 4110: loss 0.8029, time 34.48ms, mfu 14.44%\n",
            "iter 4120: loss 0.8316, time 34.70ms, mfu 14.48%\n",
            "iter 4130: loss 0.7992, time 34.02ms, mfu 14.54%\n",
            "iter 4140: loss 0.8138, time 34.53ms, mfu 14.58%\n",
            "iter 4150: loss 0.8147, time 34.23ms, mfu 14.63%\n",
            "iter 4160: loss 0.8156, time 34.49ms, mfu 14.66%\n",
            "iter 4170: loss 0.8086, time 34.47ms, mfu 14.69%\n",
            "iter 4180: loss 0.7797, time 34.39ms, mfu 14.72%\n",
            "iter 4190: loss 0.8007, time 34.67ms, mfu 14.73%\n",
            "iter 4200: loss 0.7782, time 34.45ms, mfu 14.76%\n",
            "iter 4210: loss 0.8104, time 35.22ms, mfu 14.74%\n",
            "iter 4220: loss 0.8303, time 34.70ms, mfu 14.76%\n",
            "iter 4230: loss 0.7975, time 34.52ms, mfu 14.77%\n",
            "iter 4240: loss 0.7822, time 34.25ms, mfu 14.80%\n",
            "step 4250: train loss 0.6019, val loss 1.7218\n",
            "iter 4250: loss 0.7865, time 5548.87ms, mfu 13.33%\n",
            "iter 4260: loss 0.8048, time 34.51ms, mfu 13.49%\n",
            "iter 4270: loss 0.8000, time 34.44ms, mfu 13.64%\n",
            "iter 4280: loss 0.8058, time 34.49ms, mfu 13.77%\n",
            "iter 4290: loss 0.8004, time 34.42ms, mfu 13.89%\n",
            "iter 4300: loss 0.7840, time 34.49ms, mfu 13.99%\n",
            "iter 4310: loss 0.7876, time 34.84ms, mfu 14.07%\n",
            "iter 4320: loss 0.7745, time 34.65ms, mfu 14.15%\n",
            "iter 4330: loss 0.8014, time 34.50ms, mfu 14.23%\n",
            "iter 4340: loss 0.7784, time 34.35ms, mfu 14.31%\n",
            "iter 4350: loss 0.7991, time 34.51ms, mfu 14.37%\n",
            "iter 4360: loss 0.7815, time 34.46ms, mfu 14.43%\n",
            "iter 4370: loss 0.7730, time 34.46ms, mfu 14.48%\n",
            "iter 4380: loss 0.7828, time 34.61ms, mfu 14.52%\n",
            "iter 4390: loss 0.7846, time 34.49ms, mfu 14.57%\n",
            "iter 4400: loss 0.7914, time 34.51ms, mfu 14.60%\n",
            "iter 4410: loss 0.7901, time 34.47ms, mfu 14.64%\n",
            "iter 4420: loss 0.7843, time 34.50ms, mfu 14.67%\n",
            "iter 4430: loss 0.7579, time 34.38ms, mfu 14.70%\n",
            "iter 4440: loss 0.7761, time 34.46ms, mfu 14.72%\n",
            "iter 4450: loss 0.7781, time 34.59ms, mfu 14.74%\n",
            "iter 4460: loss 0.7628, time 34.53ms, mfu 14.76%\n",
            "iter 4470: loss 0.7604, time 34.42ms, mfu 14.78%\n",
            "iter 4480: loss 0.7690, time 34.49ms, mfu 14.80%\n",
            "iter 4490: loss 0.7795, time 34.49ms, mfu 14.81%\n",
            "step 4500: train loss 0.5725, val loss 1.7562\n",
            "iter 4500: loss 0.7984, time 5539.75ms, mfu 13.34%\n",
            "iter 4510: loss 0.7582, time 34.36ms, mfu 13.51%\n",
            "iter 4520: loss 0.7735, time 34.49ms, mfu 13.65%\n",
            "iter 4530: loss 0.7627, time 34.51ms, mfu 13.78%\n",
            "iter 4540: loss 0.7737, time 34.55ms, mfu 13.89%\n",
            "iter 4550: loss 0.7655, time 34.49ms, mfu 14.00%\n",
            "iter 4560: loss 0.7695, time 34.48ms, mfu 14.09%\n",
            "iter 4570: loss 0.7551, time 34.52ms, mfu 14.18%\n",
            "iter 4580: loss 0.7688, time 34.48ms, mfu 14.25%\n",
            "iter 4590: loss 0.7596, time 34.67ms, mfu 14.31%\n",
            "iter 4600: loss 0.7506, time 34.48ms, mfu 14.38%\n",
            "iter 4610: loss 0.7579, time 34.53ms, mfu 14.43%\n",
            "iter 4620: loss 0.7551, time 34.45ms, mfu 14.48%\n",
            "iter 4630: loss 0.7886, time 34.51ms, mfu 14.53%\n",
            "iter 4640: loss 0.7703, time 34.51ms, mfu 14.57%\n",
            "iter 4650: loss 0.7498, time 34.54ms, mfu 14.60%\n",
            "iter 4660: loss 0.7391, time 34.48ms, mfu 14.64%\n",
            "iter 4670: loss 0.7702, time 34.54ms, mfu 14.67%\n",
            "iter 4680: loss 0.7732, time 34.60ms, mfu 14.69%\n",
            "iter 4690: loss 0.7573, time 34.66ms, mfu 14.71%\n",
            "iter 4700: loss 0.7491, time 34.55ms, mfu 14.73%\n",
            "iter 4710: loss 0.7526, time 34.49ms, mfu 14.75%\n",
            "iter 4720: loss 0.7536, time 34.53ms, mfu 14.77%\n",
            "iter 4730: loss 0.7516, time 34.51ms, mfu 14.78%\n",
            "iter 4740: loss 0.7591, time 34.55ms, mfu 14.80%\n",
            "step 4750: train loss 0.5515, val loss 1.7815\n",
            "iter 4750: loss 0.7529, time 5547.74ms, mfu 13.33%\n",
            "iter 4760: loss 0.7560, time 34.58ms, mfu 13.48%\n",
            "iter 4770: loss 0.7380, time 34.51ms, mfu 13.63%\n",
            "iter 4780: loss 0.7335, time 34.53ms, mfu 13.76%\n",
            "iter 4790: loss 0.7517, time 34.45ms, mfu 13.88%\n",
            "iter 4800: loss 0.7451, time 34.69ms, mfu 13.98%\n",
            "iter 4810: loss 0.7646, time 34.53ms, mfu 14.07%\n",
            "iter 4820: loss 0.7621, time 34.61ms, mfu 14.15%\n",
            "iter 4830: loss 0.7541, time 34.51ms, mfu 14.23%\n",
            "iter 4840: loss 0.7663, time 34.52ms, mfu 14.30%\n",
            "iter 4850: loss 0.7565, time 34.59ms, mfu 14.36%\n",
            "iter 4860: loss 0.7372, time 34.59ms, mfu 14.42%\n",
            "iter 4870: loss 0.7848, time 34.55ms, mfu 14.47%\n",
            "iter 4880: loss 0.7472, time 34.39ms, mfu 14.52%\n",
            "iter 4890: loss 0.7419, time 34.37ms, mfu 14.56%\n",
            "iter 4900: loss 0.7430, time 34.50ms, mfu 14.60%\n",
            "iter 4910: loss 0.7610, time 34.51ms, mfu 14.64%\n",
            "iter 4920: loss 0.7629, time 34.52ms, mfu 14.66%\n",
            "iter 4930: loss 0.7387, time 34.48ms, mfu 14.69%\n",
            "iter 4940: loss 0.7309, time 34.56ms, mfu 14.71%\n",
            "iter 4950: loss 0.7360, time 34.51ms, mfu 14.74%\n",
            "iter 4960: loss 0.7548, time 34.59ms, mfu 14.75%\n",
            "iter 4970: loss 0.7691, time 34.48ms, mfu 14.77%\n",
            "iter 4980: loss 0.7436, time 34.52ms, mfu 14.79%\n",
            "iter 4990: loss 0.7317, time 34.45ms, mfu 14.81%\n",
            "step 5000: train loss 0.5333, val loss 1.7915\n",
            "iter 5000: loss 0.7243, time 5535.44ms, mfu 13.33%\n",
            "==== 训练 n_head=5 ====\n",
            "Overriding config with config/train_shakespeare_char_5head.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 7\n",
            "n_head = 5\n",
            "n_embd = 420\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "device = \"cuda\"\n",
            "compile = False\n",
            "\n",
            "Overriding: device = cuda\n",
            "Overriding: compile = False\n",
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 14.85M\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "num decayed parameter tensors: 30, with 14,952,420 parameters\n",
            "num non-decayed parameter tensors: 15, with 6,300 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.3037, val loss 4.2998\n",
            "iter 0: loss 4.2780, time 6018.68ms, mfu -100.00%\n",
            "iter 10: loss 3.1017, time 34.56ms, mfu 14.91%\n",
            "iter 20: loss 2.6925, time 34.65ms, mfu 14.91%\n",
            "iter 30: loss 2.5744, time 34.60ms, mfu 14.91%\n",
            "iter 40: loss 2.5621, time 34.62ms, mfu 14.90%\n",
            "iter 50: loss 2.5176, time 34.61ms, mfu 14.90%\n",
            "iter 60: loss 2.4904, time 34.67ms, mfu 14.90%\n",
            "iter 70: loss 2.4812, time 34.58ms, mfu 14.90%\n",
            "iter 80: loss 2.4914, time 34.66ms, mfu 14.90%\n",
            "iter 90: loss 2.4797, time 34.66ms, mfu 14.89%\n",
            "iter 100: loss 2.4505, time 34.64ms, mfu 14.89%\n",
            "iter 110: loss 2.4319, time 34.81ms, mfu 14.88%\n",
            "iter 120: loss 2.4163, time 34.36ms, mfu 14.89%\n",
            "iter 130: loss 2.3969, time 34.67ms, mfu 14.89%\n",
            "iter 140: loss 2.4150, time 34.56ms, mfu 14.89%\n",
            "iter 150: loss 2.3808, time 34.51ms, mfu 14.90%\n",
            "iter 160: loss 2.3571, time 34.63ms, mfu 14.90%\n",
            "iter 170: loss 2.2494, time 34.60ms, mfu 14.90%\n",
            "iter 180: loss 2.2387, time 34.69ms, mfu 14.89%\n",
            "iter 190: loss 2.1521, time 34.58ms, mfu 14.89%\n",
            "iter 200: loss 2.1299, time 34.60ms, mfu 14.89%\n",
            "iter 210: loss 2.0825, time 34.64ms, mfu 14.89%\n",
            "iter 220: loss 2.0553, time 34.62ms, mfu 14.89%\n",
            "iter 230: loss 2.0139, time 34.60ms, mfu 14.89%\n",
            "iter 240: loss 1.9731, time 34.63ms, mfu 14.89%\n",
            "step 250: train loss 1.9124, val loss 2.0410\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 250: loss 1.9671, time 5898.02ms, mfu 13.41%\n",
            "iter 260: loss 1.9207, time 34.60ms, mfu 13.56%\n",
            "iter 270: loss 1.9148, time 34.56ms, mfu 13.69%\n",
            "iter 280: loss 1.8729, time 34.65ms, mfu 13.81%\n",
            "iter 290: loss 1.8617, time 34.59ms, mfu 13.92%\n",
            "iter 300: loss 1.8051, time 34.63ms, mfu 14.02%\n",
            "iter 310: loss 1.8420, time 34.62ms, mfu 14.10%\n",
            "iter 320: loss 1.8002, time 34.62ms, mfu 14.18%\n",
            "iter 330: loss 1.8012, time 34.65ms, mfu 14.25%\n",
            "iter 340: loss 1.7646, time 34.54ms, mfu 14.32%\n",
            "iter 350: loss 1.7528, time 34.59ms, mfu 14.38%\n",
            "iter 360: loss 1.7460, time 34.67ms, mfu 14.43%\n",
            "iter 370: loss 1.7066, time 34.41ms, mfu 14.48%\n",
            "iter 380: loss 1.7057, time 34.63ms, mfu 14.52%\n",
            "iter 390: loss 1.7253, time 34.21ms, mfu 14.57%\n",
            "iter 400: loss 1.6909, time 34.60ms, mfu 14.61%\n",
            "iter 410: loss 1.6742, time 34.64ms, mfu 14.63%\n",
            "iter 420: loss 1.6682, time 34.66ms, mfu 14.66%\n",
            "iter 430: loss 1.6807, time 34.92ms, mfu 14.67%\n",
            "iter 440: loss 1.6108, time 34.74ms, mfu 14.68%\n",
            "iter 450: loss 1.6426, time 34.68ms, mfu 14.70%\n",
            "iter 460: loss 1.6389, time 34.51ms, mfu 14.72%\n",
            "iter 470: loss 1.6170, time 34.65ms, mfu 14.74%\n",
            "iter 480: loss 1.6120, time 34.60ms, mfu 14.76%\n",
            "iter 490: loss 1.6349, time 34.54ms, mfu 14.77%\n",
            "step 500: train loss 1.5076, val loss 1.7165\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 500: loss 1.5721, time 5875.71ms, mfu 13.30%\n",
            "iter 510: loss 1.5502, time 34.66ms, mfu 13.46%\n",
            "iter 520: loss 1.5779, time 34.66ms, mfu 13.60%\n",
            "iter 530: loss 1.5484, time 34.61ms, mfu 13.73%\n",
            "iter 540: loss 1.5598, time 34.60ms, mfu 13.85%\n",
            "iter 550: loss 1.5554, time 34.68ms, mfu 13.95%\n",
            "iter 560: loss 1.5479, time 34.64ms, mfu 14.04%\n",
            "iter 570: loss 1.5302, time 34.61ms, mfu 14.13%\n",
            "iter 580: loss 1.4824, time 34.66ms, mfu 14.20%\n",
            "iter 590: loss 1.5022, time 34.63ms, mfu 14.27%\n",
            "iter 600: loss 1.5203, time 34.80ms, mfu 14.32%\n",
            "iter 610: loss 1.4752, time 34.66ms, mfu 14.38%\n",
            "iter 620: loss 1.5067, time 34.73ms, mfu 14.42%\n",
            "iter 630: loss 1.4749, time 34.59ms, mfu 14.47%\n",
            "iter 640: loss 1.4884, time 34.56ms, mfu 14.51%\n",
            "iter 650: loss 1.4544, time 34.66ms, mfu 14.55%\n",
            "iter 660: loss 1.4744, time 34.64ms, mfu 14.58%\n",
            "iter 670: loss 1.4909, time 35.28ms, mfu 14.59%\n",
            "iter 680: loss 1.4728, time 34.48ms, mfu 14.62%\n",
            "iter 690: loss 1.4817, time 34.62ms, mfu 14.65%\n",
            "iter 700: loss 1.4577, time 34.63ms, mfu 14.67%\n",
            "iter 710: loss 1.4610, time 34.62ms, mfu 14.69%\n",
            "iter 720: loss 1.4448, time 34.60ms, mfu 14.71%\n",
            "iter 730: loss 1.4420, time 34.67ms, mfu 14.73%\n",
            "iter 740: loss 1.4137, time 34.62ms, mfu 14.74%\n",
            "step 750: train loss 1.3444, val loss 1.5772\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 750: loss 1.4274, time 5861.92ms, mfu 13.28%\n",
            "iter 760: loss 1.3855, time 34.62ms, mfu 13.44%\n",
            "iter 770: loss 1.4088, time 34.62ms, mfu 13.58%\n",
            "iter 780: loss 1.3980, time 34.58ms, mfu 13.72%\n",
            "iter 790: loss 1.3933, time 34.65ms, mfu 13.83%\n",
            "iter 800: loss 1.4055, time 34.59ms, mfu 13.94%\n",
            "iter 810: loss 1.3906, time 34.65ms, mfu 14.03%\n",
            "iter 820: loss 1.3861, time 34.60ms, mfu 14.12%\n",
            "iter 830: loss 1.4255, time 34.67ms, mfu 14.19%\n",
            "iter 840: loss 1.3825, time 34.51ms, mfu 14.27%\n",
            "iter 850: loss 1.3899, time 34.69ms, mfu 14.33%\n",
            "iter 860: loss 1.3604, time 34.54ms, mfu 14.38%\n",
            "iter 870: loss 1.3672, time 34.51ms, mfu 14.44%\n",
            "iter 880: loss 1.3571, time 34.65ms, mfu 14.48%\n",
            "iter 890: loss 1.3709, time 34.61ms, mfu 14.52%\n",
            "iter 900: loss 1.3296, time 34.64ms, mfu 14.56%\n",
            "iter 910: loss 1.3592, time 34.65ms, mfu 14.59%\n",
            "iter 920: loss 1.3324, time 34.59ms, mfu 14.62%\n",
            "iter 930: loss 1.3580, time 34.63ms, mfu 14.65%\n",
            "iter 940: loss 1.3179, time 34.66ms, mfu 14.67%\n",
            "iter 950: loss 1.3465, time 34.53ms, mfu 14.70%\n",
            "iter 960: loss 1.2743, time 34.64ms, mfu 14.71%\n",
            "iter 970: loss 1.3310, time 34.62ms, mfu 14.73%\n",
            "iter 980: loss 1.3427, time 34.64ms, mfu 14.74%\n",
            "iter 990: loss 1.3166, time 34.39ms, mfu 14.77%\n",
            "step 1000: train loss 1.2519, val loss 1.5112\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1000: loss 1.3199, time 5874.23ms, mfu 13.30%\n",
            "iter 1010: loss 1.3079, time 34.56ms, mfu 13.46%\n",
            "iter 1020: loss 1.3342, time 34.78ms, mfu 13.60%\n",
            "iter 1030: loss 1.3308, time 34.50ms, mfu 13.73%\n",
            "iter 1040: loss 1.3254, time 34.63ms, mfu 13.85%\n",
            "iter 1050: loss 1.3076, time 34.58ms, mfu 13.95%\n",
            "iter 1060: loss 1.3090, time 34.35ms, mfu 14.06%\n",
            "iter 1070: loss 1.3003, time 34.67ms, mfu 14.14%\n",
            "iter 1080: loss 1.3010, time 34.63ms, mfu 14.21%\n",
            "iter 1090: loss 1.2964, time 34.67ms, mfu 14.28%\n",
            "iter 1100: loss 1.3050, time 34.66ms, mfu 14.34%\n",
            "iter 1110: loss 1.3244, time 34.58ms, mfu 14.39%\n",
            "iter 1120: loss 1.2508, time 34.77ms, mfu 14.44%\n",
            "iter 1130: loss 1.2841, time 34.63ms, mfu 14.48%\n",
            "iter 1140: loss 1.2909, time 34.67ms, mfu 14.52%\n",
            "iter 1150: loss 1.2958, time 34.58ms, mfu 14.56%\n",
            "iter 1160: loss 1.2829, time 34.64ms, mfu 14.59%\n",
            "iter 1170: loss 1.2785, time 34.64ms, mfu 14.62%\n",
            "iter 1180: loss 1.2745, time 34.62ms, mfu 14.64%\n",
            "iter 1190: loss 1.2522, time 34.62ms, mfu 14.67%\n",
            "iter 1200: loss 1.3003, time 34.62ms, mfu 14.69%\n",
            "iter 1210: loss 1.2963, time 34.65ms, mfu 14.71%\n",
            "iter 1220: loss 1.2868, time 34.60ms, mfu 14.73%\n",
            "iter 1230: loss 1.2829, time 34.64ms, mfu 14.74%\n",
            "iter 1240: loss 1.2710, time 34.63ms, mfu 14.76%\n",
            "step 1250: train loss 1.1831, val loss 1.4830\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1250: loss 1.2509, time 5879.20ms, mfu 13.29%\n",
            "iter 1260: loss 1.2749, time 34.63ms, mfu 13.45%\n",
            "iter 1270: loss 1.2694, time 34.37ms, mfu 13.60%\n",
            "iter 1280: loss 1.2746, time 34.60ms, mfu 13.73%\n",
            "iter 1290: loss 1.2613, time 34.66ms, mfu 13.85%\n",
            "iter 1300: loss 1.2791, time 34.69ms, mfu 13.95%\n",
            "iter 1310: loss 1.1990, time 34.57ms, mfu 14.04%\n",
            "iter 1320: loss 1.2546, time 34.64ms, mfu 14.13%\n",
            "iter 1330: loss 1.2286, time 34.61ms, mfu 14.20%\n",
            "iter 1340: loss 1.2542, time 35.03ms, mfu 14.25%\n",
            "iter 1350: loss 1.2514, time 34.56ms, mfu 14.32%\n",
            "iter 1360: loss 1.2076, time 34.60ms, mfu 14.38%\n",
            "iter 1370: loss 1.2372, time 34.60ms, mfu 14.43%\n",
            "iter 1380: loss 1.2091, time 34.64ms, mfu 14.47%\n",
            "iter 1390: loss 1.2186, time 34.61ms, mfu 14.52%\n",
            "iter 1400: loss 1.1928, time 34.64ms, mfu 14.55%\n",
            "iter 1410: loss 1.2422, time 34.65ms, mfu 14.58%\n",
            "iter 1420: loss 1.2177, time 34.60ms, mfu 14.62%\n",
            "iter 1430: loss 1.2408, time 34.68ms, mfu 14.64%\n",
            "iter 1440: loss 1.1842, time 34.66ms, mfu 14.66%\n",
            "iter 1450: loss 1.2158, time 34.65ms, mfu 14.68%\n",
            "iter 1460: loss 1.2320, time 34.56ms, mfu 14.71%\n",
            "iter 1470: loss 1.1983, time 34.62ms, mfu 14.72%\n",
            "iter 1480: loss 1.2055, time 34.61ms, mfu 14.74%\n",
            "iter 1490: loss 1.1794, time 34.62ms, mfu 14.76%\n",
            "step 1500: train loss 1.1218, val loss 1.4812\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1500: loss 1.1983, time 5909.79ms, mfu 13.29%\n",
            "iter 1510: loss 1.1975, time 34.65ms, mfu 13.45%\n",
            "iter 1520: loss 1.1932, time 34.58ms, mfu 13.59%\n",
            "iter 1530: loss 1.1795, time 34.64ms, mfu 13.72%\n",
            "iter 1540: loss 1.1814, time 34.57ms, mfu 13.84%\n",
            "iter 1550: loss 1.1916, time 34.58ms, mfu 13.95%\n",
            "iter 1560: loss 1.1902, time 34.64ms, mfu 14.04%\n",
            "iter 1570: loss 1.2199, time 34.62ms, mfu 14.12%\n",
            "iter 1580: loss 1.1649, time 34.58ms, mfu 14.20%\n",
            "iter 1590: loss 1.1468, time 34.63ms, mfu 14.27%\n",
            "iter 1600: loss 1.1801, time 34.62ms, mfu 14.33%\n",
            "iter 1610: loss 1.1767, time 34.64ms, mfu 14.39%\n",
            "iter 1620: loss 1.1406, time 34.63ms, mfu 14.44%\n",
            "iter 1630: loss 1.1773, time 34.64ms, mfu 14.48%\n",
            "iter 1640: loss 1.1656, time 34.61ms, mfu 14.52%\n",
            "iter 1650: loss 1.1858, time 34.37ms, mfu 14.57%\n",
            "iter 1660: loss 1.1601, time 34.61ms, mfu 14.60%\n",
            "iter 1670: loss 1.1890, time 34.64ms, mfu 14.63%\n",
            "iter 1680: loss 1.1428, time 34.62ms, mfu 14.65%\n",
            "iter 1690: loss 1.1585, time 34.78ms, mfu 14.67%\n",
            "iter 1700: loss 1.1662, time 34.62ms, mfu 14.69%\n",
            "iter 1710: loss 1.1552, time 34.67ms, mfu 14.71%\n",
            "iter 1720: loss 1.1477, time 34.56ms, mfu 14.73%\n",
            "iter 1730: loss 1.1458, time 34.71ms, mfu 14.74%\n",
            "iter 1740: loss 1.1736, time 34.59ms, mfu 14.76%\n",
            "step 1750: train loss 1.0595, val loss 1.4830\n",
            "iter 1750: loss 1.1385, time 5544.04ms, mfu 13.29%\n",
            "iter 1760: loss 1.1498, time 34.64ms, mfu 13.45%\n",
            "iter 1770: loss 1.1525, time 34.65ms, mfu 13.59%\n",
            "iter 1780: loss 1.1169, time 34.90ms, mfu 13.71%\n",
            "iter 1790: loss 1.1239, time 34.64ms, mfu 13.83%\n",
            "iter 1800: loss 1.1260, time 34.61ms, mfu 13.93%\n",
            "iter 1810: loss 1.1815, time 34.57ms, mfu 14.03%\n",
            "iter 1820: loss 1.1277, time 34.63ms, mfu 14.11%\n",
            "iter 1830: loss 1.1471, time 34.61ms, mfu 14.19%\n",
            "iter 1840: loss 1.1084, time 34.55ms, mfu 14.27%\n",
            "iter 1850: loss 1.1691, time 34.53ms, mfu 14.33%\n",
            "iter 1860: loss 1.1062, time 34.52ms, mfu 14.39%\n",
            "iter 1870: loss 1.1413, time 34.64ms, mfu 14.44%\n",
            "iter 1880: loss 1.1077, time 34.28ms, mfu 14.50%\n",
            "iter 1890: loss 1.1449, time 34.61ms, mfu 14.54%\n",
            "iter 1900: loss 1.1200, time 34.66ms, mfu 14.57%\n",
            "iter 1910: loss 1.0994, time 34.65ms, mfu 14.60%\n",
            "iter 1920: loss 1.0850, time 34.60ms, mfu 14.63%\n",
            "iter 1930: loss 1.1141, time 34.59ms, mfu 14.66%\n",
            "iter 1940: loss 1.1301, time 34.77ms, mfu 14.67%\n",
            "iter 1950: loss 1.1244, time 34.64ms, mfu 14.69%\n",
            "iter 1960: loss 1.1268, time 34.65ms, mfu 14.71%\n",
            "iter 1970: loss 1.1078, time 34.56ms, mfu 14.73%\n",
            "iter 1980: loss 1.1087, time 34.66ms, mfu 14.75%\n",
            "iter 1990: loss 1.0884, time 34.57ms, mfu 14.76%\n",
            "step 2000: train loss 1.0042, val loss 1.4930\n",
            "iter 2000: loss 1.0978, time 5545.72ms, mfu 13.29%\n",
            "iter 2010: loss 1.1090, time 34.48ms, mfu 13.46%\n",
            "iter 2020: loss 1.0732, time 34.61ms, mfu 13.60%\n",
            "iter 2030: loss 1.0924, time 34.62ms, mfu 13.73%\n",
            "iter 2040: loss 1.0793, time 34.65ms, mfu 13.85%\n",
            "iter 2050: loss 1.0688, time 34.60ms, mfu 13.95%\n",
            "iter 2060: loss 1.0934, time 34.63ms, mfu 14.04%\n",
            "iter 2070: loss 1.0608, time 34.68ms, mfu 14.13%\n",
            "iter 2080: loss 1.0877, time 34.76ms, mfu 14.20%\n",
            "iter 2090: loss 1.0867, time 34.56ms, mfu 14.27%\n",
            "iter 2100: loss 1.0722, time 34.50ms, mfu 14.33%\n",
            "iter 2110: loss 1.0894, time 34.63ms, mfu 14.39%\n",
            "iter 2120: loss 1.0796, time 34.68ms, mfu 14.44%\n",
            "iter 2130: loss 1.0789, time 35.15ms, mfu 14.46%\n",
            "iter 2140: loss 1.0571, time 34.54ms, mfu 14.50%\n",
            "iter 2150: loss 1.0621, time 34.64ms, mfu 14.54%\n",
            "iter 2160: loss 1.0834, time 34.59ms, mfu 14.58%\n",
            "iter 2170: loss 1.0458, time 34.63ms, mfu 14.61%\n",
            "iter 2180: loss 1.0602, time 34.60ms, mfu 14.64%\n",
            "iter 2190: loss 1.0347, time 34.60ms, mfu 14.66%\n",
            "iter 2200: loss 1.0569, time 34.59ms, mfu 14.69%\n",
            "iter 2210: loss 1.0621, time 34.65ms, mfu 14.70%\n",
            "iter 2220: loss 1.0614, time 34.60ms, mfu 14.72%\n",
            "iter 2230: loss 1.0487, time 34.62ms, mfu 14.74%\n",
            "iter 2240: loss 1.0818, time 34.62ms, mfu 14.75%\n",
            "step 2250: train loss 0.9391, val loss 1.4920\n",
            "iter 2250: loss 1.0745, time 5549.70ms, mfu 13.29%\n",
            "iter 2260: loss 1.0393, time 34.63ms, mfu 13.45%\n",
            "iter 2270: loss 1.0561, time 34.59ms, mfu 13.59%\n",
            "iter 2280: loss 1.0468, time 34.61ms, mfu 13.72%\n",
            "iter 2290: loss 1.0312, time 34.62ms, mfu 13.84%\n",
            "iter 2300: loss 1.0277, time 34.60ms, mfu 13.94%\n",
            "iter 2310: loss 1.0224, time 34.64ms, mfu 14.04%\n",
            "iter 2320: loss 1.0483, time 34.56ms, mfu 14.13%\n",
            "iter 2330: loss 1.0112, time 34.77ms, mfu 14.19%\n",
            "iter 2340: loss 1.0460, time 34.58ms, mfu 14.27%\n",
            "iter 2350: loss 1.0548, time 34.70ms, mfu 14.32%\n",
            "iter 2360: loss 1.0083, time 34.66ms, mfu 14.38%\n",
            "iter 2370: loss 1.0190, time 34.63ms, mfu 14.43%\n",
            "iter 2380: loss 1.0078, time 34.61ms, mfu 14.48%\n",
            "iter 2390: loss 1.0079, time 34.68ms, mfu 14.51%\n",
            "iter 2400: loss 1.0088, time 34.60ms, mfu 14.55%\n",
            "iter 2410: loss 1.0020, time 34.64ms, mfu 14.58%\n",
            "iter 2420: loss 0.9790, time 34.62ms, mfu 14.61%\n",
            "iter 2430: loss 1.0158, time 34.59ms, mfu 14.64%\n",
            "iter 2440: loss 1.0129, time 34.62ms, mfu 14.67%\n",
            "iter 2450: loss 0.9992, time 34.65ms, mfu 14.69%\n",
            "iter 2460: loss 1.0157, time 34.58ms, mfu 14.71%\n",
            "iter 2470: loss 0.9935, time 34.61ms, mfu 14.73%\n",
            "iter 2480: loss 1.0196, time 34.58ms, mfu 14.75%\n",
            "iter 2490: loss 0.9881, time 34.63ms, mfu 14.76%\n",
            "step 2500: train loss 0.8804, val loss 1.5285\n",
            "iter 2500: loss 1.0089, time 5549.53ms, mfu 13.29%\n",
            "iter 2510: loss 0.9845, time 34.55ms, mfu 13.46%\n",
            "iter 2520: loss 0.9981, time 34.69ms, mfu 13.60%\n",
            "iter 2530: loss 1.0014, time 34.56ms, mfu 13.73%\n",
            "iter 2540: loss 0.9846, time 34.63ms, mfu 13.84%\n",
            "iter 2550: loss 0.9898, time 34.65ms, mfu 13.95%\n",
            "iter 2560: loss 0.9559, time 34.52ms, mfu 14.04%\n",
            "iter 2570: loss 0.9721, time 34.55ms, mfu 14.13%\n",
            "iter 2580: loss 0.9977, time 34.62ms, mfu 14.21%\n",
            "iter 2590: loss 1.0093, time 34.38ms, mfu 14.29%\n",
            "iter 2600: loss 0.9689, time 34.61ms, mfu 14.35%\n",
            "iter 2610: loss 0.9794, time 34.53ms, mfu 14.40%\n",
            "iter 2620: loss 0.9934, time 34.63ms, mfu 14.45%\n",
            "iter 2630: loss 0.9594, time 34.63ms, mfu 14.49%\n",
            "iter 2640: loss 0.9663, time 34.60ms, mfu 14.53%\n",
            "iter 2650: loss 0.9416, time 34.68ms, mfu 14.57%\n",
            "iter 2660: loss 0.9152, time 34.57ms, mfu 14.60%\n",
            "iter 2670: loss 0.9626, time 34.63ms, mfu 14.63%\n",
            "iter 2680: loss 0.9548, time 34.58ms, mfu 14.66%\n",
            "iter 2690: loss 0.9552, time 34.63ms, mfu 14.68%\n",
            "iter 2700: loss 0.9734, time 34.69ms, mfu 14.70%\n",
            "iter 2710: loss 0.9498, time 34.60ms, mfu 14.72%\n",
            "iter 2720: loss 0.9567, time 34.58ms, mfu 14.74%\n",
            "iter 2730: loss 0.9619, time 34.59ms, mfu 14.75%\n",
            "iter 2740: loss 0.9504, time 34.55ms, mfu 14.77%\n",
            "step 2750: train loss 0.8177, val loss 1.5615\n",
            "iter 2750: loss 0.9449, time 5552.97ms, mfu 13.30%\n",
            "iter 2760: loss 0.9269, time 34.64ms, mfu 13.46%\n",
            "iter 2770: loss 0.9448, time 34.64ms, mfu 13.60%\n",
            "iter 2780: loss 0.9253, time 34.66ms, mfu 13.73%\n",
            "iter 2790: loss 0.9340, time 34.56ms, mfu 13.85%\n",
            "iter 2800: loss 0.9207, time 34.63ms, mfu 13.95%\n",
            "iter 2810: loss 0.9416, time 34.87ms, mfu 14.03%\n",
            "iter 2820: loss 0.9306, time 34.64ms, mfu 14.12%\n",
            "iter 2830: loss 0.9370, time 34.70ms, mfu 14.19%\n",
            "iter 2840: loss 0.9507, time 34.58ms, mfu 14.26%\n",
            "iter 2850: loss 0.9317, time 34.59ms, mfu 14.33%\n",
            "iter 2860: loss 0.9185, time 34.60ms, mfu 14.38%\n",
            "iter 2870: loss 0.9057, time 34.64ms, mfu 14.43%\n",
            "iter 2880: loss 0.9326, time 34.63ms, mfu 14.48%\n",
            "iter 2890: loss 0.9137, time 34.61ms, mfu 14.52%\n",
            "iter 2900: loss 0.9568, time 34.64ms, mfu 14.55%\n",
            "iter 2910: loss 0.9257, time 34.62ms, mfu 14.59%\n",
            "iter 2920: loss 0.9297, time 34.63ms, mfu 14.62%\n",
            "iter 2930: loss 0.9256, time 34.57ms, mfu 14.65%\n",
            "iter 2940: loss 0.9148, time 34.65ms, mfu 14.67%\n",
            "iter 2950: loss 0.9191, time 34.84ms, mfu 14.68%\n",
            "iter 2960: loss 0.8970, time 34.64ms, mfu 14.70%\n",
            "iter 2970: loss 0.9141, time 34.65ms, mfu 14.72%\n",
            "iter 2980: loss 0.8895, time 34.64ms, mfu 14.73%\n",
            "iter 2990: loss 0.9119, time 34.56ms, mfu 14.75%\n",
            "step 3000: train loss 0.7535, val loss 1.6012\n",
            "iter 3000: loss 0.9315, time 5549.13ms, mfu 13.29%\n",
            "iter 3010: loss 0.8953, time 34.54ms, mfu 13.45%\n",
            "iter 3020: loss 0.8958, time 34.64ms, mfu 13.59%\n",
            "iter 3030: loss 0.8952, time 34.62ms, mfu 13.72%\n",
            "iter 3040: loss 0.9022, time 34.64ms, mfu 13.84%\n",
            "iter 3050: loss 0.8851, time 34.59ms, mfu 13.94%\n",
            "iter 3060: loss 0.9027, time 34.67ms, mfu 14.04%\n",
            "iter 3070: loss 0.8939, time 34.58ms, mfu 14.12%\n",
            "iter 3080: loss 0.9151, time 34.73ms, mfu 14.19%\n",
            "iter 3090: loss 0.8925, time 34.56ms, mfu 14.27%\n",
            "iter 3100: loss 0.9226, time 34.56ms, mfu 14.33%\n",
            "iter 3110: loss 0.8432, time 34.57ms, mfu 14.39%\n",
            "iter 3120: loss 0.8902, time 34.59ms, mfu 14.44%\n",
            "iter 3130: loss 0.8432, time 34.64ms, mfu 14.48%\n",
            "iter 3140: loss 0.8814, time 34.64ms, mfu 14.52%\n",
            "iter 3150: loss 0.8531, time 34.61ms, mfu 14.56%\n",
            "iter 3160: loss 0.8833, time 34.64ms, mfu 14.59%\n",
            "iter 3170: loss 0.8201, time 34.63ms, mfu 14.62%\n",
            "iter 3180: loss 0.8708, time 34.63ms, mfu 14.65%\n",
            "iter 3190: loss 0.8793, time 34.60ms, mfu 14.67%\n",
            "iter 3200: loss 0.8672, time 34.55ms, mfu 14.70%\n",
            "iter 3210: loss 0.8690, time 34.77ms, mfu 14.71%\n",
            "iter 3220: loss 0.8775, time 34.73ms, mfu 14.72%\n",
            "iter 3230: loss 0.8538, time 34.76ms, mfu 14.73%\n",
            "iter 3240: loss 0.8678, time 34.39ms, mfu 14.76%\n",
            "step 3250: train loss 0.6957, val loss 1.6427\n",
            "iter 3250: loss 0.8703, time 5559.24ms, mfu 13.29%\n",
            "iter 3260: loss 0.8684, time 34.66ms, mfu 13.45%\n",
            "iter 3270: loss 0.8435, time 34.45ms, mfu 13.60%\n",
            "iter 3280: loss 0.8525, time 34.70ms, mfu 13.72%\n",
            "iter 3290: loss 0.8612, time 34.62ms, mfu 13.84%\n",
            "iter 3300: loss 0.8475, time 34.63ms, mfu 13.95%\n",
            "iter 3310: loss 0.8505, time 34.64ms, mfu 14.04%\n",
            "iter 3320: loss 0.8355, time 34.57ms, mfu 14.13%\n",
            "iter 3330: loss 0.8299, time 34.68ms, mfu 14.20%\n",
            "iter 3340: loss 0.8507, time 34.59ms, mfu 14.27%\n",
            "iter 3350: loss 0.8415, time 34.63ms, mfu 14.33%\n",
            "iter 3360: loss 0.8497, time 34.76ms, mfu 14.38%\n",
            "iter 3370: loss 0.8511, time 34.56ms, mfu 14.43%\n",
            "iter 3380: loss 0.8280, time 34.62ms, mfu 14.48%\n",
            "iter 3390: loss 0.8479, time 34.38ms, mfu 14.53%\n",
            "iter 3400: loss 0.8216, time 34.65ms, mfu 14.56%\n",
            "iter 3410: loss 0.8286, time 34.59ms, mfu 14.60%\n",
            "iter 3420: loss 0.8067, time 34.62ms, mfu 14.63%\n",
            "iter 3430: loss 0.8388, time 34.54ms, mfu 14.66%\n",
            "iter 3440: loss 0.8147, time 34.68ms, mfu 14.68%\n",
            "iter 3450: loss 0.8289, time 34.63ms, mfu 14.70%\n",
            "iter 3460: loss 0.7978, time 34.60ms, mfu 14.72%\n",
            "iter 3470: loss 0.8503, time 34.71ms, mfu 14.73%\n",
            "iter 3480: loss 0.7912, time 34.74ms, mfu 14.74%\n",
            "iter 3490: loss 0.8164, time 34.59ms, mfu 14.76%\n",
            "step 3500: train loss 0.6352, val loss 1.7052\n",
            "iter 3500: loss 0.8094, time 5565.60ms, mfu 13.29%\n",
            "iter 3510: loss 0.8112, time 34.64ms, mfu 13.45%\n",
            "iter 3520: loss 0.8089, time 34.67ms, mfu 13.59%\n",
            "iter 3530: loss 0.8077, time 34.56ms, mfu 13.72%\n",
            "iter 3540: loss 0.8194, time 34.60ms, mfu 13.84%\n",
            "iter 3550: loss 0.8049, time 34.60ms, mfu 13.94%\n",
            "iter 3560: loss 0.7835, time 34.61ms, mfu 14.04%\n",
            "iter 3570: loss 0.8308, time 34.69ms, mfu 14.12%\n",
            "iter 3580: loss 0.7900, time 34.57ms, mfu 14.20%\n",
            "iter 3590: loss 0.7992, time 34.24ms, mfu 14.29%\n",
            "iter 3600: loss 0.8217, time 34.71ms, mfu 14.34%\n",
            "iter 3610: loss 0.8339, time 34.66ms, mfu 14.39%\n",
            "iter 3620: loss 0.7820, time 34.51ms, mfu 14.45%\n",
            "iter 3630: loss 0.7725, time 34.59ms, mfu 14.49%\n",
            "iter 3640: loss 0.7992, time 34.60ms, mfu 14.53%\n",
            "iter 3650: loss 0.8003, time 34.60ms, mfu 14.57%\n",
            "iter 3660: loss 0.7873, time 34.77ms, mfu 14.59%\n",
            "iter 3670: loss 0.7785, time 34.54ms, mfu 14.63%\n",
            "iter 3680: loss 0.7970, time 34.57ms, mfu 14.66%\n",
            "iter 3690: loss 0.7572, time 34.66ms, mfu 14.68%\n",
            "iter 3700: loss 0.7667, time 34.64ms, mfu 14.70%\n",
            "iter 3710: loss 0.7789, time 34.67ms, mfu 14.71%\n",
            "iter 3720: loss 0.7722, time 34.62ms, mfu 14.73%\n",
            "iter 3730: loss 0.7793, time 34.61ms, mfu 14.75%\n",
            "iter 3740: loss 0.8018, time 34.59ms, mfu 14.76%\n",
            "step 3750: train loss 0.5874, val loss 1.7385\n",
            "iter 3750: loss 0.7793, time 5569.13ms, mfu 13.30%\n",
            "iter 3760: loss 0.7723, time 34.67ms, mfu 13.45%\n",
            "iter 3770: loss 0.7848, time 34.63ms, mfu 13.60%\n",
            "iter 3780: loss 0.7688, time 34.59ms, mfu 13.73%\n",
            "iter 3790: loss 0.7417, time 34.54ms, mfu 13.84%\n",
            "iter 3800: loss 0.7808, time 34.65ms, mfu 13.95%\n",
            "iter 3810: loss 0.7551, time 34.47ms, mfu 14.05%\n",
            "iter 3820: loss 0.7909, time 34.55ms, mfu 14.13%\n",
            "iter 3830: loss 0.7790, time 34.58ms, mfu 14.21%\n",
            "iter 3840: loss 0.7661, time 34.61ms, mfu 14.28%\n",
            "iter 3850: loss 0.7532, time 34.61ms, mfu 14.34%\n",
            "iter 3860: loss 0.7496, time 34.64ms, mfu 14.39%\n",
            "iter 3870: loss 0.7737, time 35.26ms, mfu 14.42%\n",
            "iter 3880: loss 0.7764, time 34.59ms, mfu 14.46%\n",
            "iter 3890: loss 0.7701, time 34.67ms, mfu 14.50%\n",
            "iter 3900: loss 0.7614, time 34.64ms, mfu 14.54%\n",
            "iter 3910: loss 0.7451, time 34.60ms, mfu 14.58%\n",
            "iter 3920: loss 0.7606, time 34.61ms, mfu 14.61%\n",
            "iter 3930: loss 0.7564, time 35.05ms, mfu 14.62%\n",
            "iter 3940: loss 0.7535, time 34.55ms, mfu 14.65%\n",
            "iter 3950: loss 0.7640, time 34.59ms, mfu 14.67%\n",
            "iter 3960: loss 0.7572, time 34.48ms, mfu 14.70%\n",
            "iter 3970: loss 0.7821, time 34.68ms, mfu 14.72%\n",
            "iter 3980: loss 0.7625, time 34.58ms, mfu 14.74%\n",
            "iter 3990: loss 0.7270, time 34.64ms, mfu 14.75%\n",
            "step 4000: train loss 0.5459, val loss 1.7753\n",
            "iter 4000: loss 0.7584, time 5565.26ms, mfu 13.28%\n",
            "iter 4010: loss 0.7424, time 34.15ms, mfu 13.46%\n",
            "iter 4020: loss 0.7349, time 34.62ms, mfu 13.61%\n",
            "iter 4030: loss 0.7725, time 34.74ms, mfu 13.73%\n",
            "iter 4040: loss 0.7367, time 34.62ms, mfu 13.85%\n",
            "iter 4050: loss 0.7511, time 34.54ms, mfu 13.95%\n",
            "iter 4060: loss 0.7530, time 34.66ms, mfu 14.04%\n",
            "iter 4070: loss 0.7491, time 34.52ms, mfu 14.13%\n",
            "iter 4080: loss 0.7524, time 34.62ms, mfu 14.21%\n",
            "iter 4090: loss 0.7609, time 34.79ms, mfu 14.27%\n",
            "iter 4100: loss 0.7441, time 34.52ms, mfu 14.33%\n",
            "iter 4110: loss 0.7363, time 34.57ms, mfu 14.39%\n",
            "iter 4120: loss 0.7404, time 34.52ms, mfu 14.45%\n",
            "iter 4130: loss 0.7239, time 34.60ms, mfu 14.49%\n",
            "iter 4140: loss 0.7392, time 34.78ms, mfu 14.52%\n",
            "iter 4150: loss 0.7348, time 34.62ms, mfu 14.56%\n",
            "iter 4160: loss 0.7320, time 34.72ms, mfu 14.59%\n",
            "iter 4170: loss 0.7313, time 34.80ms, mfu 14.61%\n",
            "iter 4180: loss 0.7089, time 34.61ms, mfu 14.64%\n",
            "iter 4190: loss 0.7293, time 34.53ms, mfu 14.67%\n",
            "iter 4200: loss 0.7020, time 34.71ms, mfu 14.68%\n",
            "iter 4210: loss 0.7307, time 34.58ms, mfu 14.71%\n",
            "iter 4220: loss 0.7474, time 34.60ms, mfu 14.73%\n",
            "iter 4230: loss 0.7180, time 34.56ms, mfu 14.74%\n",
            "iter 4240: loss 0.7056, time 34.63ms, mfu 14.76%\n",
            "step 4250: train loss 0.5109, val loss 1.8095\n",
            "iter 4250: loss 0.7121, time 5574.95ms, mfu 13.29%\n",
            "iter 4260: loss 0.7184, time 34.60ms, mfu 13.45%\n",
            "iter 4270: loss 0.7305, time 34.80ms, mfu 13.59%\n",
            "iter 4280: loss 0.7268, time 34.58ms, mfu 13.72%\n",
            "iter 4290: loss 0.7284, time 34.58ms, mfu 13.84%\n",
            "iter 4300: loss 0.7073, time 34.52ms, mfu 13.95%\n",
            "iter 4310: loss 0.7051, time 34.62ms, mfu 14.04%\n",
            "iter 4320: loss 0.7010, time 34.62ms, mfu 14.13%\n",
            "iter 4330: loss 0.7188, time 34.61ms, mfu 14.20%\n",
            "iter 4340: loss 0.7029, time 34.58ms, mfu 14.27%\n",
            "iter 4350: loss 0.7178, time 34.56ms, mfu 14.34%\n",
            "iter 4360: loss 0.7124, time 34.59ms, mfu 14.39%\n",
            "iter 4370: loss 0.7017, time 34.45ms, mfu 14.45%\n",
            "iter 4380: loss 0.7059, time 34.61ms, mfu 14.49%\n",
            "iter 4390: loss 0.7161, time 34.86ms, mfu 14.52%\n",
            "iter 4400: loss 0.7163, time 34.80ms, mfu 14.55%\n",
            "iter 4410: loss 0.7074, time 34.45ms, mfu 14.59%\n",
            "iter 4420: loss 0.7046, time 34.66ms, mfu 14.62%\n",
            "iter 4430: loss 0.6891, time 34.53ms, mfu 14.65%\n",
            "iter 4440: loss 0.7028, time 34.49ms, mfu 14.68%\n",
            "iter 4450: loss 0.7105, time 34.61ms, mfu 14.70%\n",
            "iter 4460: loss 0.7023, time 34.68ms, mfu 14.72%\n",
            "iter 4470: loss 0.6913, time 34.66ms, mfu 14.73%\n",
            "iter 4480: loss 0.6929, time 34.56ms, mfu 14.75%\n",
            "iter 4490: loss 0.7092, time 34.46ms, mfu 14.77%\n",
            "step 4500: train loss 0.4802, val loss 1.8402\n",
            "iter 4500: loss 0.7287, time 5574.45ms, mfu 13.30%\n",
            "iter 4510: loss 0.6920, time 34.60ms, mfu 13.46%\n",
            "iter 4520: loss 0.6896, time 34.59ms, mfu 13.61%\n",
            "iter 4530: loss 0.6916, time 34.76ms, mfu 13.73%\n",
            "iter 4540: loss 0.6926, time 34.56ms, mfu 13.85%\n",
            "iter 4550: loss 0.6988, time 34.87ms, mfu 13.94%\n",
            "iter 4560: loss 0.6959, time 34.70ms, mfu 14.03%\n",
            "iter 4570: loss 0.6863, time 34.57ms, mfu 14.12%\n",
            "iter 4580: loss 0.6925, time 34.81ms, mfu 14.19%\n",
            "iter 4590: loss 0.6705, time 34.64ms, mfu 14.26%\n",
            "iter 4600: loss 0.6747, time 34.62ms, mfu 14.32%\n",
            "iter 4610: loss 0.6807, time 34.63ms, mfu 14.38%\n",
            "iter 4620: loss 0.6877, time 34.64ms, mfu 14.43%\n",
            "iter 4630: loss 0.7073, time 34.60ms, mfu 14.47%\n",
            "iter 4640: loss 0.6892, time 34.64ms, mfu 14.51%\n",
            "iter 4650: loss 0.6704, time 34.52ms, mfu 14.55%\n",
            "iter 4660: loss 0.6618, time 34.57ms, mfu 14.59%\n",
            "iter 4670: loss 0.6915, time 34.64ms, mfu 14.62%\n",
            "iter 4680: loss 0.7005, time 34.41ms, mfu 14.65%\n",
            "iter 4690: loss 0.6863, time 34.66ms, mfu 14.68%\n",
            "iter 4700: loss 0.6754, time 34.52ms, mfu 14.70%\n",
            "iter 4710: loss 0.6793, time 34.54ms, mfu 14.72%\n",
            "iter 4720: loss 0.6772, time 34.61ms, mfu 14.74%\n",
            "iter 4730: loss 0.6677, time 34.81ms, mfu 14.75%\n",
            "iter 4740: loss 0.6845, time 34.71ms, mfu 14.76%\n",
            "step 4750: train loss 0.4588, val loss 1.8870\n",
            "iter 4750: loss 0.6725, time 5555.69ms, mfu 13.29%\n",
            "iter 4760: loss 0.6795, time 34.60ms, mfu 13.45%\n",
            "iter 4770: loss 0.6568, time 34.61ms, mfu 13.59%\n",
            "iter 4780: loss 0.6679, time 34.63ms, mfu 13.72%\n",
            "iter 4790: loss 0.6758, time 34.61ms, mfu 13.84%\n",
            "iter 4800: loss 0.6546, time 34.67ms, mfu 13.94%\n",
            "iter 4810: loss 0.6930, time 34.69ms, mfu 14.03%\n",
            "iter 4820: loss 0.6835, time 34.64ms, mfu 14.12%\n",
            "iter 4830: loss 0.6826, time 34.58ms, mfu 14.20%\n",
            "iter 4840: loss 0.6928, time 34.22ms, mfu 14.28%\n",
            "iter 4850: loss 0.6782, time 34.58ms, mfu 14.35%\n",
            "iter 4860: loss 0.6730, time 34.57ms, mfu 14.40%\n",
            "iter 4870: loss 0.7031, time 34.69ms, mfu 14.45%\n",
            "iter 4880: loss 0.6752, time 34.06ms, mfu 14.52%\n",
            "iter 4890: loss 0.6772, time 34.59ms, mfu 14.55%\n",
            "iter 4900: loss 0.6583, time 34.75ms, mfu 14.58%\n",
            "iter 4910: loss 0.6827, time 34.83ms, mfu 14.60%\n",
            "iter 4920: loss 0.7005, time 34.74ms, mfu 14.63%\n",
            "iter 4930: loss 0.6619, time 34.64ms, mfu 14.65%\n",
            "iter 4940: loss 0.6461, time 34.59ms, mfu 14.68%\n",
            "iter 4950: loss 0.6574, time 34.52ms, mfu 14.70%\n",
            "iter 4960: loss 0.6679, time 34.74ms, mfu 14.71%\n",
            "iter 4970: loss 0.6821, time 34.56ms, mfu 14.73%\n",
            "iter 4980: loss 0.6595, time 34.71ms, mfu 14.75%\n",
            "iter 4990: loss 0.6463, time 34.51ms, mfu 14.76%\n",
            "step 5000: train loss 0.4405, val loss 1.8891\n",
            "iter 5000: loss 0.6506, time 5553.77ms, mfu 13.30%\n",
            "==== 训练 n_head=7 ====\n",
            "Overriding config with config/train_shakespeare_char_7head.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 7\n",
            "n_head = 7\n",
            "n_embd = 420\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "device = \"cuda\"\n",
            "compile = False\n",
            "\n",
            "Overriding: device = cuda\n",
            "Overriding: compile = False\n",
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 14.85M\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "num decayed parameter tensors: 30, with 14,952,420 parameters\n",
            "num non-decayed parameter tensors: 15, with 6,300 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.3025, val loss 4.2986\n",
            "iter 0: loss 4.2762, time 5952.75ms, mfu -100.00%\n",
            "iter 10: loss 3.1046, time 34.30ms, mfu 15.02%\n",
            "iter 20: loss 2.6993, time 34.22ms, mfu 15.03%\n",
            "iter 30: loss 2.5738, time 34.00ms, mfu 15.04%\n",
            "iter 40: loss 2.5625, time 34.31ms, mfu 15.04%\n",
            "iter 50: loss 2.5183, time 34.11ms, mfu 15.05%\n",
            "iter 60: loss 2.4895, time 34.23ms, mfu 15.05%\n",
            "iter 70: loss 2.4822, time 34.25ms, mfu 15.05%\n",
            "iter 80: loss 2.4922, time 34.25ms, mfu 15.05%\n",
            "iter 90: loss 2.4586, time 34.20ms, mfu 15.05%\n",
            "iter 100: loss 2.4491, time 34.25ms, mfu 15.05%\n",
            "iter 110: loss 2.4399, time 34.27ms, mfu 15.05%\n",
            "iter 120: loss 2.4218, time 34.59ms, mfu 15.03%\n",
            "iter 130: loss 2.3960, time 34.17ms, mfu 15.04%\n",
            "iter 140: loss 2.3991, time 34.24ms, mfu 15.04%\n",
            "iter 150: loss 2.3789, time 34.24ms, mfu 15.04%\n",
            "iter 160: loss 2.3735, time 34.35ms, mfu 15.04%\n",
            "iter 170: loss 2.2905, time 34.32ms, mfu 15.03%\n",
            "iter 180: loss 2.2835, time 34.24ms, mfu 15.04%\n",
            "iter 190: loss 2.1932, time 34.39ms, mfu 15.03%\n",
            "iter 200: loss 2.1842, time 34.25ms, mfu 15.03%\n",
            "iter 210: loss 2.1054, time 34.26ms, mfu 15.03%\n",
            "iter 220: loss 2.0818, time 33.76ms, mfu 15.06%\n",
            "iter 230: loss 2.0489, time 34.25ms, mfu 15.06%\n",
            "iter 240: loss 2.0085, time 34.44ms, mfu 15.05%\n",
            "step 250: train loss 1.9386, val loss 2.0569\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 250: loss 1.9854, time 5838.75ms, mfu 13.55%\n",
            "iter 260: loss 1.9515, time 34.20ms, mfu 13.70%\n",
            "iter 270: loss 1.9411, time 34.20ms, mfu 13.84%\n",
            "iter 280: loss 1.8845, time 34.27ms, mfu 13.96%\n",
            "iter 290: loss 1.8884, time 34.23ms, mfu 14.07%\n",
            "iter 300: loss 1.8371, time 34.24ms, mfu 14.17%\n",
            "iter 310: loss 1.8553, time 34.26ms, mfu 14.25%\n",
            "iter 320: loss 1.8215, time 34.08ms, mfu 14.34%\n",
            "iter 330: loss 1.8060, time 34.25ms, mfu 14.41%\n",
            "iter 340: loss 1.7754, time 34.35ms, mfu 14.47%\n",
            "iter 350: loss 1.7713, time 34.30ms, mfu 14.53%\n",
            "iter 360: loss 1.7551, time 34.27ms, mfu 14.58%\n",
            "iter 370: loss 1.7084, time 34.16ms, mfu 14.63%\n",
            "iter 380: loss 1.7031, time 34.15ms, mfu 14.68%\n",
            "iter 390: loss 1.7286, time 34.21ms, mfu 14.71%\n",
            "iter 400: loss 1.6912, time 33.95ms, mfu 14.76%\n",
            "iter 410: loss 1.6740, time 34.29ms, mfu 14.79%\n",
            "iter 420: loss 1.6535, time 34.29ms, mfu 14.81%\n",
            "iter 430: loss 1.6801, time 34.26ms, mfu 14.83%\n",
            "iter 440: loss 1.6041, time 34.32ms, mfu 14.85%\n",
            "iter 450: loss 1.6228, time 34.24ms, mfu 14.87%\n",
            "iter 460: loss 1.6431, time 34.24ms, mfu 14.89%\n",
            "iter 470: loss 1.6026, time 34.54ms, mfu 14.89%\n",
            "iter 480: loss 1.5776, time 34.27ms, mfu 14.91%\n",
            "iter 490: loss 1.6111, time 34.28ms, mfu 14.92%\n",
            "step 500: train loss 1.5067, val loss 1.7096\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 500: loss 1.5587, time 5835.57ms, mfu 13.44%\n",
            "iter 510: loss 1.5351, time 34.27ms, mfu 13.60%\n",
            "iter 520: loss 1.5534, time 34.21ms, mfu 13.74%\n",
            "iter 530: loss 1.5308, time 34.27ms, mfu 13.87%\n",
            "iter 540: loss 1.5413, time 34.25ms, mfu 13.99%\n",
            "iter 550: loss 1.5395, time 34.54ms, mfu 14.08%\n",
            "iter 560: loss 1.5330, time 34.18ms, mfu 14.18%\n",
            "iter 570: loss 1.5078, time 34.29ms, mfu 14.27%\n",
            "iter 580: loss 1.4628, time 34.20ms, mfu 14.35%\n",
            "iter 590: loss 1.4744, time 34.26ms, mfu 14.42%\n",
            "iter 600: loss 1.4958, time 34.42ms, mfu 14.47%\n",
            "iter 610: loss 1.4484, time 34.25ms, mfu 14.53%\n",
            "iter 620: loss 1.4833, time 34.26ms, mfu 14.58%\n",
            "iter 630: loss 1.4531, time 34.23ms, mfu 14.63%\n",
            "iter 640: loss 1.4756, time 34.28ms, mfu 14.67%\n",
            "iter 650: loss 1.4378, time 34.31ms, mfu 14.71%\n",
            "iter 660: loss 1.4528, time 34.28ms, mfu 14.74%\n",
            "iter 670: loss 1.4567, time 34.30ms, mfu 14.77%\n",
            "iter 680: loss 1.4493, time 34.17ms, mfu 14.80%\n",
            "iter 690: loss 1.4530, time 34.55ms, mfu 14.81%\n",
            "iter 700: loss 1.4311, time 34.23ms, mfu 14.83%\n",
            "iter 710: loss 1.4364, time 34.29ms, mfu 14.85%\n",
            "iter 720: loss 1.4157, time 34.31ms, mfu 14.87%\n",
            "iter 730: loss 1.4155, time 34.31ms, mfu 14.89%\n",
            "iter 740: loss 1.3826, time 34.29ms, mfu 14.90%\n",
            "step 750: train loss 1.3310, val loss 1.5575\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 750: loss 1.4058, time 5846.34ms, mfu 13.42%\n",
            "iter 760: loss 1.3544, time 34.26ms, mfu 13.58%\n",
            "iter 770: loss 1.3901, time 34.27ms, mfu 13.73%\n",
            "iter 780: loss 1.3773, time 34.26ms, mfu 13.86%\n",
            "iter 790: loss 1.3707, time 34.24ms, mfu 13.98%\n",
            "iter 800: loss 1.3715, time 34.19ms, mfu 14.09%\n",
            "iter 810: loss 1.3653, time 34.24ms, mfu 14.18%\n",
            "iter 820: loss 1.3577, time 34.23ms, mfu 14.27%\n",
            "iter 830: loss 1.4019, time 34.27ms, mfu 14.35%\n",
            "iter 840: loss 1.3595, time 34.27ms, mfu 14.42%\n",
            "iter 850: loss 1.3606, time 34.23ms, mfu 14.48%\n",
            "iter 860: loss 1.3378, time 34.37ms, mfu 14.53%\n",
            "iter 870: loss 1.3407, time 34.26ms, mfu 14.58%\n",
            "iter 880: loss 1.3420, time 34.26ms, mfu 14.63%\n",
            "iter 890: loss 1.3435, time 34.28ms, mfu 14.67%\n",
            "iter 900: loss 1.3110, time 34.32ms, mfu 14.70%\n",
            "iter 910: loss 1.3298, time 34.26ms, mfu 14.74%\n",
            "iter 920: loss 1.3092, time 34.27ms, mfu 14.77%\n",
            "iter 930: loss 1.3208, time 34.25ms, mfu 14.80%\n",
            "iter 940: loss 1.2944, time 34.25ms, mfu 14.82%\n",
            "iter 950: loss 1.3226, time 34.17ms, mfu 14.85%\n",
            "iter 960: loss 1.2491, time 34.23ms, mfu 14.87%\n",
            "iter 970: loss 1.3058, time 33.99ms, mfu 14.90%\n",
            "iter 980: loss 1.3238, time 34.29ms, mfu 14.91%\n",
            "iter 990: loss 1.2875, time 34.20ms, mfu 14.93%\n",
            "step 1000: train loss 1.2354, val loss 1.5081\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1000: loss 1.3018, time 5830.24ms, mfu 13.44%\n",
            "iter 1010: loss 1.2813, time 34.28ms, mfu 13.60%\n",
            "iter 1020: loss 1.3144, time 34.29ms, mfu 13.74%\n",
            "iter 1030: loss 1.3138, time 34.32ms, mfu 13.87%\n",
            "iter 1040: loss 1.2973, time 34.28ms, mfu 13.99%\n",
            "iter 1050: loss 1.2810, time 34.28ms, mfu 14.09%\n",
            "iter 1060: loss 1.2849, time 34.25ms, mfu 14.19%\n",
            "iter 1070: loss 1.2678, time 34.29ms, mfu 14.27%\n",
            "iter 1080: loss 1.2782, time 34.29ms, mfu 14.35%\n",
            "iter 1090: loss 1.2651, time 34.26ms, mfu 14.42%\n",
            "iter 1100: loss 1.2761, time 34.31ms, mfu 14.48%\n",
            "iter 1110: loss 1.3006, time 34.25ms, mfu 14.53%\n",
            "iter 1120: loss 1.2262, time 34.27ms, mfu 14.59%\n",
            "iter 1130: loss 1.2626, time 34.30ms, mfu 14.63%\n",
            "iter 1140: loss 1.2680, time 34.23ms, mfu 14.67%\n",
            "iter 1150: loss 1.2739, time 34.29ms, mfu 14.71%\n",
            "iter 1160: loss 1.2589, time 34.26ms, mfu 14.74%\n",
            "iter 1170: loss 1.2586, time 34.26ms, mfu 14.77%\n",
            "iter 1180: loss 1.2519, time 34.24ms, mfu 14.80%\n",
            "iter 1190: loss 1.2240, time 34.46ms, mfu 14.81%\n",
            "iter 1200: loss 1.2729, time 34.40ms, mfu 14.83%\n",
            "iter 1210: loss 1.2630, time 34.30ms, mfu 14.85%\n",
            "iter 1220: loss 1.2617, time 34.31ms, mfu 14.87%\n",
            "iter 1230: loss 1.2610, time 34.35ms, mfu 14.88%\n",
            "iter 1240: loss 1.2511, time 34.27ms, mfu 14.90%\n",
            "step 1250: train loss 1.1625, val loss 1.4829\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1250: loss 1.2283, time 5833.11ms, mfu 13.42%\n",
            "iter 1260: loss 1.2468, time 33.95ms, mfu 13.59%\n",
            "iter 1270: loss 1.2437, time 34.28ms, mfu 13.74%\n",
            "iter 1280: loss 1.2558, time 34.29ms, mfu 13.87%\n",
            "iter 1290: loss 1.2315, time 34.26ms, mfu 13.98%\n",
            "iter 1300: loss 1.2467, time 34.28ms, mfu 14.09%\n",
            "iter 1310: loss 1.1821, time 34.29ms, mfu 14.18%\n",
            "iter 1320: loss 1.2338, time 34.28ms, mfu 14.27%\n",
            "iter 1330: loss 1.2058, time 34.27ms, mfu 14.34%\n",
            "iter 1340: loss 1.2366, time 34.29ms, mfu 14.41%\n",
            "iter 1350: loss 1.2269, time 34.26ms, mfu 14.48%\n",
            "iter 1360: loss 1.1873, time 34.17ms, mfu 14.54%\n",
            "iter 1370: loss 1.2021, time 34.29ms, mfu 14.59%\n",
            "iter 1380: loss 1.1830, time 34.27ms, mfu 14.63%\n",
            "iter 1390: loss 1.1922, time 34.24ms, mfu 14.67%\n",
            "iter 1400: loss 1.1794, time 34.26ms, mfu 14.71%\n",
            "iter 1410: loss 1.2179, time 34.27ms, mfu 14.74%\n",
            "iter 1420: loss 1.1842, time 34.25ms, mfu 14.77%\n",
            "iter 1430: loss 1.2132, time 34.35ms, mfu 14.80%\n",
            "iter 1440: loss 1.1627, time 34.31ms, mfu 14.82%\n",
            "iter 1450: loss 1.1961, time 34.03ms, mfu 14.85%\n",
            "iter 1460: loss 1.2045, time 34.34ms, mfu 14.87%\n",
            "iter 1470: loss 1.1744, time 34.29ms, mfu 14.88%\n",
            "iter 1480: loss 1.1836, time 34.28ms, mfu 14.90%\n",
            "iter 1490: loss 1.1590, time 34.39ms, mfu 14.91%\n",
            "step 1500: train loss 1.0978, val loss 1.4716\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1500: loss 1.1740, time 5838.02ms, mfu 13.43%\n",
            "iter 1510: loss 1.1699, time 34.14ms, mfu 13.59%\n",
            "iter 1520: loss 1.1685, time 34.28ms, mfu 13.74%\n",
            "iter 1530: loss 1.1513, time 34.25ms, mfu 13.87%\n",
            "iter 1540: loss 1.1653, time 34.25ms, mfu 13.99%\n",
            "iter 1550: loss 1.1595, time 34.27ms, mfu 14.09%\n",
            "iter 1560: loss 1.1543, time 34.22ms, mfu 14.19%\n",
            "iter 1570: loss 1.1845, time 34.26ms, mfu 14.27%\n",
            "iter 1580: loss 1.1485, time 34.28ms, mfu 14.35%\n",
            "iter 1590: loss 1.1246, time 34.12ms, mfu 14.42%\n",
            "iter 1600: loss 1.1485, time 34.28ms, mfu 14.49%\n",
            "iter 1610: loss 1.1555, time 34.24ms, mfu 14.54%\n",
            "iter 1620: loss 1.1189, time 34.27ms, mfu 14.59%\n",
            "iter 1630: loss 1.1485, time 34.23ms, mfu 14.64%\n",
            "iter 1640: loss 1.1430, time 34.57ms, mfu 14.67%\n",
            "iter 1650: loss 1.1589, time 34.26ms, mfu 14.70%\n",
            "iter 1660: loss 1.1376, time 34.23ms, mfu 14.74%\n",
            "iter 1670: loss 1.1620, time 34.31ms, mfu 14.77%\n",
            "iter 1680: loss 1.1253, time 34.24ms, mfu 14.80%\n",
            "iter 1690: loss 1.1334, time 34.33ms, mfu 14.82%\n",
            "iter 1700: loss 1.1489, time 34.32ms, mfu 14.84%\n",
            "iter 1710: loss 1.1391, time 34.28ms, mfu 14.86%\n",
            "iter 1720: loss 1.1320, time 34.27ms, mfu 14.87%\n",
            "iter 1730: loss 1.1214, time 34.25ms, mfu 14.89%\n",
            "iter 1740: loss 1.1565, time 34.32ms, mfu 14.90%\n",
            "step 1750: train loss 1.0367, val loss 1.4815\n",
            "iter 1750: loss 1.1163, time 5490.89ms, mfu 13.42%\n",
            "iter 1760: loss 1.1301, time 34.20ms, mfu 13.59%\n",
            "iter 1770: loss 1.1249, time 34.18ms, mfu 13.74%\n",
            "iter 1780: loss 1.0921, time 34.36ms, mfu 13.86%\n",
            "iter 1790: loss 1.0936, time 34.21ms, mfu 13.98%\n",
            "iter 1800: loss 1.0993, time 34.26ms, mfu 14.09%\n",
            "iter 1810: loss 1.1611, time 34.28ms, mfu 14.18%\n",
            "iter 1820: loss 1.0972, time 34.27ms, mfu 14.27%\n",
            "iter 1830: loss 1.1215, time 34.22ms, mfu 14.35%\n",
            "iter 1840: loss 1.0830, time 34.25ms, mfu 14.42%\n",
            "iter 1850: loss 1.1410, time 34.26ms, mfu 14.48%\n",
            "iter 1860: loss 1.0803, time 34.73ms, mfu 14.52%\n",
            "iter 1870: loss 1.1109, time 34.25ms, mfu 14.57%\n",
            "iter 1880: loss 1.0874, time 34.29ms, mfu 14.62%\n",
            "iter 1890: loss 1.1148, time 34.25ms, mfu 14.66%\n",
            "iter 1900: loss 1.0927, time 34.22ms, mfu 14.70%\n",
            "iter 1910: loss 1.0808, time 34.72ms, mfu 14.71%\n",
            "iter 1920: loss 1.0475, time 34.23ms, mfu 14.75%\n",
            "iter 1930: loss 1.0932, time 34.22ms, mfu 14.78%\n",
            "iter 1940: loss 1.0966, time 34.28ms, mfu 14.80%\n",
            "iter 1950: loss 1.1018, time 34.29ms, mfu 14.83%\n",
            "iter 1960: loss 1.1053, time 34.30ms, mfu 14.85%\n",
            "iter 1970: loss 1.0742, time 34.25ms, mfu 14.87%\n",
            "iter 1980: loss 1.0835, time 34.36ms, mfu 14.88%\n",
            "iter 1990: loss 1.0604, time 34.38ms, mfu 14.89%\n",
            "step 2000: train loss 0.9716, val loss 1.5070\n",
            "iter 2000: loss 1.0645, time 5498.64ms, mfu 13.41%\n",
            "iter 2010: loss 1.0837, time 34.31ms, mfu 13.57%\n",
            "iter 2020: loss 1.0471, time 34.72ms, mfu 13.70%\n",
            "iter 2030: loss 1.0635, time 34.33ms, mfu 13.83%\n",
            "iter 2040: loss 1.0610, time 34.27ms, mfu 13.95%\n",
            "iter 2050: loss 1.0411, time 34.58ms, mfu 14.05%\n",
            "iter 2060: loss 1.0611, time 34.30ms, mfu 14.14%\n",
            "iter 2070: loss 1.0319, time 34.25ms, mfu 14.23%\n",
            "iter 2080: loss 1.0622, time 34.12ms, mfu 14.32%\n",
            "iter 2090: loss 1.0530, time 34.33ms, mfu 14.39%\n",
            "iter 2100: loss 1.0546, time 34.37ms, mfu 14.45%\n",
            "iter 2110: loss 1.0664, time 34.28ms, mfu 14.51%\n",
            "iter 2120: loss 1.0479, time 34.49ms, mfu 14.55%\n",
            "iter 2130: loss 1.0507, time 34.37ms, mfu 14.60%\n",
            "iter 2140: loss 1.0297, time 34.32ms, mfu 14.64%\n",
            "iter 2150: loss 1.0365, time 34.25ms, mfu 14.68%\n",
            "iter 2160: loss 1.0549, time 34.34ms, mfu 14.71%\n",
            "iter 2170: loss 1.0135, time 34.32ms, mfu 14.74%\n",
            "iter 2180: loss 1.0306, time 34.25ms, mfu 14.77%\n",
            "iter 2190: loss 1.0043, time 34.33ms, mfu 14.80%\n",
            "iter 2200: loss 1.0272, time 34.24ms, mfu 14.82%\n",
            "iter 2210: loss 1.0263, time 34.31ms, mfu 14.84%\n",
            "iter 2220: loss 1.0279, time 34.39ms, mfu 14.86%\n",
            "iter 2230: loss 1.0166, time 34.33ms, mfu 14.87%\n",
            "iter 2240: loss 1.0480, time 34.34ms, mfu 14.89%\n",
            "step 2250: train loss 0.9049, val loss 1.5172\n",
            "iter 2250: loss 1.0445, time 5509.53ms, mfu 13.41%\n",
            "iter 2260: loss 1.0010, time 34.32ms, mfu 13.57%\n",
            "iter 2270: loss 1.0206, time 34.20ms, mfu 13.72%\n",
            "iter 2280: loss 1.0152, time 34.41ms, mfu 13.84%\n",
            "iter 2290: loss 0.9925, time 34.30ms, mfu 13.96%\n",
            "iter 2300: loss 0.9980, time 34.31ms, mfu 14.07%\n",
            "iter 2310: loss 1.0008, time 34.33ms, mfu 14.16%\n",
            "iter 2320: loss 1.0107, time 34.21ms, mfu 14.25%\n",
            "iter 2330: loss 0.9798, time 34.22ms, mfu 14.33%\n",
            "iter 2340: loss 1.0094, time 34.23ms, mfu 14.41%\n",
            "iter 2350: loss 1.0185, time 34.24ms, mfu 14.47%\n",
            "iter 2360: loss 0.9687, time 33.53ms, mfu 14.56%\n",
            "iter 2370: loss 0.9874, time 34.23ms, mfu 14.61%\n",
            "iter 2380: loss 0.9691, time 34.29ms, mfu 14.65%\n",
            "iter 2390: loss 0.9698, time 34.19ms, mfu 14.69%\n",
            "iter 2400: loss 0.9756, time 34.26ms, mfu 14.73%\n",
            "iter 2410: loss 0.9763, time 34.27ms, mfu 14.76%\n",
            "iter 2420: loss 0.9555, time 34.20ms, mfu 14.79%\n",
            "iter 2430: loss 0.9969, time 34.14ms, mfu 14.82%\n",
            "iter 2440: loss 0.9784, time 34.25ms, mfu 14.84%\n",
            "iter 2450: loss 0.9676, time 34.34ms, mfu 14.86%\n",
            "iter 2460: loss 0.9727, time 34.55ms, mfu 14.87%\n",
            "iter 2470: loss 0.9533, time 34.80ms, mfu 14.86%\n",
            "iter 2480: loss 0.9809, time 34.23ms, mfu 14.88%\n",
            "iter 2490: loss 0.9488, time 34.37ms, mfu 14.89%\n",
            "step 2500: train loss 0.8410, val loss 1.5649\n",
            "iter 2500: loss 0.9760, time 5498.26ms, mfu 13.41%\n",
            "iter 2510: loss 0.9469, time 34.26ms, mfu 13.57%\n",
            "iter 2520: loss 0.9633, time 34.35ms, mfu 13.72%\n",
            "iter 2530: loss 0.9698, time 34.34ms, mfu 13.85%\n",
            "iter 2540: loss 0.9582, time 34.84ms, mfu 13.94%\n",
            "iter 2550: loss 0.9547, time 34.25ms, mfu 14.05%\n",
            "iter 2560: loss 0.9261, time 34.23ms, mfu 14.15%\n",
            "iter 2570: loss 0.9335, time 34.24ms, mfu 14.24%\n",
            "iter 2580: loss 0.9605, time 34.21ms, mfu 14.32%\n",
            "iter 2590: loss 0.9706, time 34.30ms, mfu 14.39%\n",
            "iter 2600: loss 0.9311, time 34.17ms, mfu 14.46%\n",
            "iter 2610: loss 0.9322, time 34.28ms, mfu 14.52%\n",
            "iter 2620: loss 0.9647, time 34.24ms, mfu 14.57%\n",
            "iter 2630: loss 0.9131, time 34.32ms, mfu 14.62%\n",
            "iter 2640: loss 0.9328, time 34.29ms, mfu 14.66%\n",
            "iter 2650: loss 0.9149, time 34.26ms, mfu 14.70%\n",
            "iter 2660: loss 0.8878, time 34.24ms, mfu 14.73%\n",
            "iter 2670: loss 0.9214, time 34.22ms, mfu 14.77%\n",
            "iter 2680: loss 0.9230, time 34.23ms, mfu 14.79%\n",
            "iter 2690: loss 0.9239, time 34.14ms, mfu 14.82%\n",
            "iter 2700: loss 0.9384, time 34.27ms, mfu 14.85%\n",
            "iter 2710: loss 0.9160, time 34.43ms, mfu 14.86%\n",
            "iter 2720: loss 0.9173, time 34.23ms, mfu 14.88%\n",
            "iter 2730: loss 0.9169, time 34.52ms, mfu 14.88%\n",
            "iter 2740: loss 0.9114, time 34.17ms, mfu 14.90%\n",
            "step 2750: train loss 0.7712, val loss 1.5909\n",
            "iter 2750: loss 0.9037, time 5513.82ms, mfu 13.42%\n",
            "iter 2760: loss 0.8797, time 34.25ms, mfu 13.58%\n",
            "iter 2770: loss 0.9053, time 34.23ms, mfu 13.73%\n",
            "iter 2780: loss 0.8836, time 34.23ms, mfu 13.86%\n",
            "iter 2790: loss 0.9031, time 34.39ms, mfu 13.98%\n",
            "iter 2800: loss 0.8740, time 34.25ms, mfu 14.08%\n",
            "iter 2810: loss 0.9046, time 34.22ms, mfu 14.18%\n",
            "iter 2820: loss 0.8925, time 34.33ms, mfu 14.26%\n",
            "iter 2830: loss 0.8957, time 34.25ms, mfu 14.34%\n",
            "iter 2840: loss 0.9113, time 34.25ms, mfu 14.41%\n",
            "iter 2850: loss 0.8925, time 34.20ms, mfu 14.48%\n",
            "iter 2860: loss 0.8731, time 34.34ms, mfu 14.53%\n",
            "iter 2870: loss 0.8651, time 34.30ms, mfu 14.58%\n",
            "iter 2880: loss 0.8940, time 34.26ms, mfu 14.63%\n",
            "iter 2890: loss 0.8805, time 34.72ms, mfu 14.65%\n",
            "iter 2900: loss 0.9112, time 34.22ms, mfu 14.69%\n",
            "iter 2910: loss 0.8871, time 34.10ms, mfu 14.73%\n",
            "iter 2920: loss 0.8931, time 34.30ms, mfu 14.76%\n",
            "iter 2930: loss 0.8707, time 34.28ms, mfu 14.79%\n",
            "iter 2940: loss 0.8819, time 34.29ms, mfu 14.81%\n",
            "iter 2950: loss 0.8673, time 33.94ms, mfu 14.85%\n",
            "iter 2960: loss 0.8668, time 34.30ms, mfu 14.87%\n",
            "iter 2970: loss 0.8751, time 34.24ms, mfu 14.89%\n",
            "iter 2980: loss 0.8548, time 34.30ms, mfu 14.90%\n",
            "iter 2990: loss 0.8758, time 34.12ms, mfu 14.92%\n",
            "step 3000: train loss 0.7053, val loss 1.6373\n",
            "iter 3000: loss 0.8926, time 5510.24ms, mfu 13.44%\n",
            "iter 3010: loss 0.8534, time 34.26ms, mfu 13.60%\n",
            "iter 3020: loss 0.8542, time 34.26ms, mfu 13.74%\n",
            "iter 3030: loss 0.8619, time 34.35ms, mfu 13.87%\n",
            "iter 3040: loss 0.8653, time 34.12ms, mfu 13.99%\n",
            "iter 3050: loss 0.8468, time 34.40ms, mfu 14.09%\n",
            "iter 3060: loss 0.8617, time 34.24ms, mfu 14.19%\n",
            "iter 3070: loss 0.8447, time 34.28ms, mfu 14.27%\n",
            "iter 3080: loss 0.8824, time 34.19ms, mfu 14.35%\n",
            "iter 3090: loss 0.8459, time 34.18ms, mfu 14.42%\n",
            "iter 3100: loss 0.8813, time 34.34ms, mfu 14.48%\n",
            "iter 3110: loss 0.8119, time 34.25ms, mfu 14.54%\n",
            "iter 3120: loss 0.8416, time 34.47ms, mfu 14.58%\n",
            "iter 3130: loss 0.8074, time 34.26ms, mfu 14.63%\n",
            "iter 3140: loss 0.8421, time 34.27ms, mfu 14.67%\n",
            "iter 3150: loss 0.8121, time 34.68ms, mfu 14.69%\n",
            "iter 3160: loss 0.8503, time 34.27ms, mfu 14.72%\n",
            "iter 3170: loss 0.7801, time 34.43ms, mfu 14.75%\n",
            "iter 3180: loss 0.8358, time 34.19ms, mfu 14.78%\n",
            "iter 3190: loss 0.8300, time 34.26ms, mfu 14.81%\n",
            "iter 3200: loss 0.8237, time 34.25ms, mfu 14.83%\n",
            "iter 3210: loss 0.8320, time 34.21ms, mfu 14.85%\n",
            "iter 3220: loss 0.8475, time 34.35ms, mfu 14.87%\n",
            "iter 3230: loss 0.8171, time 34.30ms, mfu 14.88%\n",
            "iter 3240: loss 0.8204, time 34.24ms, mfu 14.90%\n",
            "step 3250: train loss 0.6401, val loss 1.6955\n",
            "iter 3250: loss 0.8254, time 5510.03ms, mfu 13.42%\n",
            "iter 3260: loss 0.8286, time 34.16ms, mfu 13.59%\n",
            "iter 3270: loss 0.8016, time 34.31ms, mfu 13.73%\n",
            "iter 3280: loss 0.8124, time 34.25ms, mfu 13.86%\n",
            "iter 3290: loss 0.8148, time 34.44ms, mfu 13.97%\n",
            "iter 3300: loss 0.8035, time 34.26ms, mfu 14.08%\n",
            "iter 3310: loss 0.8013, time 34.88ms, mfu 14.15%\n",
            "iter 3320: loss 0.8005, time 34.28ms, mfu 14.24%\n",
            "iter 3330: loss 0.7970, time 34.24ms, mfu 14.32%\n",
            "iter 3340: loss 0.8101, time 33.90ms, mfu 14.41%\n",
            "iter 3350: loss 0.7826, time 34.14ms, mfu 14.48%\n",
            "iter 3360: loss 0.8061, time 34.40ms, mfu 14.53%\n",
            "iter 3370: loss 0.8084, time 34.23ms, mfu 14.58%\n",
            "iter 3380: loss 0.7830, time 34.17ms, mfu 14.63%\n",
            "iter 3390: loss 0.8016, time 34.38ms, mfu 14.67%\n",
            "iter 3400: loss 0.7825, time 34.31ms, mfu 14.70%\n",
            "iter 3410: loss 0.7821, time 34.08ms, mfu 14.74%\n",
            "iter 3420: loss 0.7624, time 34.21ms, mfu 14.78%\n",
            "iter 3430: loss 0.7997, time 34.31ms, mfu 14.80%\n",
            "iter 3440: loss 0.7708, time 34.17ms, mfu 14.83%\n",
            "iter 3450: loss 0.7706, time 34.40ms, mfu 14.84%\n",
            "iter 3460: loss 0.7397, time 34.30ms, mfu 14.86%\n",
            "iter 3470: loss 0.8028, time 34.23ms, mfu 14.88%\n",
            "iter 3480: loss 0.7540, time 34.27ms, mfu 14.90%\n",
            "iter 3490: loss 0.7714, time 34.18ms, mfu 14.91%\n",
            "step 3500: train loss 0.5836, val loss 1.7491\n",
            "iter 3500: loss 0.7600, time 5494.60ms, mfu 13.43%\n",
            "iter 3510: loss 0.7705, time 34.19ms, mfu 13.60%\n",
            "iter 3520: loss 0.7641, time 34.28ms, mfu 13.74%\n",
            "iter 3530: loss 0.7609, time 34.98ms, mfu 13.84%\n",
            "iter 3540: loss 0.7723, time 34.28ms, mfu 13.96%\n",
            "iter 3550: loss 0.7627, time 34.33ms, mfu 14.06%\n",
            "iter 3560: loss 0.7319, time 34.26ms, mfu 14.16%\n",
            "iter 3570: loss 0.7831, time 34.28ms, mfu 14.25%\n",
            "iter 3580: loss 0.7503, time 34.19ms, mfu 14.33%\n",
            "iter 3590: loss 0.7552, time 34.24ms, mfu 14.40%\n",
            "iter 3600: loss 0.7754, time 34.15ms, mfu 14.47%\n",
            "iter 3610: loss 0.7952, time 34.12ms, mfu 14.54%\n",
            "iter 3620: loss 0.7349, time 34.64ms, mfu 14.57%\n",
            "iter 3630: loss 0.7303, time 34.19ms, mfu 14.62%\n",
            "iter 3640: loss 0.7515, time 34.50ms, mfu 14.65%\n",
            "iter 3650: loss 0.7613, time 34.22ms, mfu 14.69%\n",
            "iter 3660: loss 0.7368, time 34.27ms, mfu 14.73%\n",
            "iter 3670: loss 0.7343, time 34.66ms, mfu 14.74%\n",
            "iter 3680: loss 0.7386, time 34.19ms, mfu 14.77%\n",
            "iter 3690: loss 0.7252, time 34.45ms, mfu 14.79%\n",
            "iter 3700: loss 0.7228, time 34.20ms, mfu 14.82%\n",
            "iter 3710: loss 0.7318, time 34.33ms, mfu 14.84%\n",
            "iter 3720: loss 0.7356, time 34.15ms, mfu 14.87%\n",
            "iter 3730: loss 0.7360, time 34.55ms, mfu 14.87%\n",
            "iter 3740: loss 0.7613, time 34.23ms, mfu 14.89%\n",
            "step 3750: train loss 0.5349, val loss 1.7959\n",
            "iter 3750: loss 0.7294, time 5504.64ms, mfu 13.41%\n",
            "iter 3760: loss 0.7262, time 34.25ms, mfu 13.57%\n",
            "iter 3770: loss 0.7388, time 34.27ms, mfu 13.72%\n",
            "iter 3780: loss 0.7121, time 34.51ms, mfu 13.84%\n",
            "iter 3790: loss 0.6867, time 34.30ms, mfu 13.96%\n",
            "iter 3800: loss 0.7339, time 34.37ms, mfu 14.06%\n",
            "iter 3810: loss 0.7108, time 34.24ms, mfu 14.16%\n",
            "iter 3820: loss 0.7472, time 34.32ms, mfu 14.25%\n",
            "iter 3830: loss 0.7357, time 34.30ms, mfu 14.33%\n",
            "iter 3840: loss 0.7241, time 34.25ms, mfu 14.40%\n",
            "iter 3850: loss 0.7126, time 34.26ms, mfu 14.46%\n",
            "iter 3860: loss 0.7077, time 34.30ms, mfu 14.52%\n",
            "iter 3870: loss 0.7260, time 34.27ms, mfu 14.57%\n",
            "iter 3880: loss 0.7302, time 34.28ms, mfu 14.62%\n",
            "iter 3890: loss 0.7261, time 34.26ms, mfu 14.66%\n",
            "iter 3900: loss 0.7178, time 34.45ms, mfu 14.69%\n",
            "iter 3910: loss 0.7024, time 34.20ms, mfu 14.73%\n",
            "iter 3920: loss 0.7174, time 34.21ms, mfu 14.76%\n",
            "iter 3930: loss 0.7199, time 34.11ms, mfu 14.80%\n",
            "iter 3940: loss 0.7069, time 34.24ms, mfu 14.82%\n",
            "iter 3950: loss 0.7215, time 34.27ms, mfu 14.84%\n",
            "iter 3960: loss 0.7066, time 34.26ms, mfu 14.86%\n",
            "iter 3970: loss 0.7258, time 34.24ms, mfu 14.88%\n",
            "iter 3980: loss 0.7137, time 34.10ms, mfu 14.90%\n",
            "iter 3990: loss 0.6885, time 34.28ms, mfu 14.92%\n",
            "step 4000: train loss 0.4925, val loss 1.8301\n",
            "iter 4000: loss 0.7114, time 5497.14ms, mfu 13.44%\n",
            "iter 4010: loss 0.7031, time 34.38ms, mfu 13.59%\n",
            "iter 4020: loss 0.6931, time 34.32ms, mfu 13.73%\n",
            "iter 4030: loss 0.7260, time 34.32ms, mfu 13.86%\n",
            "iter 4040: loss 0.7083, time 34.54ms, mfu 13.97%\n",
            "iter 4050: loss 0.7133, time 34.18ms, mfu 14.08%\n",
            "iter 4060: loss 0.7014, time 34.22ms, mfu 14.18%\n",
            "iter 4070: loss 0.7079, time 34.39ms, mfu 14.26%\n",
            "iter 4080: loss 0.6983, time 34.24ms, mfu 14.34%\n",
            "iter 4090: loss 0.7108, time 34.38ms, mfu 14.40%\n",
            "iter 4100: loss 0.6967, time 34.25ms, mfu 14.47%\n",
            "iter 4110: loss 0.6857, time 34.20ms, mfu 14.53%\n",
            "iter 4120: loss 0.6965, time 34.41ms, mfu 14.57%\n",
            "iter 4130: loss 0.6777, time 34.34ms, mfu 14.62%\n",
            "iter 4140: loss 0.6898, time 34.30ms, mfu 14.66%\n",
            "iter 4150: loss 0.6861, time 33.79ms, mfu 14.72%\n",
            "iter 4160: loss 0.6881, time 34.34ms, mfu 14.75%\n",
            "iter 4170: loss 0.6755, time 34.18ms, mfu 14.78%\n",
            "iter 4180: loss 0.6651, time 34.24ms, mfu 14.81%\n",
            "iter 4190: loss 0.6755, time 34.07ms, mfu 14.84%\n",
            "iter 4200: loss 0.6624, time 34.26ms, mfu 14.86%\n",
            "iter 4210: loss 0.6821, time 34.41ms, mfu 14.87%\n",
            "iter 4220: loss 0.7067, time 34.28ms, mfu 14.89%\n",
            "iter 4230: loss 0.6667, time 34.33ms, mfu 14.90%\n",
            "iter 4240: loss 0.6573, time 34.16ms, mfu 14.92%\n",
            "step 4250: train loss 0.4559, val loss 1.8751\n",
            "iter 4250: loss 0.6590, time 5512.73ms, mfu 13.44%\n",
            "iter 4260: loss 0.6644, time 34.29ms, mfu 13.59%\n",
            "iter 4270: loss 0.6807, time 34.37ms, mfu 13.73%\n",
            "iter 4280: loss 0.6897, time 34.22ms, mfu 13.87%\n",
            "iter 4290: loss 0.6844, time 34.53ms, mfu 13.97%\n",
            "iter 4300: loss 0.6598, time 34.32ms, mfu 14.08%\n",
            "iter 4310: loss 0.6651, time 34.27ms, mfu 14.17%\n",
            "iter 4320: loss 0.6633, time 34.31ms, mfu 14.26%\n",
            "iter 4330: loss 0.6722, time 34.20ms, mfu 14.34%\n",
            "iter 4340: loss 0.6636, time 34.25ms, mfu 14.41%\n",
            "iter 4350: loss 0.6764, time 34.31ms, mfu 14.47%\n",
            "iter 4360: loss 0.6615, time 34.24ms, mfu 14.53%\n",
            "iter 4370: loss 0.6496, time 34.38ms, mfu 14.58%\n",
            "iter 4380: loss 0.6519, time 34.23ms, mfu 14.62%\n",
            "iter 4390: loss 0.6627, time 34.43ms, mfu 14.66%\n",
            "iter 4400: loss 0.6665, time 34.16ms, mfu 14.70%\n",
            "iter 4410: loss 0.6542, time 34.26ms, mfu 14.73%\n",
            "iter 4420: loss 0.6653, time 34.25ms, mfu 14.77%\n",
            "iter 4430: loss 0.6407, time 34.28ms, mfu 14.79%\n",
            "iter 4440: loss 0.6491, time 34.13ms, mfu 14.82%\n",
            "iter 4450: loss 0.6572, time 34.17ms, mfu 14.85%\n",
            "iter 4460: loss 0.6442, time 34.42ms, mfu 14.86%\n",
            "iter 4470: loss 0.6360, time 34.32ms, mfu 14.88%\n",
            "iter 4480: loss 0.6474, time 34.26ms, mfu 14.89%\n",
            "iter 4490: loss 0.6549, time 34.09ms, mfu 14.92%\n",
            "step 4500: train loss 0.4273, val loss 1.9036\n",
            "iter 4500: loss 0.6704, time 5510.53ms, mfu 13.43%\n",
            "iter 4510: loss 0.6319, time 34.17ms, mfu 13.60%\n",
            "iter 4520: loss 0.6403, time 34.29ms, mfu 13.74%\n",
            "iter 4530: loss 0.6396, time 34.67ms, mfu 13.85%\n",
            "iter 4540: loss 0.6448, time 34.24ms, mfu 13.97%\n",
            "iter 4550: loss 0.6501, time 34.20ms, mfu 14.08%\n",
            "iter 4560: loss 0.6472, time 34.13ms, mfu 14.19%\n",
            "iter 4570: loss 0.6311, time 34.25ms, mfu 14.27%\n",
            "iter 4580: loss 0.6332, time 34.29ms, mfu 14.35%\n",
            "iter 4590: loss 0.6392, time 34.33ms, mfu 14.41%\n",
            "iter 4600: loss 0.6240, time 34.39ms, mfu 14.47%\n",
            "iter 4610: loss 0.6270, time 34.36ms, mfu 14.52%\n",
            "iter 4620: loss 0.6421, time 34.22ms, mfu 14.58%\n",
            "iter 4630: loss 0.6567, time 35.05ms, mfu 14.59%\n",
            "iter 4640: loss 0.6423, time 34.12ms, mfu 14.64%\n",
            "iter 4650: loss 0.6248, time 34.17ms, mfu 14.69%\n",
            "iter 4660: loss 0.6171, time 34.22ms, mfu 14.72%\n",
            "iter 4670: loss 0.6423, time 34.26ms, mfu 14.75%\n",
            "iter 4680: loss 0.6486, time 34.20ms, mfu 14.79%\n",
            "iter 4690: loss 0.6343, time 34.26ms, mfu 14.81%\n",
            "iter 4700: loss 0.6276, time 34.28ms, mfu 14.83%\n",
            "iter 4710: loss 0.6354, time 34.26ms, mfu 14.86%\n",
            "iter 4720: loss 0.6219, time 34.28ms, mfu 14.87%\n",
            "iter 4730: loss 0.6192, time 34.24ms, mfu 14.89%\n",
            "iter 4740: loss 0.6424, time 34.29ms, mfu 14.90%\n",
            "step 4750: train loss 0.4041, val loss 1.9514\n",
            "iter 4750: loss 0.6385, time 5491.92ms, mfu 13.42%\n",
            "iter 4760: loss 0.6301, time 34.27ms, mfu 13.59%\n",
            "iter 4770: loss 0.6166, time 34.25ms, mfu 13.73%\n",
            "iter 4780: loss 0.6150, time 34.27ms, mfu 13.86%\n",
            "iter 4790: loss 0.6242, time 34.21ms, mfu 13.98%\n",
            "iter 4800: loss 0.6153, time 34.24ms, mfu 14.09%\n",
            "iter 4810: loss 0.6485, time 34.29ms, mfu 14.18%\n",
            "iter 4820: loss 0.6311, time 34.31ms, mfu 14.27%\n",
            "iter 4830: loss 0.6386, time 34.35ms, mfu 14.34%\n",
            "iter 4840: loss 0.6322, time 34.39ms, mfu 14.41%\n",
            "iter 4850: loss 0.6296, time 34.31ms, mfu 14.47%\n",
            "iter 4860: loss 0.6268, time 34.33ms, mfu 14.52%\n",
            "iter 4870: loss 0.6469, time 34.24ms, mfu 14.57%\n",
            "iter 4880: loss 0.6172, time 34.32ms, mfu 14.62%\n",
            "iter 4890: loss 0.6116, time 34.31ms, mfu 14.66%\n",
            "iter 4900: loss 0.6216, time 34.15ms, mfu 14.70%\n",
            "iter 4910: loss 0.6272, time 34.36ms, mfu 14.73%\n",
            "iter 4920: loss 0.6409, time 34.64ms, mfu 14.75%\n",
            "iter 4930: loss 0.6149, time 34.39ms, mfu 14.77%\n",
            "iter 4940: loss 0.5991, time 34.17ms, mfu 14.80%\n",
            "iter 4950: loss 0.6022, time 34.24ms, mfu 14.83%\n",
            "iter 4960: loss 0.6259, time 34.46ms, mfu 14.84%\n",
            "iter 4970: loss 0.6396, time 34.22ms, mfu 14.86%\n",
            "iter 4980: loss 0.6067, time 34.54ms, mfu 14.87%\n",
            "iter 4990: loss 0.6079, time 34.16ms, mfu 14.89%\n",
            "step 5000: train loss 0.3874, val loss 1.9614\n",
            "iter 5000: loss 0.5968, time 5495.93ms, mfu 13.41%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "loss_dict = {2: 0.7879, 3: 0.7243, 5: 0.6506, 7: 0.5968}\n",
        "plt.plot(list(loss_dict.keys()), list(loss_dict.values()), marker='o')\n",
        "plt.xlabel('Number of Heads')\n",
        "plt.ylabel('Loss at Iter 5000')\n",
        "plt.title('Validation Loss vs Number of Heads (n_layer=7)')\n",
        "os.makedirs('figures', exist_ok=True)\n",
        "plt.savefig('figures/loss_vs_heads.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2cSJjnLuv5U-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "d91a2727-64a5-4405-d2f6-aae88b329df8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 复制shakespeare_char文件夹，创建code_generation数据集\n",
        "!cp -r data/shakespeare_char data/code_generation\n",
        "\n",
        "# 用你自己的Python代码替换 data/code_generation/input.txt\n",
        "# 你可以用如下命令上传文件\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "# 然后将上传的文件覆盖 input.txt\n",
        "!mv input.txt data/code_generation/input.txt"
      ],
      "metadata": {
        "id": "gi4BNpysv7Ag",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "outputId": "bc369154-7118-4b46-ca53-5bb2ab3243b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e7e7b81d-ef3a-45bf-8f4f-da8f5d5ceca8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e7e7b81d-ef3a-45bf-8f4f-da8f5d5ceca8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving input.txt to input (1).txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 处理新数据集，生成 train.bin 和 val.bin\n",
        "!python data/code_generation/prepare.py"
      ],
      "metadata": {
        "id": "evoOy5Kmv8pg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72414cbf-ae05-4f63-9b88-0a55ade339ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1,056,211\n",
            "all the unique characters: \n",
            " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\n",
            "vocab size: 96\n",
            "train has 950,589 tokens\n",
            "val has 105,622 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 复制并修改配置文件\n",
        "!cp config/train_shakespeare_char.py config/train_code_generation.py\n",
        "# 用代码修改 config/train_code_generation.py 里的数据路径等参数\n",
        "config_path = 'config/train_code_generation.py'\n",
        "\n",
        "with open(config_path, 'r', encoding='utf-8') as f:\n",
        "    content = f.read()\n",
        "\n",
        "# 替换 out_dir 和 dataset 的值\n",
        "content = content.replace(\"out_dir = 'out-shakespeare-char'\", \"out_dir = 'out-code-generation'\")\n",
        "content = content.replace(\"dataset = 'data/shakespeare_char'\", \"dataset = 'data/code_generation'\")\n",
        "\n",
        "with open(config_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(content)\n",
        "# 训练新模型\n",
        "!python train.py config/train_code_generation.py"
      ],
      "metadata": {
        "id": "mcZ3z1wXv-LF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b6d6419-9313-4a31-e8f2-d03b5b14d02d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_code_generation.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-code-generation'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 10.65M\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "num decayed parameter tensors: 26, with 10,740,096 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n",
            "compiling the model... (takes a ~minute)\n",
            "step 0: train loss 4.2874, val loss 4.2823\n",
            "iter 0: loss 4.2654, time 11880.96ms, mfu -100.00%\n",
            "iter 10: loss 3.1461, time 10.61ms, mfu 35.11%\n",
            "iter 20: loss 2.7315, time 10.74ms, mfu 35.07%\n",
            "iter 30: loss 2.6182, time 10.53ms, mfu 35.10%\n",
            "iter 40: loss 2.5754, time 10.54ms, mfu 35.12%\n",
            "iter 50: loss 2.5251, time 10.48ms, mfu 35.17%\n",
            "iter 60: loss 2.5142, time 10.86ms, mfu 35.08%\n",
            "iter 70: loss 2.4945, time 10.51ms, mfu 35.12%\n",
            "iter 80: loss 2.4939, time 10.92ms, mfu 35.02%\n",
            "iter 90: loss 2.4673, time 10.57ms, mfu 35.04%\n",
            "iter 100: loss 2.4593, time 10.52ms, mfu 35.08%\n",
            "iter 110: loss 2.4619, time 10.86ms, mfu 35.00%\n",
            "iter 120: loss 2.4285, time 10.62ms, mfu 35.01%\n",
            "iter 130: loss 2.4126, time 10.73ms, mfu 34.98%\n",
            "iter 140: loss 2.4170, time 10.65ms, mfu 34.98%\n",
            "iter 150: loss 2.4140, time 10.66ms, mfu 34.98%\n",
            "iter 160: loss 2.3776, time 11.33ms, mfu 34.77%\n",
            "iter 170: loss 2.3557, time 10.84ms, mfu 34.73%\n",
            "iter 180: loss 2.3219, time 10.89ms, mfu 34.68%\n",
            "iter 190: loss 2.2534, time 10.82ms, mfu 34.65%\n",
            "iter 200: loss 2.2068, time 10.87ms, mfu 34.62%\n",
            "iter 210: loss 2.1451, time 12.04ms, mfu 34.25%\n",
            "iter 220: loss 2.1384, time 10.56ms, mfu 34.35%\n",
            "iter 230: loss 2.0698, time 10.54ms, mfu 34.45%\n",
            "iter 240: loss 2.0857, time 10.53ms, mfu 34.55%\n",
            "step 250: train loss 1.9719, val loss 2.0667\n",
            "saving checkpoint to out-code-generation\n",
            "iter 250: loss 2.0359, time 2834.48ms, mfu 31.11%\n",
            "iter 260: loss 1.9752, time 11.97ms, mfu 31.11%\n",
            "iter 270: loss 1.9815, time 10.84ms, mfu 31.43%\n",
            "iter 280: loss 1.9769, time 10.55ms, mfu 31.82%\n",
            "iter 290: loss 1.9129, time 10.69ms, mfu 32.13%\n",
            "iter 300: loss 1.9009, time 10.71ms, mfu 32.39%\n",
            "iter 310: loss 1.8708, time 11.11ms, mfu 32.51%\n",
            "iter 320: loss 1.8523, time 11.17ms, mfu 32.59%\n",
            "iter 330: loss 1.8171, time 10.93ms, mfu 32.74%\n",
            "iter 340: loss 1.7833, time 10.90ms, mfu 32.89%\n",
            "iter 350: loss 1.8271, time 11.09ms, mfu 32.96%\n",
            "iter 360: loss 1.7737, time 10.89ms, mfu 33.08%\n",
            "iter 370: loss 1.7435, time 10.82ms, mfu 33.22%\n",
            "iter 380: loss 1.7310, time 11.24ms, mfu 33.21%\n",
            "iter 390: loss 1.7304, time 11.37ms, mfu 33.17%\n",
            "iter 400: loss 1.7699, time 10.96ms, mfu 33.25%\n",
            "iter 410: loss 1.6981, time 11.44ms, mfu 33.18%\n",
            "iter 420: loss 1.7083, time 10.88ms, mfu 33.29%\n",
            "iter 430: loss 1.6837, time 10.88ms, mfu 33.39%\n",
            "iter 440: loss 1.6534, time 11.85ms, mfu 33.19%\n",
            "iter 450: loss 1.6483, time 10.57ms, mfu 33.40%\n",
            "iter 460: loss 1.5975, time 11.13ms, mfu 33.41%\n",
            "iter 470: loss 1.6443, time 10.52ms, mfu 33.61%\n",
            "iter 480: loss 1.6176, time 10.67ms, mfu 33.74%\n",
            "iter 490: loss 1.5921, time 10.69ms, mfu 33.85%\n",
            "step 500: train loss 1.5178, val loss 1.7116\n",
            "saving checkpoint to out-code-generation\n",
            "iter 500: loss 1.5943, time 2883.24ms, mfu 30.48%\n",
            "iter 510: loss 1.6071, time 10.65ms, mfu 30.93%\n",
            "iter 520: loss 1.5903, time 10.68ms, mfu 31.33%\n",
            "iter 530: loss 1.5621, time 10.70ms, mfu 31.68%\n",
            "iter 540: loss 1.6131, time 10.66ms, mfu 32.01%\n",
            "iter 550: loss 1.5608, time 10.90ms, mfu 32.22%\n",
            "iter 560: loss 1.5533, time 10.66ms, mfu 32.50%\n",
            "iter 570: loss 1.5613, time 10.62ms, mfu 32.76%\n",
            "iter 580: loss 1.5340, time 10.76ms, mfu 32.94%\n",
            "iter 590: loss 1.4889, time 10.75ms, mfu 33.12%\n",
            "iter 600: loss 1.5099, time 11.45ms, mfu 33.06%\n",
            "iter 610: loss 1.5446, time 11.01ms, mfu 33.14%\n",
            "iter 620: loss 1.5334, time 10.64ms, mfu 33.33%\n",
            "iter 630: loss 1.5074, time 10.77ms, mfu 33.45%\n",
            "iter 640: loss 1.4598, time 10.66ms, mfu 33.60%\n",
            "iter 650: loss 1.4988, time 10.64ms, mfu 33.75%\n",
            "iter 660: loss 1.5013, time 10.60ms, mfu 33.89%\n",
            "iter 670: loss 1.4455, time 10.65ms, mfu 34.00%\n",
            "iter 680: loss 1.5058, time 10.66ms, mfu 34.09%\n",
            "iter 690: loss 1.4647, time 10.58ms, mfu 34.21%\n",
            "iter 700: loss 1.4836, time 10.58ms, mfu 34.31%\n",
            "iter 710: loss 1.4555, time 10.92ms, mfu 34.29%\n",
            "iter 720: loss 1.4365, time 10.84ms, mfu 34.30%\n",
            "iter 730: loss 1.4248, time 10.66ms, mfu 34.36%\n",
            "iter 740: loss 1.4247, time 10.64ms, mfu 34.43%\n",
            "step 750: train loss 1.3611, val loss 1.5886\n",
            "saving checkpoint to out-code-generation\n",
            "iter 750: loss 1.4215, time 2855.77ms, mfu 31.00%\n",
            "iter 760: loss 1.4448, time 10.65ms, mfu 31.40%\n",
            "iter 770: loss 1.4284, time 10.76ms, mfu 31.72%\n",
            "iter 780: loss 1.4241, time 10.59ms, mfu 32.07%\n",
            "iter 790: loss 1.4161, time 12.27ms, mfu 31.90%\n",
            "iter 800: loss 1.4316, time 11.40ms, mfu 31.98%\n",
            "iter 810: loss 1.4023, time 10.74ms, mfu 32.25%\n",
            "iter 820: loss 1.4044, time 10.67ms, mfu 32.51%\n",
            "iter 830: loss 1.3956, time 10.59ms, mfu 32.78%\n",
            "iter 840: loss 1.3966, time 10.72ms, mfu 32.98%\n",
            "iter 850: loss 1.3898, time 10.82ms, mfu 33.13%\n",
            "iter 860: loss 1.3938, time 10.87ms, mfu 33.24%\n",
            "iter 870: loss 1.3965, time 10.58ms, mfu 33.44%\n",
            "iter 880: loss 1.3746, time 10.73ms, mfu 33.57%\n",
            "iter 890: loss 1.3868, time 10.74ms, mfu 33.68%\n",
            "iter 900: loss 1.3687, time 10.62ms, mfu 33.82%\n",
            "iter 910: loss 1.3216, time 10.56ms, mfu 33.97%\n",
            "iter 920: loss 1.3623, time 10.80ms, mfu 34.02%\n",
            "iter 930: loss 1.3685, time 14.40ms, mfu 33.21%\n",
            "iter 940: loss 1.3440, time 10.91ms, mfu 33.30%\n",
            "iter 950: loss 1.3472, time 11.54ms, mfu 33.20%\n",
            "iter 960: loss 1.3602, time 10.54ms, mfu 33.42%\n",
            "iter 970: loss 1.3575, time 12.69ms, mfu 33.01%\n",
            "iter 980: loss 1.3574, time 11.82ms, mfu 32.86%\n",
            "iter 990: loss 1.3392, time 10.48ms, mfu 33.13%\n",
            "step 1000: train loss 1.2717, val loss 1.5231\n",
            "saving checkpoint to out-code-generation\n",
            "iter 1000: loss 1.3362, time 3002.20ms, mfu 29.83%\n",
            "iter 1010: loss 1.3436, time 10.49ms, mfu 30.40%\n",
            "iter 1020: loss 1.3163, time 10.61ms, mfu 30.87%\n",
            "iter 1030: loss 1.3296, time 10.54ms, mfu 31.32%\n",
            "iter 1040: loss 1.3526, time 11.22ms, mfu 31.51%\n",
            "iter 1050: loss 1.2902, time 10.86ms, mfu 31.79%\n",
            "iter 1060: loss 1.3344, time 10.52ms, mfu 32.15%\n",
            "iter 1070: loss 1.3302, time 10.51ms, mfu 32.48%\n",
            "iter 1080: loss 1.3336, time 11.61ms, mfu 32.44%\n",
            "iter 1090: loss 1.3510, time 10.50ms, mfu 32.75%\n",
            "iter 1100: loss 1.3194, time 10.52ms, mfu 33.01%\n",
            "iter 1110: loss 1.2984, time 10.44ms, mfu 33.28%\n",
            "iter 1120: loss 1.2959, time 10.78ms, mfu 33.41%\n",
            "iter 1130: loss 1.2934, time 10.55ms, mfu 33.60%\n",
            "iter 1140: loss 1.2971, time 10.53ms, mfu 33.78%\n",
            "iter 1150: loss 1.3104, time 10.46ms, mfu 33.97%\n",
            "iter 1160: loss 1.3242, time 10.56ms, mfu 34.10%\n",
            "iter 1170: loss 1.2983, time 10.49ms, mfu 34.24%\n",
            "iter 1180: loss 1.3127, time 10.62ms, mfu 34.33%\n",
            "iter 1190: loss 1.2663, time 10.48ms, mfu 34.45%\n",
            "iter 1200: loss 1.2901, time 10.49ms, mfu 34.56%\n",
            "iter 1210: loss 1.2650, time 10.44ms, mfu 34.67%\n",
            "iter 1220: loss 1.3079, time 10.56ms, mfu 34.73%\n",
            "iter 1230: loss 1.2987, time 10.76ms, mfu 34.72%\n",
            "iter 1240: loss 1.3056, time 10.61ms, mfu 34.76%\n",
            "step 1250: train loss 1.2065, val loss 1.4949\n",
            "saving checkpoint to out-code-generation\n",
            "iter 1250: loss 1.2754, time 2868.18ms, mfu 31.30%\n",
            "iter 1260: loss 1.2821, time 10.59ms, mfu 31.69%\n",
            "iter 1270: loss 1.2624, time 10.78ms, mfu 31.98%\n",
            "iter 1280: loss 1.2594, time 10.62ms, mfu 32.29%\n",
            "iter 1290: loss 1.2786, time 10.59ms, mfu 32.58%\n",
            "iter 1300: loss 1.2980, time 11.13ms, mfu 32.67%\n",
            "iter 1310: loss 1.2343, time 10.60ms, mfu 32.92%\n",
            "iter 1320: loss 1.3062, time 11.36ms, mfu 32.91%\n",
            "iter 1330: loss 1.2692, time 10.70ms, mfu 33.10%\n",
            "iter 1340: loss 1.2970, time 10.92ms, mfu 33.20%\n",
            "iter 1350: loss 1.2523, time 11.25ms, mfu 33.19%\n",
            "iter 1360: loss 1.2741, time 10.53ms, mfu 33.41%\n",
            "iter 1370: loss 1.2599, time 10.52ms, mfu 33.61%\n",
            "iter 1380: loss 1.2578, time 11.12ms, mfu 33.60%\n",
            "iter 1390: loss 1.2473, time 10.90ms, mfu 33.66%\n",
            "iter 1400: loss 1.2598, time 10.61ms, mfu 33.80%\n",
            "iter 1410: loss 1.2556, time 10.50ms, mfu 33.97%\n",
            "iter 1420: loss 1.2795, time 10.70ms, mfu 34.06%\n",
            "iter 1430: loss 1.2451, time 10.58ms, mfu 34.17%\n",
            "iter 1440: loss 1.2542, time 12.48ms, mfu 33.74%\n",
            "iter 1450: loss 1.2365, time 11.57ms, mfu 33.59%\n",
            "iter 1460: loss 1.2394, time 12.35ms, mfu 33.25%\n",
            "iter 1470: loss 1.2258, time 10.66ms, mfu 33.42%\n",
            "iter 1480: loss 1.2147, time 12.77ms, mfu 32.99%\n",
            "iter 1490: loss 1.2382, time 10.91ms, mfu 33.11%\n",
            "step 1500: train loss 1.1526, val loss 1.4732\n",
            "saving checkpoint to out-code-generation\n",
            "iter 1500: loss 1.1841, time 3020.79ms, mfu 29.81%\n",
            "iter 1510: loss 1.2352, time 10.58ms, mfu 30.35%\n",
            "iter 1520: loss 1.2286, time 10.80ms, mfu 30.77%\n",
            "iter 1530: loss 1.2594, time 10.75ms, mfu 31.15%\n",
            "iter 1540: loss 1.1916, time 11.02ms, mfu 31.42%\n",
            "iter 1550: loss 1.2323, time 10.85ms, mfu 31.71%\n",
            "iter 1560: loss 1.2097, time 11.11ms, mfu 31.90%\n",
            "iter 1570: loss 1.2401, time 10.79ms, mfu 32.16%\n",
            "iter 1580: loss 1.2040, time 10.70ms, mfu 32.43%\n",
            "iter 1590: loss 1.1960, time 10.81ms, mfu 32.63%\n",
            "iter 1600: loss 1.1983, time 11.01ms, mfu 32.75%\n",
            "iter 1610: loss 1.2406, time 10.73ms, mfu 32.95%\n",
            "iter 1620: loss 1.1867, time 12.61ms, mfu 32.61%\n",
            "iter 1630: loss 1.2098, time 10.71ms, mfu 32.83%\n",
            "iter 1640: loss 1.1982, time 10.62ms, mfu 33.05%\n",
            "iter 1650: loss 1.1814, time 10.60ms, mfu 33.26%\n",
            "iter 1660: loss 1.2259, time 10.60ms, mfu 33.45%\n",
            "iter 1670: loss 1.2013, time 10.62ms, mfu 33.62%\n",
            "iter 1680: loss 1.2016, time 11.23ms, mfu 33.57%\n",
            "iter 1690: loss 1.2005, time 10.65ms, mfu 33.71%\n",
            "iter 1700: loss 1.1793, time 11.31ms, mfu 33.64%\n",
            "iter 1710: loss 1.1804, time 10.67ms, mfu 33.77%\n",
            "iter 1720: loss 1.1835, time 10.71ms, mfu 33.87%\n",
            "iter 1730: loss 1.2027, time 10.89ms, mfu 33.90%\n",
            "iter 1740: loss 1.1649, time 10.63ms, mfu 34.02%\n",
            "step 1750: train loss 1.1066, val loss 1.4694\n",
            "saving checkpoint to out-code-generation\n",
            "iter 1750: loss 1.1923, time 2888.77ms, mfu 30.63%\n",
            "iter 1760: loss 1.1953, time 10.82ms, mfu 31.01%\n",
            "iter 1770: loss 1.2007, time 10.51ms, mfu 31.45%\n",
            "iter 1780: loss 1.1934, time 10.47ms, mfu 31.87%\n",
            "iter 1790: loss 1.1991, time 10.59ms, mfu 32.20%\n",
            "iter 1800: loss 1.1814, time 10.55ms, mfu 32.51%\n",
            "iter 1810: loss 1.1686, time 10.75ms, mfu 32.73%\n",
            "iter 1820: loss 1.1739, time 10.79ms, mfu 32.91%\n",
            "iter 1830: loss 1.1705, time 11.03ms, mfu 33.00%\n",
            "iter 1840: loss 1.1620, time 10.78ms, mfu 33.15%\n",
            "iter 1850: loss 1.1580, time 11.16ms, mfu 33.18%\n",
            "iter 1860: loss 1.1783, time 11.31ms, mfu 33.15%\n",
            "iter 1870: loss 1.1443, time 11.01ms, mfu 33.22%\n",
            "iter 1880: loss 1.1829, time 11.16ms, mfu 33.24%\n",
            "iter 1890: loss 1.1807, time 10.89ms, mfu 33.34%\n",
            "iter 1900: loss 1.1270, time 11.04ms, mfu 33.38%\n",
            "iter 1910: loss 1.1766, time 11.44ms, mfu 33.30%\n",
            "iter 1920: loss 1.1711, time 11.02ms, mfu 33.35%\n",
            "iter 1930: loss 1.1499, time 11.31ms, mfu 33.31%\n",
            "iter 1940: loss 1.1301, time 10.60ms, mfu 33.50%\n",
            "iter 1950: loss 1.1389, time 10.83ms, mfu 33.59%\n",
            "iter 1960: loss 1.1492, time 10.77ms, mfu 33.69%\n",
            "iter 1970: loss 1.1549, time 10.72ms, mfu 33.79%\n",
            "iter 1980: loss 1.1516, time 10.81ms, mfu 33.86%\n",
            "iter 1990: loss 1.1650, time 10.98ms, mfu 33.87%\n",
            "step 2000: train loss 1.0620, val loss 1.4786\n",
            "iter 2000: loss 1.1308, time 2624.56ms, mfu 30.50%\n",
            "iter 2010: loss 1.1331, time 10.61ms, mfu 30.96%\n",
            "iter 2020: loss 1.1213, time 10.52ms, mfu 31.40%\n",
            "iter 2030: loss 1.1590, time 10.69ms, mfu 31.75%\n",
            "iter 2040: loss 1.1375, time 10.62ms, mfu 32.08%\n",
            "iter 2050: loss 1.1129, time 10.58ms, mfu 32.40%\n",
            "iter 2060: loss 1.1035, time 10.53ms, mfu 32.69%\n",
            "iter 2070: loss 1.1240, time 10.52ms, mfu 32.97%\n",
            "iter 2080: loss 1.1184, time 10.81ms, mfu 33.12%\n",
            "iter 2090: loss 1.1373, time 11.29ms, mfu 33.11%\n",
            "iter 2100: loss 1.1331, time 10.99ms, mfu 33.19%\n",
            "iter 2110: loss 1.1402, time 10.54ms, mfu 33.40%\n",
            "iter 2120: loss 1.1325, time 10.63ms, mfu 33.57%\n",
            "iter 2130: loss 1.1376, time 10.93ms, mfu 33.62%\n",
            "iter 2140: loss 1.1408, time 10.48ms, mfu 33.82%\n",
            "iter 2150: loss 1.1267, time 10.60ms, mfu 33.95%\n",
            "iter 2160: loss 1.1430, time 11.18ms, mfu 33.89%\n",
            "iter 2170: loss 1.1372, time 11.01ms, mfu 33.88%\n",
            "iter 2180: loss 1.1096, time 10.61ms, mfu 34.01%\n",
            "iter 2190: loss 1.1092, time 10.71ms, mfu 34.09%\n",
            "iter 2200: loss 1.1257, time 10.59ms, mfu 34.19%\n",
            "iter 2210: loss 1.1197, time 10.63ms, mfu 34.28%\n",
            "iter 2220: loss 1.1263, time 10.70ms, mfu 34.34%\n",
            "iter 2230: loss 1.1197, time 10.96ms, mfu 34.30%\n",
            "iter 2240: loss 1.1202, time 10.60ms, mfu 34.39%\n",
            "step 2250: train loss 1.0119, val loss 1.4897\n",
            "iter 2250: loss 1.1149, time 2615.11ms, mfu 30.96%\n",
            "iter 2260: loss 1.1126, time 10.50ms, mfu 31.41%\n",
            "iter 2270: loss 1.1300, time 10.51ms, mfu 31.82%\n",
            "iter 2280: loss 1.0938, time 10.99ms, mfu 32.03%\n",
            "iter 2290: loss 1.1464, time 10.81ms, mfu 32.27%\n",
            "iter 2300: loss 1.1142, time 10.58ms, mfu 32.57%\n",
            "iter 2310: loss 1.0998, time 10.67ms, mfu 32.80%\n",
            "iter 2320: loss 1.0979, time 10.87ms, mfu 32.95%\n",
            "iter 2330: loss 1.1062, time 10.51ms, mfu 33.20%\n",
            "iter 2340: loss 1.1192, time 10.55ms, mfu 33.41%\n",
            "iter 2350: loss 1.1058, time 10.58ms, mfu 33.59%\n",
            "iter 2360: loss 1.1101, time 10.55ms, mfu 33.77%\n",
            "iter 2370: loss 1.1020, time 10.54ms, mfu 33.92%\n",
            "iter 2380: loss 1.0855, time 10.53ms, mfu 34.07%\n",
            "iter 2390: loss 1.0845, time 10.65ms, mfu 34.16%\n",
            "iter 2400: loss 1.0858, time 10.60ms, mfu 34.26%\n",
            "iter 2410: loss 1.0725, time 10.61ms, mfu 34.35%\n",
            "iter 2420: loss 1.0816, time 10.98ms, mfu 34.30%\n",
            "iter 2430: loss 1.0550, time 11.89ms, mfu 34.01%\n",
            "iter 2440: loss 1.0631, time 10.92ms, mfu 34.02%\n",
            "iter 2450: loss 1.0717, time 10.65ms, mfu 34.12%\n",
            "iter 2460: loss 1.0875, time 10.71ms, mfu 34.19%\n",
            "iter 2470: loss 1.0881, time 10.72ms, mfu 34.24%\n",
            "iter 2480: loss 1.0865, time 10.61ms, mfu 34.33%\n",
            "iter 2490: loss 1.0593, time 10.68ms, mfu 34.39%\n",
            "step 2500: train loss 0.9641, val loss 1.5075\n",
            "iter 2500: loss 1.0869, time 2643.49ms, mfu 30.96%\n",
            "iter 2510: loss 1.0700, time 10.85ms, mfu 31.30%\n",
            "iter 2520: loss 1.0440, time 10.67ms, mfu 31.66%\n",
            "iter 2530: loss 1.0558, time 10.46ms, mfu 32.06%\n",
            "iter 2540: loss 1.0607, time 10.61ms, mfu 32.37%\n",
            "iter 2550: loss 1.0698, time 10.52ms, mfu 32.67%\n",
            "iter 2560: loss 1.0623, time 10.50ms, mfu 32.96%\n",
            "iter 2570: loss 1.0790, time 10.81ms, mfu 33.11%\n",
            "iter 2580: loss 1.0793, time 10.53ms, mfu 33.34%\n",
            "iter 2590: loss 1.0710, time 10.57ms, mfu 33.53%\n",
            "iter 2600: loss 1.0674, time 10.73ms, mfu 33.65%\n",
            "iter 2610: loss 1.0575, time 10.64ms, mfu 33.78%\n",
            "iter 2620: loss 1.0468, time 10.48ms, mfu 33.96%\n",
            "iter 2630: loss 1.0305, time 10.61ms, mfu 34.08%\n",
            "iter 2640: loss 1.0493, time 10.51ms, mfu 34.21%\n",
            "iter 2650: loss 1.0655, time 10.74ms, mfu 34.26%\n",
            "iter 2660: loss 1.0531, time 10.76ms, mfu 34.30%\n",
            "iter 2670: loss 1.0154, time 10.68ms, mfu 34.36%\n",
            "iter 2680: loss 1.0544, time 10.65ms, mfu 34.42%\n",
            "iter 2690: loss 1.0532, time 10.77ms, mfu 34.44%\n",
            "iter 2700: loss 1.0269, time 10.56ms, mfu 34.52%\n",
            "iter 2710: loss 1.0496, time 10.55ms, mfu 34.60%\n",
            "iter 2720: loss 1.0431, time 10.59ms, mfu 34.66%\n",
            "iter 2730: loss 1.0610, time 10.55ms, mfu 34.73%\n",
            "iter 2740: loss 1.0331, time 10.54ms, mfu 34.79%\n",
            "step 2750: train loss 0.9190, val loss 1.5191\n",
            "iter 2750: loss 1.0368, time 2621.30ms, mfu 31.33%\n",
            "iter 2760: loss 1.0358, time 10.50ms, mfu 31.74%\n",
            "iter 2770: loss 1.0254, time 10.73ms, mfu 32.04%\n",
            "iter 2780: loss 1.0249, time 10.60ms, mfu 32.35%\n",
            "iter 2790: loss 1.0408, time 10.63ms, mfu 32.62%\n",
            "iter 2800: loss 1.0094, time 10.63ms, mfu 32.86%\n",
            "iter 2810: loss 1.0478, time 10.61ms, mfu 33.09%\n",
            "iter 2820: loss 1.0228, time 10.74ms, mfu 33.25%\n",
            "iter 2830: loss 1.0377, time 10.60ms, mfu 33.44%\n",
            "iter 2840: loss 0.9978, time 10.62ms, mfu 33.60%\n",
            "iter 2850: loss 1.0192, time 10.67ms, mfu 33.74%\n",
            "iter 2860: loss 1.0289, time 10.90ms, mfu 33.78%\n",
            "iter 2870: loss 1.0068, time 10.58ms, mfu 33.92%\n",
            "iter 2880: loss 1.0432, time 10.48ms, mfu 34.09%\n",
            "iter 2890: loss 1.0138, time 10.71ms, mfu 34.16%\n",
            "iter 2900: loss 0.9936, time 11.21ms, mfu 34.06%\n",
            "iter 2910: loss 1.0417, time 10.58ms, mfu 34.18%\n",
            "iter 2920: loss 1.0184, time 10.55ms, mfu 34.29%\n",
            "iter 2930: loss 0.9911, time 10.54ms, mfu 34.40%\n",
            "iter 2940: loss 0.9922, time 10.80ms, mfu 34.41%\n",
            "iter 2950: loss 1.0286, time 10.57ms, mfu 34.49%\n",
            "iter 2960: loss 1.0033, time 10.52ms, mfu 34.58%\n",
            "iter 2970: loss 0.9950, time 10.56ms, mfu 34.65%\n",
            "iter 2980: loss 0.9960, time 10.50ms, mfu 34.74%\n",
            "iter 2990: loss 0.9833, time 10.52ms, mfu 34.80%\n",
            "step 3000: train loss 0.8722, val loss 1.5318\n",
            "iter 3000: loss 0.9895, time 2646.04ms, mfu 31.34%\n",
            "iter 3010: loss 0.9975, time 10.92ms, mfu 31.62%\n",
            "iter 3020: loss 0.9999, time 10.57ms, mfu 31.98%\n",
            "iter 3030: loss 1.0057, time 10.90ms, mfu 32.20%\n",
            "iter 3040: loss 1.0214, time 10.74ms, mfu 32.45%\n",
            "iter 3050: loss 0.9820, time 11.17ms, mfu 32.54%\n",
            "iter 3060: loss 0.9986, time 10.51ms, mfu 32.84%\n",
            "iter 3070: loss 1.0218, time 10.55ms, mfu 33.08%\n",
            "iter 3080: loss 0.9950, time 10.44ms, mfu 33.35%\n",
            "iter 3090: loss 0.9869, time 10.44ms, mfu 33.58%\n",
            "iter 3100: loss 1.0026, time 10.51ms, mfu 33.77%\n",
            "iter 3110: loss 0.9701, time 10.49ms, mfu 33.94%\n",
            "iter 3120: loss 1.0046, time 10.57ms, mfu 34.08%\n",
            "iter 3130: loss 0.9793, time 10.67ms, mfu 34.16%\n",
            "iter 3140: loss 0.9841, time 10.60ms, mfu 34.26%\n",
            "iter 3150: loss 0.9982, time 10.50ms, mfu 34.38%\n",
            "iter 3160: loss 1.0084, time 10.59ms, mfu 34.46%\n",
            "iter 3170: loss 0.9645, time 10.57ms, mfu 34.54%\n",
            "iter 3180: loss 0.9800, time 10.74ms, mfu 34.55%\n",
            "iter 3190: loss 0.9977, time 10.73ms, mfu 34.57%\n",
            "iter 3200: loss 0.9694, time 10.93ms, mfu 34.52%\n",
            "iter 3210: loss 0.9715, time 10.58ms, mfu 34.59%\n",
            "iter 3220: loss 0.9631, time 10.54ms, mfu 34.67%\n",
            "iter 3230: loss 0.9569, time 10.64ms, mfu 34.70%\n",
            "iter 3240: loss 0.9564, time 10.55ms, mfu 34.77%\n",
            "step 3250: train loss 0.8270, val loss 1.5611\n",
            "iter 3250: loss 0.9725, time 2622.00ms, mfu 31.30%\n",
            "iter 3260: loss 0.9639, time 11.16ms, mfu 31.51%\n",
            "iter 3270: loss 0.9722, time 10.55ms, mfu 31.89%\n",
            "iter 3280: loss 0.9575, time 10.66ms, mfu 32.20%\n",
            "iter 3290: loss 0.9502, time 10.58ms, mfu 32.50%\n",
            "iter 3300: loss 0.9487, time 10.86ms, mfu 32.68%\n",
            "iter 3310: loss 0.9534, time 11.13ms, mfu 32.76%\n",
            "iter 3320: loss 0.9707, time 10.55ms, mfu 33.02%\n",
            "iter 3330: loss 0.9639, time 10.70ms, mfu 33.20%\n",
            "iter 3340: loss 0.9579, time 10.48ms, mfu 33.44%\n",
            "iter 3350: loss 0.9636, time 10.48ms, mfu 33.65%\n",
            "iter 3360: loss 0.9348, time 10.55ms, mfu 33.82%\n",
            "iter 3370: loss 0.9682, time 10.54ms, mfu 33.97%\n",
            "iter 3380: loss 0.9487, time 10.83ms, mfu 34.01%\n",
            "iter 3390: loss 0.9538, time 10.52ms, mfu 34.15%\n",
            "iter 3400: loss 0.9616, time 10.65ms, mfu 34.24%\n",
            "iter 3410: loss 0.9455, time 10.52ms, mfu 34.36%\n",
            "iter 3420: loss 0.9412, time 10.73ms, mfu 34.40%\n",
            "iter 3430: loss 0.9474, time 10.58ms, mfu 34.48%\n",
            "iter 3440: loss 0.9768, time 10.71ms, mfu 34.51%\n",
            "iter 3450: loss 0.9553, time 10.60ms, mfu 34.58%\n",
            "iter 3460: loss 0.9558, time 10.55ms, mfu 34.65%\n",
            "iter 3470: loss 0.9487, time 10.64ms, mfu 34.69%\n",
            "iter 3480: loss 0.9486, time 10.44ms, mfu 34.79%\n",
            "iter 3490: loss 0.9179, time 10.55ms, mfu 34.84%\n",
            "step 3500: train loss 0.7854, val loss 1.5804\n",
            "iter 3500: loss 0.9109, time 2645.57ms, mfu 31.37%\n",
            "iter 3510: loss 0.9226, time 11.29ms, mfu 31.54%\n",
            "iter 3520: loss 0.9285, time 11.23ms, mfu 31.70%\n",
            "iter 3530: loss 0.9559, time 11.38ms, mfu 31.80%\n",
            "iter 3540: loss 0.9295, time 10.82ms, mfu 32.07%\n",
            "iter 3550: loss 0.9265, time 12.55ms, mfu 31.83%\n",
            "iter 3560: loss 0.9671, time 10.73ms, mfu 32.12%\n",
            "iter 3570: loss 0.9371, time 10.94ms, mfu 32.32%\n",
            "iter 3580: loss 0.9328, time 10.72ms, mfu 32.56%\n",
            "iter 3590: loss 0.9230, time 10.60ms, mfu 32.82%\n",
            "iter 3600: loss 0.9310, time 11.40ms, mfu 32.81%\n",
            "iter 3610: loss 0.9072, time 10.65ms, mfu 33.03%\n",
            "iter 3620: loss 0.9175, time 10.60ms, mfu 33.24%\n",
            "iter 3630: loss 0.9338, time 10.59ms, mfu 33.43%\n",
            "iter 3640: loss 0.9175, time 10.74ms, mfu 33.56%\n",
            "iter 3650: loss 0.9120, time 10.63ms, mfu 33.71%\n",
            "iter 3660: loss 0.9440, time 10.70ms, mfu 33.82%\n",
            "iter 3670: loss 0.9365, time 10.67ms, mfu 33.93%\n",
            "iter 3680: loss 0.9115, time 10.65ms, mfu 34.04%\n",
            "iter 3690: loss 0.9429, time 10.87ms, mfu 34.06%\n",
            "iter 3700: loss 0.8772, time 10.70ms, mfu 34.14%\n",
            "iter 3710: loss 0.8832, time 10.62ms, mfu 34.23%\n",
            "iter 3720: loss 0.9104, time 10.64ms, mfu 34.31%\n",
            "iter 3730: loss 0.9047, time 10.68ms, mfu 34.37%\n",
            "iter 3740: loss 0.9127, time 10.71ms, mfu 34.41%\n",
            "step 3750: train loss 0.7458, val loss 1.6004\n",
            "iter 3750: loss 0.9033, time 2620.31ms, mfu 30.98%\n",
            "iter 3760: loss 0.9441, time 10.55ms, mfu 31.42%\n",
            "iter 3770: loss 0.9377, time 10.57ms, mfu 31.80%\n",
            "iter 3780: loss 0.9280, time 10.72ms, mfu 32.10%\n",
            "iter 3790: loss 0.9087, time 10.56ms, mfu 32.41%\n",
            "iter 3800: loss 0.9163, time 11.03ms, mfu 32.55%\n",
            "iter 3810: loss 0.9316, time 10.85ms, mfu 32.73%\n",
            "iter 3820: loss 0.8982, time 10.59ms, mfu 32.98%\n",
            "iter 3830: loss 0.9094, time 10.56ms, mfu 33.21%\n",
            "iter 3840: loss 0.8898, time 10.61ms, mfu 33.40%\n",
            "iter 3850: loss 0.9078, time 10.61ms, mfu 33.57%\n",
            "iter 3860: loss 0.8756, time 10.67ms, mfu 33.70%\n",
            "iter 3870: loss 0.8863, time 11.03ms, mfu 33.71%\n",
            "iter 3880: loss 0.8962, time 10.57ms, mfu 33.87%\n",
            "iter 3890: loss 0.8981, time 10.87ms, mfu 33.91%\n",
            "iter 3900: loss 0.8872, time 10.62ms, mfu 34.03%\n",
            "iter 3910: loss 0.8944, time 10.79ms, mfu 34.08%\n",
            "iter 3920: loss 0.8852, time 10.70ms, mfu 34.15%\n",
            "iter 3930: loss 0.8977, time 10.62ms, mfu 34.24%\n",
            "iter 3940: loss 0.8743, time 10.86ms, mfu 34.25%\n",
            "iter 3950: loss 0.8849, time 10.57ms, mfu 34.35%\n",
            "iter 3960: loss 0.9126, time 10.69ms, mfu 34.40%\n",
            "iter 3970: loss 0.8964, time 10.54ms, mfu 34.50%\n",
            "iter 3980: loss 0.9081, time 10.58ms, mfu 34.57%\n",
            "iter 3990: loss 0.8818, time 10.53ms, mfu 34.65%\n",
            "step 4000: train loss 0.7127, val loss 1.6261\n",
            "iter 4000: loss 0.8500, time 2635.88ms, mfu 31.20%\n",
            "iter 4010: loss 0.8853, time 11.01ms, mfu 31.46%\n",
            "iter 4020: loss 0.8995, time 10.91ms, mfu 31.73%\n",
            "iter 4030: loss 0.8776, time 10.70ms, mfu 32.04%\n",
            "iter 4040: loss 0.8825, time 10.76ms, mfu 32.30%\n",
            "iter 4050: loss 0.8813, time 10.72ms, mfu 32.55%\n",
            "iter 4060: loss 0.8659, time 10.61ms, mfu 32.80%\n",
            "iter 4070: loss 0.8605, time 10.47ms, mfu 33.08%\n",
            "iter 4080: loss 0.8949, time 11.08ms, mfu 33.14%\n",
            "iter 4090: loss 0.8470, time 10.90ms, mfu 33.24%\n",
            "iter 4100: loss 0.9037, time 10.80ms, mfu 33.37%\n",
            "iter 4110: loss 0.8782, time 10.78ms, mfu 33.49%\n",
            "iter 4120: loss 0.8786, time 10.74ms, mfu 33.61%\n",
            "iter 4130: loss 0.8692, time 10.95ms, mfu 33.65%\n",
            "iter 4140: loss 0.8742, time 10.58ms, mfu 33.81%\n",
            "iter 4150: loss 0.8730, time 10.45ms, mfu 33.99%\n",
            "iter 4160: loss 0.8602, time 10.43ms, mfu 34.17%\n",
            "iter 4170: loss 0.8754, time 10.43ms, mfu 34.32%\n",
            "iter 4180: loss 0.8727, time 10.56ms, mfu 34.42%\n",
            "iter 4190: loss 0.8730, time 10.50ms, mfu 34.52%\n",
            "iter 4200: loss 0.8572, time 10.41ms, mfu 34.65%\n",
            "iter 4210: loss 0.8754, time 10.50ms, mfu 34.73%\n",
            "iter 4220: loss 0.8618, time 10.54ms, mfu 34.79%\n",
            "iter 4230: loss 0.8864, time 10.56ms, mfu 34.84%\n",
            "iter 4240: loss 0.8704, time 10.57ms, mfu 34.88%\n",
            "step 4250: train loss 0.6847, val loss 1.6488\n",
            "iter 4250: loss 0.8777, time 2622.18ms, mfu 31.41%\n",
            "iter 4260: loss 0.8672, time 10.86ms, mfu 31.70%\n",
            "iter 4270: loss 0.8780, time 10.51ms, mfu 32.07%\n",
            "iter 4280: loss 0.8592, time 10.48ms, mfu 32.42%\n",
            "iter 4290: loss 0.8313, time 10.50ms, mfu 32.72%\n",
            "iter 4300: loss 0.8321, time 10.55ms, mfu 32.98%\n",
            "iter 4310: loss 0.8555, time 10.77ms, mfu 33.15%\n",
            "iter 4320: loss 0.8436, time 10.55ms, mfu 33.36%\n",
            "iter 4330: loss 0.8747, time 10.57ms, mfu 33.55%\n",
            "iter 4340: loss 0.8344, time 10.73ms, mfu 33.67%\n",
            "iter 4350: loss 0.8478, time 10.63ms, mfu 33.81%\n",
            "iter 4360: loss 0.8609, time 10.92ms, mfu 33.84%\n",
            "iter 4370: loss 0.8634, time 11.30ms, mfu 33.75%\n",
            "iter 4380: loss 0.8487, time 10.66ms, mfu 33.87%\n",
            "iter 4390: loss 0.8732, time 10.71ms, mfu 33.96%\n",
            "iter 4400: loss 0.8406, time 10.58ms, mfu 34.09%\n",
            "iter 4410: loss 0.8684, time 10.69ms, mfu 34.17%\n",
            "iter 4420: loss 0.8674, time 10.69ms, mfu 34.24%\n",
            "iter 4430: loss 0.8468, time 10.55ms, mfu 34.34%\n",
            "iter 4440: loss 0.8517, time 10.61ms, mfu 34.42%\n",
            "iter 4450: loss 0.8552, time 10.63ms, mfu 34.48%\n",
            "iter 4460: loss 0.8432, time 10.96ms, mfu 34.44%\n",
            "iter 4470: loss 0.8613, time 10.50ms, mfu 34.54%\n",
            "iter 4480: loss 0.8405, time 10.71ms, mfu 34.57%\n",
            "iter 4490: loss 0.8382, time 10.62ms, mfu 34.62%\n",
            "step 4500: train loss 0.6595, val loss 1.6689\n",
            "iter 4500: loss 0.8583, time 2632.76ms, mfu 31.17%\n",
            "iter 4510: loss 0.8540, time 10.52ms, mfu 31.60%\n",
            "iter 4520: loss 0.8389, time 10.51ms, mfu 31.98%\n",
            "iter 4530: loss 0.8481, time 10.43ms, mfu 32.35%\n",
            "iter 4540: loss 0.8530, time 10.64ms, mfu 32.62%\n",
            "iter 4550: loss 0.8713, time 10.65ms, mfu 32.86%\n",
            "iter 4560: loss 0.8493, time 11.25ms, mfu 32.88%\n",
            "iter 4570: loss 0.8457, time 10.83ms, mfu 33.03%\n",
            "iter 4580: loss 0.8552, time 10.55ms, mfu 33.26%\n",
            "iter 4590: loss 0.8493, time 10.72ms, mfu 33.41%\n",
            "iter 4600: loss 0.8323, time 10.78ms, mfu 33.53%\n",
            "iter 4610: loss 0.8748, time 10.88ms, mfu 33.60%\n",
            "iter 4620: loss 0.8388, time 10.56ms, mfu 33.77%\n",
            "iter 4630: loss 0.8252, time 10.58ms, mfu 33.91%\n",
            "iter 4640: loss 0.8543, time 10.57ms, mfu 34.05%\n",
            "iter 4650: loss 0.8702, time 10.62ms, mfu 34.15%\n",
            "iter 4660: loss 0.8544, time 10.53ms, mfu 34.27%\n",
            "iter 4670: loss 0.8500, time 11.49ms, mfu 34.09%\n",
            "iter 4680: loss 0.8591, time 11.79ms, mfu 33.84%\n",
            "iter 4690: loss 0.8488, time 12.97ms, mfu 33.33%\n",
            "iter 4700: loss 0.8201, time 11.31ms, mfu 33.29%\n",
            "iter 4710: loss 0.7899, time 11.90ms, mfu 33.09%\n",
            "iter 4720: loss 0.8358, time 10.93ms, mfu 33.19%\n",
            "iter 4730: loss 0.8271, time 10.65ms, mfu 33.37%\n",
            "iter 4740: loss 0.8287, time 10.48ms, mfu 33.59%\n",
            "step 4750: train loss 0.6402, val loss 1.6886\n",
            "iter 4750: loss 0.8117, time 2633.07ms, mfu 30.25%\n",
            "iter 4760: loss 0.8230, time 10.66ms, mfu 30.72%\n",
            "iter 4770: loss 0.8052, time 10.88ms, mfu 31.07%\n",
            "iter 4780: loss 0.8154, time 10.88ms, mfu 31.39%\n",
            "iter 4790: loss 0.8440, time 10.83ms, mfu 31.69%\n",
            "iter 4800: loss 0.8283, time 10.77ms, mfu 31.98%\n",
            "iter 4810: loss 0.8450, time 10.92ms, mfu 32.19%\n",
            "iter 4820: loss 0.8336, time 10.92ms, mfu 32.39%\n",
            "iter 4830: loss 0.8338, time 10.59ms, mfu 32.67%\n",
            "iter 4840: loss 0.8353, time 11.02ms, mfu 32.78%\n",
            "iter 4850: loss 0.8302, time 10.84ms, mfu 32.94%\n",
            "iter 4860: loss 0.8245, time 10.78ms, mfu 33.10%\n",
            "iter 4870: loss 0.8069, time 10.83ms, mfu 33.24%\n",
            "iter 4880: loss 0.8349, time 10.49ms, mfu 33.47%\n",
            "iter 4890: loss 0.8150, time 10.98ms, mfu 33.51%\n",
            "iter 4900: loss 0.8098, time 10.54ms, mfu 33.70%\n",
            "iter 4910: loss 0.8330, time 10.60ms, mfu 33.84%\n",
            "iter 4920: loss 0.8337, time 10.53ms, mfu 34.00%\n",
            "iter 4930: loss 0.8129, time 10.48ms, mfu 34.15%\n",
            "iter 4940: loss 0.8120, time 10.76ms, mfu 34.20%\n",
            "iter 4950: loss 0.8410, time 10.56ms, mfu 34.31%\n",
            "iter 4960: loss 0.8386, time 10.87ms, mfu 34.31%\n",
            "iter 4970: loss 0.7908, time 10.65ms, mfu 34.38%\n",
            "iter 4980: loss 0.8010, time 10.67ms, mfu 34.43%\n",
            "iter 4990: loss 0.8227, time 10.98ms, mfu 34.38%\n",
            "step 5000: train loss 0.6259, val loss 1.7041\n",
            "iter 5000: loss 0.8264, time 2624.95ms, mfu 30.96%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 采样生成代码\n",
        "!python sample.py --out_dir=out-code-generation"
      ],
      "metadata": {
        "id": "b9J1lYoQwAAj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ccf8fce-56f7-4b98-baf9-31464da69c52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: out_dir = out-code-generation\n",
            "number of parameters: 10.65M\n",
            "Loading meta from data/shakespeare_char/meta.pkl...\n",
            "\n",
            "\n",
            "ANGELO:\n",
            "Ay, coward, will, by my cheek way to take him so.\n",
            "\n",
            "ELBOW:\n",
            "Far it is a quarrel back: let me saw your honour,\n",
            "And I must be some further of mine own.\n",
            "\n",
            "ESCALUS:\n",
            "Even in this maid overthree with him: if you speak in your holing, be gone:\n",
            "therefore, they have not known to live you:\n",
            "I would tell thou the gods Capitol will I carry him shore\n",
            "A poor of his guilty dangers, and one of his live,\n",
            "I may do thee for his good corruption!\n",
            "\n",
            "CORIOLANUS:\n",
            "Ha! what!\n",
            "\n",
            "COMINIUS:\n",
            "No matter?\n",
            "\n",
            "CORIOLANUS:\n",
            "I do bu\n",
            "---------------\n",
            "\n",
            "Men pardon, I will not stay them so a child,\n",
            "And that the midst and show the head of watch.\n",
            "\n",
            "RICHARD:\n",
            "If you should so, my lord, I thus wot thou art,\n",
            "And your true dissolved our grace to the state,\n",
            "And you shall say, 'tis my soul's false is dead.\n",
            "\n",
            "DUKE OF YORK:\n",
            "Heaven against the walls of Bolingbroke,\n",
            "Whose lady wounds and the high-holding kingdom on me,\n",
            "To the yoke of some time other face of your lips,\n",
            "And will not so the same English keeps as hear the world.\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "Lord; you have no\n",
            "---------------\n",
            "\n",
            "Messenger:\n",
            "The seal'd of the noble day of conceit,\n",
            "That disance and that have loved the marsh mine\n",
            "Of feeble his body fellow growing but\n",
            "The children of the high aI have a cause of a love\n",
            "Of all the time of his customan of death,\n",
            "To take him or woe. Come, get the duke Hecules,\n",
            "Nor like oppose thou to the just of his prince throne,\n",
            "Hath brought the world of loss of the law,\n",
            "His church beguided of volument. The commons both who hath they\n",
            "The proud Tarquine of his light: as you are so are gold\n",
            "To h\n",
            "---------------\n",
            "\n",
            "The sleep of your knee,--\n",
            "\n",
            "MOPSA:\n",
            "This is the old virtuous Capulets,\n",
            "And I hear, for the strange of the drinks.\n",
            "\n",
            "POMPEY:\n",
            "My lord, pray,\n",
            "'Tis descend to her and tarry how got it.\n",
            "\n",
            "POMPEY:\n",
            "Here's a sister.\n",
            "\n",
            "Claudio, beggar brave, sir, be flayed for that\n",
            "here would have offend him.\n",
            "\n",
            "POMPEY:\n",
            "I am repent to fair a gentleman but that he would were\n",
            "a punish of honour and would change.\n",
            "\n",
            "CAPULET:\n",
            "My lord, I do wrong the fairest and of this is half, for then\n",
            "for his bed, and his son who for he would have \n",
            "---------------\n",
            "\n",
            "BUCKINGHAM:\n",
            "Then, hold me not.\n",
            "\n",
            "LADY GREY:\n",
            "O child, how no fashion in a bit of tears\n",
            "And reconcile their souls, which we loved in prive!\n",
            "\n",
            "CATESBY:\n",
            "Bushy, old Gaunt on my souls,\n",
            "That he cannot drink with him our penniciously.\n",
            "\n",
            "HASTINGS:\n",
            "He was his suble course so his house made that which as done.\n",
            "\n",
            "CATESBY:\n",
            "I would not say you of it.\n",
            "\n",
            "GLOUCESTER:\n",
            "So it is my good mother, that I do thee\n",
            "I have say to our brother will off Broke\n",
            "On the gods who that do confess him to sleep him;\n",
            "And therefore, man th\n",
            "---------------\n",
            "\n",
            "\n",
            "MENENIUS:\n",
            "Ha!\n",
            "\n",
            "CORIOLANUS:\n",
            "Look, he that had carved him would not?\n",
            "\n",
            "All:\n",
            "No, I contend he will be a story on the poor:\n",
            "But I think he begg'd to condemn him and slept:\n",
            "Why, the double would speak close him for Rome,\n",
            "Which we have deserved in the skyey-harm for his death.\n",
            "\n",
            "COMINIUS:\n",
            "Thou the gods had too denied.\n",
            "\n",
            "CORIOLANUS:\n",
            "Good lady.\n",
            "\n",
            "CORIOLANUS:\n",
            "I do none.\n",
            "\n",
            "First Senator:\n",
            "\n",
            "CORIOLANUS:\n",
            "Mark you, or sir, I have not half to speak it.\n",
            "\n",
            "AEdile:\n",
            "Now I do disgrace: but I do, for myself were not\n",
            "A spi\n",
            "---------------\n",
            "\n",
            "She will be a goldfellow:\n",
            "Away with him! Prison! ha! not you an honest good.\n",
            "\n",
            "POLIXENES:\n",
            "Let's say: yea, I know not at your graces,\n",
            "When you have a dead roguest for harm.\n",
            "\n",
            "POLIXENES:\n",
            "You shall not be so a born.\n",
            "\n",
            "CLAUDIO:\n",
            "I would know your honour where I must be unrawledged,\n",
            "And say you the world's land in that world's death.\n",
            "\n",
            "LEONTES:\n",
            "You have father'd ordered with madness\n",
            "How I remember'd my tents of your own, you are not\n",
            "To put your times, to do go on to the point of the whole,\n",
            "When it is give\n",
            "---------------\n",
            "\n",
            "lady, who doth kill thou hearst thine arm,\n",
            "Angelo to their gentle parts, to thee, to hear thy state,\n",
            "For their volumes and renown place, best thy wild.\n",
            "Come, see, how dost thou not take poor of Burgundy\n",
            "To the sea-rathe batter show sound, King Hereford Scotle,\n",
            "And stand teached strange, there do oppose the posterity\n",
            "Of the triumph have the downfall of rotten winter\n",
            "To hold the wars of the father's death:\n",
            "And wherefore the king contrary I would you did me from of my success,\n",
            "Which most unlook'd w\n",
            "---------------\n",
            "\n",
            "GLOUCESTER:\n",
            "Lords, I charge you, Catesby, for I might not like a noble world:\n",
            "Why, where you shows unto the sun, when\n",
            "May poor to the sound is Admiralem,\n",
            "To be a bay, thou hast chose thee for the crown.\n",
            "\n",
            "GLOUCESTER:\n",
            "What, sir? what sarrier? what canst thou? come hither?\n",
            "\n",
            "PRINCE EDWARD:\n",
            "My lord, Somerset, I not for a fool's death;\n",
            "But life gallows than thy liberty thus to the storers,\n",
            "To the blower from the shoulder, throngs of the world,\n",
            "To break fournish the courter, Duke of York,\n",
            "Whose things \n",
            "---------------\n",
            "\n",
            "How chance straight and reason my hands:\n",
            "You are for my bosom's eyes\n",
            "Are stood in your land; it is a proof so swort,\n",
            "With such disposterition. Show me to prove so,\n",
            "The gates and heart of yours, are going. To what you\n",
            "That you stood of such lowers? Is yourself dead?\n",
            "Where's Montague and your lack are the drum?\n",
            "\n",
            "CLAUDIO:\n",
            "Your pleasure is comfort his fortune's oath;\n",
            "And the side hanged of your coming purpose,\n",
            "No such in being content of your lasting place.\n",
            "\n",
            "ISABELLA:\n",
            "No, 'tis not but so, if you hav\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-X4rmPkwwBeU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "328adefb-d38c-474f-f117-2e783f159aa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "head: cannot open 'out-code-generation/generated.txt' for reading: No such file or directory\n"
          ]
        }
      ]
    }
  ]
}